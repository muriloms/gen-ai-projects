{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aba1070-8974-4f31-8b89-67a836718bdc",
   "metadata": {},
   "source": [
    "## Bibliotecas de Gen AI e introdução as estruturas de redes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82fbc8d-bf6e-48e4-a2e4-12c971512ab6",
   "metadata": {},
   "source": [
    "### Estruturas neurais que impulsionam a IA generativa\n",
    "Antes do surgimento dos transformers, que funcionam como leitores extremamente rápidos capazes de compreender várias palavras de uma vez, outros métodos foram utilizados para possibilitar a geração de texto por computadores. Esses métodos foram essenciais como base para as capacidades impressionantes que temos atualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c81256-8995-46b2-8f39-d68ab93ad303",
   "metadata": {},
   "source": [
    "### Geração de texto antes dos transformers\n",
    "\n",
    "#### 1. Modelos de linguagem N-grama\n",
    "\n",
    "Os modelos N-grama funcionam como detetives da linguagem. Eles preveem qual palavra virá a seguir em uma frase com base nas palavras que vieram antes. Por exemplo, se você disser \"O céu é,\" esses modelos podem adivinhar que a próxima palavra seja \"azul.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf520af-e4b6-4b67-a9a8-b926b0a0dba8",
   "metadata": {},
   "source": [
    "#### 2. Redes neurais recorrentes (RNN)\n",
    "As redes neurais recorrentes (RNNs) são projetadas especialmente para lidar com dados sequenciais, tornando-as uma ferramenta poderosa para aplicações como modelagem de linguagem e previsão de séries temporais. A essência de seu design está em manter uma 'memória' ou 'estado oculto' ao longo da sequência, utilizando laços. Isso permite que as RNNs reconheçam e capturem as dependências temporais inerentes aos dados sequenciais.\n",
    "- Estado oculto: Frequentemente referido como a 'memória' da rede, o estado oculto é um armazenamento dinâmico de informações sobre as entradas anteriores da sequência. A cada nova entrada, esse estado oculto é atualizado, considerando tanto a nova entrada quanto seu valor anterior.\n",
    "- Dependência temporal: Os laços nas RNNs possibilitam a transferência de informações entre as etapas da sequência.\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0J87EN/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\" width=\"60%\" height=\"60%\"> \n",
    "\n",
    "<div style=\"text-align:center\"><a href=\"https://commons.wikimedia.org/wiki/File:%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\">Image Source</a></div>\n",
    "\n",
    "Ilustração do funcionamento de uma RNN: Considere uma sequência simples, como a frase: \"Eu amo RNNs\". A RNN interpreta essa frase palavra por palavra. Começando com a palavra \"Eu\", a RNN a processa, gera uma saída e atualiza seu estado oculto. Ao passar para \"amo\", a RNN processa essa palavra junto com o estado oculto atualizado, que já contém informações sobre a palavra \"Eu\". O estado oculto é novamente atualizado após essa etapa. Esse padrão de processamento e atualização continua até a última palavra. Ao final da sequência, o estado oculto idealmente encapsula as informações de toda a frase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65578b-a789-4c56-8b45-b8e196975f50",
   "metadata": {},
   "source": [
    "#### 3. Memória de longo curto prazo (Long short-term memory - LSTM) e unidades recorrentes com portas (gated recurrent units - GRUs)\n",
    "As redes LSTM (Memória de Longo Curto Prazo) e as GRUs (Unidades Recorrentes com Portas) são variações avançadas das redes neurais recorrentes (RNNs), projetadas para superar as limitações das RNNs tradicionais e aprimorar sua capacidade de modelar dados sequenciais de forma eficaz. Elas processam as sequências um elemento de cada vez e mantêm um estado interno para lembrar de elementos passados. Embora sejam eficazes para uma variedade de tarefas, enfrentam dificuldades com sequências longas e dependências de longo prazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe708b-ee5c-45aa-aa7b-782e4e36eb9c",
   "metadata": {},
   "source": [
    "#### 4. Modelos Seq2seq com atenção\n",
    "Modelos de sequência para sequência (seq2seq), geralmente construídos com RNNs ou LSTMs, foram projetados para lidar com tarefas como tradução, onde uma sequência de entrada é transformada em uma sequência de saída.\n",
    "O mecanismo de atenção foi introduzido para permitir que o modelo \"focalize\" partes relevantes da sequência de entrada ao gerar a saída, melhorando significativamente o desempenho em tarefas como tradução automática.\n",
    "Embora esses métodos tenham trazido avanços significativos nas tarefas de geração de texto, a introdução dos transformers levou a uma mudança de paradigma. Os transformers, com seu mecanismo de autoatenção, provaram ser altamente eficientes em capturar informações contextuais em sequências longas, estabelecendo novos padrões em várias tarefas de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb0f94-d6d6-4b54-a8e2-0ee776021519",
   "metadata": {},
   "source": [
    "#### Transformers\n",
    "Propostos em um artigo intitulado \"Attention Is All You Need\" por Vaswani et al. em 2017, a arquitetura transformer substituiu o processamento sequencial pelo processamento paralelo. O principal componente por trás de seu sucesso? O mecanismo de atenção, mais precisamente, a autoatenção.\n",
    "\n",
    "Passos principais incluem:\n",
    "\n",
    "- Tokenização (Tokenization): Primeiro, a frase é dividida em tokens (palavras ou subpalavras).\n",
    "- Embeddings: Cada token é representado como um vetor, capturando seu significado.\n",
    "- Autoatenção (Self-attention): O modelo calcula pontuações que determinam a importância de cada palavra em relação a uma palavra específica da sequência. Essas pontuações são usadas para ponderar os tokens de entrada e gerar uma nova representação da sequência. Por exemplo, na frase \"Ele deu a ela um presente porque ela o ajudou\", para entender quem \"ela\" se refere, o modelo precisa prestar atenção em outras palavras da frase. O transformer faz isso para cada palavra, considerando o contexto completo, o que é especialmente poderoso para entender significados.\n",
    "- Redes neurais feed-forward: Após a atenção, cada posição é passada por uma rede feed-forward separadamente.\n",
    "- Sequência de saída: O modelo gera uma sequência de saída, que pode ser usada em várias tarefas, como classificação, tradução ou geração de texto.\n",
    "- Camadas (Layers): Transformers são modelos profundos com várias camadas de atenção e redes feed-forward, permitindo que aprendam padrões complexos.\n",
    "\n",
    "A flexibilidade da arquitetura permitiu que transformers fossem usados além do NLP, aplicando-se também no processamento de imagens e vídeos. Em NLP, modelos baseados em transformers, como BERT, GPT e suas variantes, estabeleceram o estado da arte em várias tarefas, desde classificação de texto até tradução."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f84378-9667-4ace-bd7d-5597d87d1305",
   "metadata": {},
   "source": [
    "#### Construir um chatbot simples com transformers\n",
    "Usando a biblioteca transformers da Hugging Face, uma ferramenta de código aberto para processamento de linguagem natural (NLP) que oferece muitos recursos úteis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40236dbf-b633-478d-b3e4-7c58cce4202a",
   "metadata": {},
   "source": [
    "#### 1. Instalar as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b55d019-056b-4eb4-b687-34ad3b5610f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qq tensorflow\n",
    "#!pip install -qq transformers\n",
    "#!pip install sentencepiece\n",
    "#!pip install torch==2.0.3\n",
    "#!pip install torchtext==0.15.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faad163-4278-491f-9859-8bab9776bf98",
   "metadata": {},
   "source": [
    "#### 2. Importar as ferramentas e bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ca83f9-658d-40e0-84dc-4b2269c32e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47729d4-94af-4c94-b0a4-f52304ce7ab1",
   "metadata": {},
   "source": [
    "Iniciar as variáveis usando duas classes indispensáveis da biblioteca transformers:\n",
    "\n",
    "- model é uma instância da classe AutoModelForSeq2SeqLM. Essa classe permite que você interaja com o modelo de linguagem escolhido.\n",
    "- tokenizer é uma instância da classe AutoTokenizer. Essa classe otimiza sua entrada e a apresenta ao modelo de linguagem da maneira mais eficiente. Ela faz isso convertendo o texto em \"tokens\", a forma preferida do modelo para interpretar texto. Para este exemplo, você escolhe o modelo \"facebook/blenderbot-400M-distill\" porque ele é livremente disponível sob uma licença de código aberto e opera em um ritmo relativamente rápido. Para uma variedade de modelos e suas capacidades, você pode explorar o site da Hugging Face: [Hugging Face Models](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407d677f-b931-48ae-9f66-43a0f99f1e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|████████████████████████████████████████████████████████████| 730M/730M [00:33<00:00, 22.0MB/s]\n",
      "generation_config.json: 100%|█████████████████████████████████████████████████████████████████| 347/347 [00:00<?, ?B/s]\n",
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████| 1.15k/1.15k [00:00<?, ?B/s]\n",
      "vocab.json: 100%|████████████████████████████████████████████████████████████████████| 127k/127k [00:00<00:00, 575kB/s]\n",
      "merges.txt: 100%|██████████████████████████████████████████████████████████████████| 62.9k/62.9k [00:00<00:00, 572kB/s]\n",
      "added_tokens.json: 100%|████████████████████████████████████████████████████████████████████| 16.0/16.0 [00:00<?, ?B/s]\n",
      "special_tokens_map.json: 100%|████████████████████████████████████████████████████████████████| 772/772 [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "# Selecionar modelo\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "# Carregar modelo e tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03f961-1518-46be-9df9-8721b12434d1",
   "metadata": {},
   "source": [
    "Após a inicialização, vamos configurar a função de chat para possibilitar a interação em tempo real com o chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c163f6c0-4c19-47a4-83b7-61c6af4ae62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat function\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You:\")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        # Tokenizer input and generate response\n",
    "        inputs = tokenizer.encode(input_text,\n",
    "                                    return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens = 150)\n",
    "        response = tokenizer.decode(outputs[0],\n",
    "                                    skip_special_tokens=True\n",
    "                                   ).strip()\n",
    "\n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed9746bf-072f-4e88-b1a3-2795aa392db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Hi Chat, how are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm doing well, thank you. How are you this fine evening? Do you have any plans?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Start chatting...\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9922f2-cdec-4eac-8bed-bb63f491b370",
   "metadata": {},
   "source": [
    "#### 3. Experimentando outro modelo de linguagem e comparando a saída\n",
    "Vamos utilizar um modelo de linguagem diferente, como o modelo \"flan-t5-base\" do Google, para criar um chatbot semelhante. É possível usar uma função de chat semelhante à definida no Passo 2 e comparar as respostas dos dois modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32850be7-ca9e-4b82-969e-7d3a44c92133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████| 2.54k/2.54k [00:00<?, ?B/s]\n",
      "spiece.model: 100%|█████████████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 9.91MB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████| 2.42M/2.42M [00:00<00:00, 3.58MB/s]\n",
      "special_tokens_map.json: 100%|████████████████████████████████████████████████████████████| 2.20k/2.20k [00:00<?, ?B/s]\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████| 1.40k/1.40k [00:00<?, ?B/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 990M/990M [00:50<00:00, 19.5MB/s]\n",
      "generation_config.json: 100%|█████████████████████████████████████████████████████████████████| 147/147 [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c51b008-3eae-439f-9483-f0ea3b26667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Hi Chat, are you well?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm fine.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Start chatting\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4637e7a-5259-4c92-9718-1f63fd421ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████| 2.54k/2.54k [00:00<?, ?B/s]\n",
      "spiece.model: 100%|█████████████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 3.28MB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████| 2.42M/2.42M [00:00<00:00, 3.38MB/s]\n",
      "special_tokens_map.json: 100%|████████████████████████████████████████████████████████████| 2.20k/2.20k [00:00<?, ?B/s]\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████| 1.40k/1.40k [00:00<?, ?B/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 308M/308M [00:18<00:00, 16.3MB/s]\n",
      "generation_config.json: 100%|█████████████████████████████████████████████████████████████████| 147/147 [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21ad342f-8760-4bcc-a11e-89fde4208254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Hi, how are you chat?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: i'm a sailor\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: What is a sailor?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: a sailor\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Ok, so you're not gonna be able to get a t-shirt.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Start chatting\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac442ccd-e26b-4661-b763-34f4ace143da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|████████████████████████████████████████████████████████████████████████| 1.72k/1.72k [00:00<?, ?B/s]\n",
      "vocab.json: 100%|███████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 10.8MB/s]\n",
      "merges.txt: 100%|███████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 1.96MB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 5.81MB/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 558M/558M [00:30<00:00, 18.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "289a7ffc-7a76-470b-b6aa-f43be88c4143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Hi, testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi, testing...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Why this is?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Why this is?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Start chatting\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c4d55-d832-47d3-bbf9-0d2aec6e8987",
   "metadata": {},
   "source": [
    "____\n",
    "Esse material tem como referência o curso [Generative AI and LLMs: Architecture and Data Preparation](https://www.coursera.org/learn/generative-ai-llm-architecture-data-preparation?specialization=generative-ai-engineering-with-llms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423128b-8524-4b6a-8579-e5ee6f8d5677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
