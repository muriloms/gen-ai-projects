{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919da87d-0d70-46b8-835e-2259fd98049a",
   "metadata": {},
   "source": [
    "# Carregar models e inferencia com Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f841d-4e54-4b69-ac9f-1a92cdd110f1",
   "metadata": {},
   "source": [
    "Neste projeto, vamos explorar a biblioteca de transformadores Hugging Face para várias tarefas de processamento de linguagem natural (NLP). Vamos realizar a classificação de texto e a geração de texto usando modelos pré-treinados como `DistilBERT` e `GPT-2` sem usar a função `pipeline()`, entendendo as etapas envolvidas no carregamento de modelos, tokenização de entrada, execução de inferência e processamento de saídas. Vamos entender a simplicidade e a eficiência de usar a função `pipeline()` para realizar as mesmas tarefas com o mínimo de código. A função `pipeline()` simplifica o processo, tornando mais fácil e rápido implementar soluções de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2db4dc-35b9-4f74-a703-a38af73ad998",
   "metadata": {},
   "source": [
    "### Objetivos\n",
    "\n",
    "- Aprenda a configurar e usar a biblioteca Hugging Face `transformers`.\n",
    "- Execute a classificação de texto e geração de texto usando modelos DistilBERT e GPT-2 sem `pipeline()`.\n",
    "- Entenda e utilize a função `pipeline()` para simplificar várias tarefas de PNL.\n",
    "- Compare a facilidade de usar modelos diretamente versus usar a função `pipeline()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7f38a-6694-47f1-b7e6-cc6002f105ef",
   "metadata": {},
   "source": [
    "### Preparar setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab7a641-37d8-441e-bcdb-d2ffc0927fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "    \n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install(package, pip_name=None):\n",
    "    if pip_name is None:\n",
    "        pip_name = package\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is None:\n",
    "        print(f\"{package} não está instalado. Instalando...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "    else:\n",
    "        print(f\"{package} já está instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73f25c3-fee9-4515-a195-36097eb75a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch já está instalado.\n",
      "transformers já está instalado.\n"
     ]
    }
   ],
   "source": [
    "check_and_install('torch')\n",
    "check_and_install('transformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb705fe-cca3-4efd-84a5-de934144e802",
   "metadata": {},
   "source": [
    "### Carregar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7a80f3-7f9a-4f37-b1a0-61a4ae63af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b753bd-7b4b-4b2c-910e-bdb196e0295c",
   "metadata": {},
   "source": [
    "### Classificação de texto com DistilBERT\n",
    "Inicializar um tokenizador e um modelo para análise de sentimento usando o DistilBERT ajustado no conjunto de dados SST-2. Essa configuração é útil para tarefas em que você precisa classificar rapidamente o sentimento de um pedaço de texto com um modelo de transformador eficiente e pré-treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ba8ffc-1afd-4bde-b82b-20370821026d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560ce082c93b44c3b70918e205bf3f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e703f683b6e4108be74ead89eaa611a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2351b56b1a4f0bb4339c53c16fc17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333b61faa79e4d7cafb2a2504f6d5bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8c363-5987-4ae5-aa5b-16067c2f7a2d",
   "metadata": {},
   "source": [
    "### Pre processar texto de input\n",
    "Tokenizar o texto de input e convertar para o formato adequado para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9157559-6b4d-4ecf-a8de-656920b0a4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCongratulation! You've won a free ticket to the Bahamas.\\nReply WIN to claim.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"\"\"\n",
    "Congratulation! You've won a free ticket to the Bahamas.\n",
    "Reply WIN to claim.\n",
    "\"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8dedefe-15d2-4d10-b6b3-252796397a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 26478,  8609,  9513,   999,  2017,  1005,  2310,  2180,  1037,\n",
      "          2489,  7281,  2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,\n",
      "          1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3c288-0831-4e89-af68-7178ff06c439",
   "metadata": {},
   "source": [
    "Os ids de token são os índices de token ```attention_mask``` são essenciais para processar corretamente sequências preenchidas, garantindo computação eficiente e mantendo o desempenho do modelo. Mesmo quando nenhum token é explicitamente mascarado, ele ajuda o modelo a diferenciar entre conteúdo real e preenchimento, o que é crítico para o processamento preciso e eficiente de dados de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9c163-e160-4ec1-80a5-b3571ac1de94",
   "metadata": {},
   "source": [
    "### Executar inferência\n",
    "O gerenciador de contexto `torch.no_grad()` é usado para desabilitar o cálculo de gradiente.\n",
    "Isso reduz o consumo de memória e acelera a computação, pois gradientes não são necessários para inferência (ou seja, quando você não está treinando o modelo). A sintaxe **inputs é usada para descompactar um dicionário de argumentos de palavras-chave em Python. No contexto do model(**inputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7a4a0c2-5273-4f5c-979d-5a8dc537b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe5e647b-c6cd-4e49-a175-5c26e843a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method is input_ids, and attention_mask is their own parameter\n",
    "#model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299259ee-a261-4570-998e-e8f5a8326d9d",
   "metadata": {},
   "source": [
    "#### Obtenha os logits\n",
    "Os logits são as previsões brutas e não normalizadas do modelo. Vamos extrair os logits das saídas do modelo para executar processamento posterior, como determinar a classe prevista ou calcular probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa79ad67-cee0-450b-8f15-a1be7ab729af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57016d52-d692-4c75-8e52-600fc5eb0dd7",
   "metadata": {},
   "source": [
    "### Pós-processar a saída\n",
    "Converter os logits em probabilidades e obter a classe prevista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dd19068-ddc1-410a-9876-387c3da15212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b2aecf-3942-4dac-86ab-c8653cab9210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# Map the predicted class to the label\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "predicted_label = labels[predicted_class]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b693e-2b33-423b-b914-8200b4f199ab",
   "metadata": {},
   "source": [
    "### Geração de texto com GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655033a-06d5-495d-9237-5d4be1f88d80",
   "metadata": {},
   "source": [
    "### Carregar tokenizador\n",
    "Carregar o tokenizador pre-treinado GPT-2. O tokenizador pe responsável por converter texto em tokens que o modelo consegue entender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43d255b8-1876-44fd-a471-a4d11cd57122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36a2cbf5f464fd198b1e58b010757bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59725bc1dc3d44dbbe2b40f52d0bf497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff61a99eb8194f7ba903803bcdf4eef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1817da9d8848849e77645912fc674e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759649e620b04787a3b8d19e4aa6abce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532accfc-2834-4b7b-ad96-f9c72617ec6f",
   "metadata": {},
   "source": [
    "Carregar o modelo GPT-2 pré-treinado com um head de modelagem de linguagem. O modelo gera texto com base nos tokens de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48b3b53f-c485-463c-a01f-a9f207ee0924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3916f7f18aba47599563d9a326050b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0c1ab624994b7d8639ce59a1c17350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd6e51-0d72-4759-8ac6-7e3a050fe0b7",
   "metadata": {},
   "source": [
    "## Pré-processe o texto de entrada\n",
    "Tokenize o texto de entrada e converta-o para um formato adequado ao modelo, como antes de ter os índices de token, ou seja, entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d855bb1-26d9-4e92-88ce-2746ba40e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0500202a-4dbb-4baa-87a8-36d84489c31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c482b0-519f-4228-b6bd-06b85b8ee5ff",
   "metadata": {},
   "source": [
    "### Executar inferência\n",
    "Gerar texto usando o modelo\n",
    "\n",
    "```inputs:``` IDs de token de entrada do tokenizador\n",
    "\n",
    "```attention_mask:``` Máscara indicando quais tokens atender\n",
    "\n",
    "```pad_token_id:```ID de token de preenchimento definido para o ID de token de fim de sequência\n",
    "\n",
    "```max_length:``` Comprimento máximo das sequências geradas\n",
    "\n",
    "```num_return_sequence:``` Número de sequências a serem geradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2b50e2-bf21-4830-a183-f9d6e1f43ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "output_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a60f6d-8953-46f5-83e0-d3c39a9d55b1",
   "metadata": {},
   "source": [
    "## Pós-processar a saída\n",
    "Decodifique os tokens gerados para obter o texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a1c7283-3eb6-4723-bee9-b2aebe43148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output_ids[0],\n",
    "                                  skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65c66566-55b4-45a2-83b6-18a1daa65581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775c6c6-49a7-47c1-91d6-564be30f89ee",
   "metadata": {},
   "source": [
    "### Função `pipeline()` do Hugging Face\n",
    "\n",
    "A função `pipeline()` da biblioteca `transformers` do Hugging Face é uma API de alto nível projetada para simplificar o uso de modelos pré-treinados para várias tarefas de processamento de linguagem natural (NLP). Ela abstrai as complexidades do carregamento de modelos, tokenização, inferência e pós-processamento, permitindo que os usuários realizem tarefas NLP complexas com apenas algumas linhas de código."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b3181-4044-44ba-810e-4ab58f6e09e0",
   "metadata": {},
   "source": [
    "#### Definição\n",
    "\n",
    "```python\n",
    "transformers.pipeline(\n",
    "    task: str,\n",
    "    model: Optional = None,\n",
    "    config: Optional = None,\n",
    "    tokenizer: Optional = None,\n",
    "    feature_extractor: Optional = None,\n",
    "    framework: Optional = None,\n",
    "    revision: str = 'main',\n",
    "    use_fast: bool = True,\n",
    "    model_kwargs: Dict[str, Any] = None,\n",
    "    **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3fb92b-09b1-456a-925d-b9c0b739e11f",
   "metadata": {},
   "source": [
    "## Parâmetros\n",
    "\n",
    "- **task**: `str`\n",
    "- A tarefa a ser executada, como \"classificação de texto\", \"geração de texto\", \"resposta a perguntas\", etc.\n",
    "- Exemplo: `\"text-classification\"`\n",
    "\n",
    "- **model**: `Opcional`\n",
    "- O modelo a ser usado. Pode ser uma string (identificador de modelo do hub de modelos Hugging Face), um caminho para um diretório contendo arquivos de modelo ou uma instância de modelo pré-carregada.\n",
    "- Exemplo: `\"distilbert-base-uncased-finetuned-sst-2-english\"`\n",
    "\n",
    "- **config**: `Opcional`\n",
    "- A configuração a ser usada. Pode ser uma string, um caminho para um diretório ou um objeto de configuração pré-carregado.\n",
    "- Exemplo: `{\"output_attentions\": True}`\n",
    "\n",
    "- **tokenizer**: `Opcional`\n",
    "- O tokenizador a ser usado. Pode ser uma string, um caminho para um diretório ou uma instância de tokenizador pré-carregada.\n",
    "- Exemplo: `\"bert-base-uncased\"`\n",
    "\n",
    "- **feature_extractor**: `Opcional`\n",
    "- O extrator de recursos a ser usado para tarefas que o exigem (por exemplo, processamento de imagens).\n",
    "- Exemplo: `\"facebook/detectron2\"`\n",
    "\n",
    "- **framework**: `Opcional`\n",
    "- O framework a ser usado, seja `\"pt\"` para PyTorch ou `\"tf\"` para TensorFlow. Se não for especificado, será inferido.\n",
    "- Exemplo: `\"pt\"`\n",
    "\n",
    "- **revision**: `str`, padrão `'main'`\n",
    "- A versão específica do modelo a ser usada (branch, tag ou hash de commit).\n",
    "- Exemplo: `\"v1.0\"`\n",
    "\n",
    "- **use_fast**: `bool`, padrão `True`\n",
    "- Se deve usar a versão rápida do tokenizador, se disponível.\n",
    "- Exemplo: `True`\n",
    "\n",
    "- **model_kwargs**: `Dict[str, Any]`, padrão `None`\n",
    "- Argumentos de palavra-chave adicionais passados ​​para o modelo durante a inicialização.\n",
    "- Exemplo: `{\"output_hidden_states\": True}`\n",
    "\n",
    "- **kwargs**: `Any`\n",
    "- Argumentos de palavra-chave adicionais passados ​​para os componentes do pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f12a7b-7f31-4f5a-aabb-3b4edf62eff0",
   "metadata": {},
   "source": [
    "## Tipos de tarefa\n",
    "\n",
    "A função `pipeline()` suporta uma ampla gama de tarefas de PNL. Aqui estão algumas das tarefas comuns:\n",
    "\n",
    "1. **Classificação de texto**: `text-classification`\n",
    "- **Objetivo**: Classificar texto em categorias predefinidas.\n",
    "- **Casos de uso**: Análise de sentimento, detecção de spam, classificação de tópicos.\n",
    "\n",
    "2. **Geração de texto**: `text-generation`\n",
    "- **Objetivo**: Gerar texto coerente com base em um prompt fornecido.\n",
    "- **Casos de uso**: Escrita criativa, geração de diálogo, conclusão de história.\n",
    "\n",
    "3. **Resposta a perguntas**: `question-answering`\n",
    "- **Objetivo**: Responder a perguntas com base em um contexto fornecido.\n",
    "- **Casos de uso**: Construir sistemas de perguntas e respostas, recuperação de informações de documentos.\n",
    "\n",
    "4. **Reconhecimento de Entidade Nomeada (NER)**: `ner` (ou `token-classification`)\n",
    "- **Objetivo**: Identificar e classificar entidades nomeadas (como pessoas, organizações, locais) no texto.\n",
    "- **Casos de Uso**: Extrair informações estruturadas de texto não estruturado.\n",
    "\n",
    "5. **Resumo**: `summarization`\n",
    "- **Objetivo**: Resumir longos trechos de texto em resumos mais curtos e coerentes.\n",
    "- **Casos de Uso**: Resumo de documentos, resumo de notícias.\n",
    "\n",
    "6. **Tradução**: `translation_xx_to_yy` (por exemplo, `translation_en_to_fr`)\n",
    "- **Objetivo**: Traduzir texto de um idioma para outro.\n",
    "- **Casos de Uso**: Tradução de idiomas, aplicativos multilíngues.\n",
    "\n",
    "7. **Fill-Mask**: `fill-mask`\n",
    "- **Propósito**: Prever palavras mascaradas em uma frase (útil para modelagem de linguagem mascarada).\n",
    "- **Casos de uso**: Tarefas de modelagem de linguagem, compreensão de previsões de modelo.\n",
    "\n",
    "8. **Classificação Zero-Shot**: `zero-shot-classification`\n",
    "- **Propósito**: Classificar texto em categorias sem precisar de dados de treinamento para essas categorias.\n",
    "- **Casos de uso**: Tarefas de classificação flexíveis e adaptáveis.\n",
    "\n",
    "9. **Extração de recursos**: `feature-extraction`\n",
    "- **Propósito**: Extrair recursos de estado ocultos do texto.\n",
    "- **Casos de uso**: Tarefas posteriores que exigem representações de texto, como agrupamento, similaridade ou treinamento de modelo personalizado adicional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a700e1-c989-463a-9695-310db9ee9724",
   "metadata": {},
   "source": [
    "### Exemplo 1: Classificação de texto usando `pipeline()`\n",
    "\n",
    "Neste exemplo, você usará a função `pipeline()` para executar a classificação de texto. Você carregará um modelo de classificação de texto pré-treinado e o usará para classificar um texto de amostra.\n",
    "\n",
    "#### Carregue o modelo de classificação de texto:\n",
    "Inicializamos o pipeline para a tarefa `text-classification`, especificando o modelo `\"distilbert-base-uncased-finetuned-sst-2-english\"`. Este modelo é ajustado para análise de sentimento.\n",
    "\n",
    "#### Classifique o texto de amostra:\n",
    "Usamos o classificador para classificar um texto de amostra: \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\" A função `classifier` retorna o resultado da classificação, que é então impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cecb0339-5f83-4e0b-9597-55d13823c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a general text classification model\n",
    "classifier = pipeline(\"text-classification\",\n",
    "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                     framework='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32912722-cb7e-49fa-bec4-63665d570a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997586607933044}]\n"
     ]
    }
   ],
   "source": [
    "# Classify a sample text\n",
    "result = classifier(\"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17078e9d-0493-41ce-86ac-0658cd99ace8",
   "metadata": {},
   "source": [
    "### Exemplo 2: Detecção de idioma usando `pipeline()`\n",
    "\n",
    "Neste exemplo, você usará a função `pipeline()` para executar a detecção de idioma. Você carregará um modelo de detecção de idioma pré-treinado e o usará para identificar o idioma de um texto de amostra.\n",
    "\n",
    "#### Carregue o modelo de detecção de idioma:\n",
    "Inicializamos o pipeline para a tarefa `text-classification`, especificando o modelo `\"papluca/xlm-roberta-base-language-detection\"`. Este modelo é ajustado para detecção de idioma.\n",
    "\n",
    "#### Classifique o texto de amostra:\n",
    "Usamos o classificador para detectar o idioma de um texto de amostra: \"Bonjour, comment ça va?\" A função `classifier` retorna o resultado da classificação, que é então impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7906900a-4209-41ad-ab26-85b2a498205e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6609bdfd56ad47639169014a6615465e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2122dbaf1d524ab6ba971617f1cc6ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab35f67944e42ca9c50c5a64dfdd2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac352afcc71a4e579619bda490a93ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651652c89d344aa18ce6b345ed93959e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b847e41736e4a9e8e2d1c3f7ad17ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", \n",
    "                      model=\"papluca/xlm-roberta-base-language-detection\",\n",
    "                     framework='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9f12b47-4704-48b9-96e3-bc098edc77f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'fr', 'score': 0.9934879541397095}]\n"
     ]
    }
   ],
   "source": [
    "result = classifier(\"Bonjour, comment ça va?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54273ba1-bbfe-4696-a713-a465dabb96fc",
   "metadata": {},
   "source": [
    "### Exemplo 3: Geração de texto usando `pipeline()`\n",
    "\n",
    "Neste exemplo, você usará a função `pipeline()` para executar a geração de texto. Você carregará um modelo de geração de texto pré-treinado e o usará para gerar texto com base em um prompt fornecido.\n",
    "\n",
    "#### Inicialize o modelo de geração de texto:\n",
    "Inicializamos o pipeline para a tarefa `text-generation`, especificando o modelo `\"gpt2\"`. GPT-2 é um modelo bem conhecido para tarefas de geração de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a2f68a5-61f8-4a17-bdf4-5b6c4bfef045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text generation pipeline with GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", framework='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877edb5c-ae45-40a8-aca5-987b53d72e66",
   "metadata": {},
   "source": [
    "#### Gerar texto com base em um prompt fornecido:\n",
    "Usamos o gerador para gerar texto com base em um prompt: \"Once upon a time\". Vamos especificar `max_length=50`, `truncation=True` para limitar o texto gerado a 50 tokens e `num_return_sequences=1` para gerar uma sequência. A função `generator` retorna o texto gerado, que é então impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2edf1cb-b2ac-495c-bf5b-d2e786431c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text based on a given prompt\n",
    "prompt = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc53887f-edda-49a3-9840-a2242902144a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = generator(prompt, \n",
    "                   max_length=50,\n",
    "                   num_return_sequences=1,\n",
    "                   truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0dfd218-7db8-4dfb-9156-5781415c7450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time it was expected that the entire country would start the debate, but just six days later, the President decided that time had actually run out. The American people have done enough. They have demanded change. They have called for a new\n"
     ]
    }
   ],
   "source": [
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f21d7-274a-41de-8c0b-59a7a2ecc91b",
   "metadata": {},
   "source": [
    "### Exemplo 4: Geração de texto usando T5 com `pipeline()`\n",
    "\n",
    "Neste exemplo, você usará a função `pipeline()` para executar a geração de texto para texto com o modelo T5. Você carregará um modelo T5 pré-treinado e o usará para traduzir uma frase do inglês para o francês com base em um prompt fornecido.\n",
    "\n",
    "#### Inicialize o modelo de geração de texto:\n",
    "Inicializamos o pipeline para a tarefa `text2text-generation, especificando o modelo \"t5-small\". O T5 é um modelo versátil que pode executar várias tarefas de geração de texto para texto, incluindo tradução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cfac1f1-69bf-4dfd-a67d-058189cd9523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3c40e96be24eda8af8260bdabcc1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f97caba0d94161b571854db7d2ddb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dff4437626467496161a0151400965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2535cb09aafa40c9933693c948af6391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf13df383f3a4e24a28f43bee0805155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9eeedecaee442f8954cc8bd11167650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the text generation pipeline with T5\n",
    "generator = pipeline(\"text2text-generation\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bbcf67-1a1d-4868-b455-73d02cce5a56",
   "metadata": {},
   "source": [
    "#### Gerar texto com base em um prompt fornecido:\n",
    "Usamos o gerador para traduzir uma frase do inglês para o francês com base no prompt: \"translate English to French: How are you?\". Vamos especificar `max_length=50` para limitar o texto gerado a 50 tokens e `num_return_sequences=1` para gerar uma sequência. A função `generator` retorna o texto traduzido, que é então impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a11eb48e-b54a-45f1-8a4f-7c88c280cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text based on a given prompt\n",
    "prompt = \"translate English to French: How are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1e226c5-8b77-4c43-8b4c-8d999db5efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generator(prompt,\n",
    "                   max_length=50,\n",
    "                   num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12e8cea2-a9e5-4f8a-8874-e4d9f3505893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment êtes-vous?\n"
     ]
    }
   ],
   "source": [
    "# Print the genrated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8104f17-ccf1-47ab-b880-5a3062210c95",
   "metadata": {},
   "source": [
    "## Benefícios de usar `pipeline()`\n",
    "\n",
    "- **Código Boilerplate Reduzido**: Simplifica o código necessário para executar tarefas de PNL.\n",
    "- **Legibilidade Aprimorada**: Torna o código mais legível e expressivo.\n",
    "- **Eficiência de Tempo**: Economiza tempo ao manipular o carregamento de modelos, tokenização, inferência e pós-processamento automaticamente.\n",
    "- **API Consistente**: Fornece uma API consistente em diferentes tarefas, permitindo fácil experimentação e prototipagem rápida.\n",
    "- **Manipulação Automática de Framework**: Manipula automaticamente o framework subjacente (TensorFlow ou PyTorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8f965-2741-48f9-a9eb-ca0815b5de13",
   "metadata": {},
   "source": [
    "## Quando usar `pipeline()`\n",
    "\n",
    "- **Prototipagem rápida**: quando você precisa prototipar rapidamente um aplicativo NLP ou experimentar diferentes modelos.\n",
    "- **Tarefas simples**: ao executar tarefas NLP simples ou comuns que são bem suportadas pela função `pipeline()`.\n",
    "- **Implantação**: ao implantar modelos NLP em ambientes onde simplicidade e facilidade de uso são cruciais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b1f37-3d95-47e1-9858-4e45dbc8bb6c",
   "metadata": {},
   "source": [
    "## Quando evitar `pipeline()`\n",
    "\n",
    "- **Tarefas personalizadas**: quando você precisa executar tarefas altamente personalizadas que não são bem suportadas pela função `pipeline()`.\n",
    "- **Otimização de desempenho**: quando você precisa de controle refinado sobre o modelo e o processo de tokenização para otimização de desempenho ou casos de uso específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf6793-c846-49e0-8fc1-5f41518fa6bc",
   "metadata": {},
   "source": [
    "### Exercício: tarefa de máscara de preenchimento usando BERT com `pipeline()`\n",
    "\n",
    "Neste exercício, você usará a função `pipeline()` para executar uma tarefa de máscara de preenchimento usando o modelo BERT. Você carregará um modelo BERT pré-treinado e o usará para prever a palavra mascarada em uma determinada frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05e50f00-ff0b-41a0-9ffd-13027badd53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a1b5636e804f98b4d215fff664fe00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd5120c51b24f01bacfdff55bf5f8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.416788786649704, 'token': 3000, 'token_str': 'paris', 'sequence': 'the capital of france is paris.'}, {'score': 0.07141651213169098, 'token': 22479, 'token_str': 'lille', 'sequence': 'the capital of france is lille.'}, {'score': 0.06339280307292938, 'token': 10241, 'token_str': 'lyon', 'sequence': 'the capital of france is lyon.'}, {'score': 0.04444750398397446, 'token': 16766, 'token_str': 'marseille', 'sequence': 'the capital of france is marseille.'}, {'score': 0.03029724210500717, 'token': 7562, 'token_str': 'tours', 'sequence': 'the capital of france is tours.'}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the fill-mask pipeline with BERT\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Generate text by filling in the masked token\n",
    "prompt = \"The capital of France is [MASK].\"\n",
    "result = fill_mask(prompt)\n",
    "\n",
    "# Print the generated text\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b191c1-d29b-478a-a475-3d824a0c105e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
