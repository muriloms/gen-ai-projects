{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919da87d-0d70-46b8-835e-2259fd98049a",
   "metadata": {},
   "source": [
    "# Carregar models e inferencia com Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f841d-4e54-4b69-ac9f-1a92cdd110f1",
   "metadata": {},
   "source": [
    "Neste projeto, vamos explorar a biblioteca de transformadores Hugging Face para v√°rias tarefas de processamento de linguagem natural (NLP). Vamos realizar a classifica√ß√£o de texto e a gera√ß√£o de texto usando modelos pr√©-treinados como `DistilBERT` e `GPT-2` sem usar a fun√ß√£o `pipeline()`, entendendo as etapas envolvidas no carregamento de modelos, tokeniza√ß√£o de entrada, execu√ß√£o de infer√™ncia e processamento de sa√≠das. Vamos entender a simplicidade e a efici√™ncia de usar a fun√ß√£o `pipeline()` para realizar as mesmas tarefas com o m√≠nimo de c√≥digo. A fun√ß√£o `pipeline()` simplifica o processo, tornando mais f√°cil e r√°pido implementar solu√ß√µes de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2db4dc-35b9-4f74-a703-a38af73ad998",
   "metadata": {},
   "source": [
    "### Objetivos\n",
    "\n",
    "- Aprenda a configurar e usar a biblioteca Hugging Face `transformers`.\n",
    "- Execute a classifica√ß√£o de texto e gera√ß√£o de texto usando modelos DistilBERT e GPT-2 sem `pipeline()`.\n",
    "- Entenda e utilize a fun√ß√£o `pipeline()` para simplificar v√°rias tarefas de PNL.\n",
    "- Compare a facilidade de usar modelos diretamente versus usar a fun√ß√£o `pipeline()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7f38a-6694-47f1-b7e6-cc6002f105ef",
   "metadata": {},
   "source": [
    "### Preparar setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab7a641-37d8-441e-bcdb-d2ffc0927fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "    \n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install(package, pip_name=None):\n",
    "    if pip_name is None:\n",
    "        pip_name = package\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is None:\n",
    "        print(f\"{package} n√£o est√° instalado. Instalando...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "    else:\n",
    "        print(f\"{package} j√° est√° instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73f25c3-fee9-4515-a195-36097eb75a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch j√° est√° instalado.\n",
      "transformers j√° est√° instalado.\n"
     ]
    }
   ],
   "source": [
    "check_and_install('torch')\n",
    "check_and_install('transformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb705fe-cca3-4efd-84a5-de934144e802",
   "metadata": {},
   "source": [
    "### Carregar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7a80f3-7f9a-4f37-b1a0-61a4ae63af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b753bd-7b4b-4b2c-910e-bdb196e0295c",
   "metadata": {},
   "source": [
    "### Classifica√ß√£o de texto com DistilBERT\n",
    "Inicializar um tokenizador e um modelo para an√°lise de sentimento usando o DistilBERT ajustado no conjunto de dados SST-2. Essa configura√ß√£o √© √∫til para tarefas em que voc√™ precisa classificar rapidamente o sentimento de um peda√ßo de texto com um modelo de transformador eficiente e pr√©-treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ba8ffc-1afd-4bde-b82b-20370821026d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560ce082c93b44c3b70918e205bf3f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e703f683b6e4108be74ead89eaa611a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2351b56b1a4f0bb4339c53c16fc17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333b61faa79e4d7cafb2a2504f6d5bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8c363-5987-4ae5-aa5b-16067c2f7a2d",
   "metadata": {},
   "source": [
    "### Pre processar texto de input\n",
    "Tokenizar o texto de input e convertar para o formato adequado para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9157559-6b4d-4ecf-a8de-656920b0a4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCongratulation! You've won a free ticket to the Bahamas.\\nReply WIN to claim.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"\"\"\n",
    "Congratulation! You've won a free ticket to the Bahamas.\n",
    "Reply WIN to claim.\n",
    "\"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8dedefe-15d2-4d10-b6b3-252796397a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 26478,  8609,  9513,   999,  2017,  1005,  2310,  2180,  1037,\n",
      "          2489,  7281,  2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,\n",
      "          1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3c288-0831-4e89-af68-7178ff06c439",
   "metadata": {},
   "source": [
    "Os ids de token s√£o os √≠ndices de token ```attention_mask``` s√£o essenciais para processar corretamente sequ√™ncias preenchidas, garantindo computa√ß√£o eficiente e mantendo o desempenho do modelo. Mesmo quando nenhum token √© explicitamente mascarado, ele ajuda o modelo a diferenciar entre conte√∫do real e preenchimento, o que √© cr√≠tico para o processamento preciso e eficiente de dados de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9c163-e160-4ec1-80a5-b3571ac1de94",
   "metadata": {},
   "source": [
    "### Executar infer√™ncia\n",
    "O gerenciador de contexto `torch.no_grad()` √© usado para desabilitar o c√°lculo de gradiente.\n",
    "Isso reduz o consumo de mem√≥ria e acelera a computa√ß√£o, pois gradientes n√£o s√£o necess√°rios para infer√™ncia (ou seja, quando voc√™ n√£o est√° treinando o modelo). A sintaxe **inputs √© usada para descompactar um dicion√°rio de argumentos de palavras-chave em Python. No contexto do model(**inputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7a4a0c2-5273-4f5c-979d-5a8dc537b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe5e647b-c6cd-4e49-a175-5c26e843a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method is input_ids, and attention_mask is their own parameter\n",
    "#model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299259ee-a261-4570-998e-e8f5a8326d9d",
   "metadata": {},
   "source": [
    "#### Obtenha os logits\n",
    "Os logits s√£o as previs√µes brutas e n√£o normalizadas do modelo. Vamos extrair os logits das sa√≠das do modelo para executar processamento posterior, como determinar a classe prevista ou calcular probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa79ad67-cee0-450b-8f15-a1be7ab729af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57016d52-d692-4c75-8e52-600fc5eb0dd7",
   "metadata": {},
   "source": [
    "### P√≥s-processar a sa√≠da\n",
    "Converter os logits em probabilidades e obter a classe prevista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dd19068-ddc1-410a-9876-387c3da15212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b2aecf-3942-4dac-86ab-c8653cab9210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# Map the predicted class to the label\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "predicted_label = labels[predicted_class]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b693e-2b33-423b-b914-8200b4f199ab",
   "metadata": {},
   "source": [
    "### Gera√ß√£o de texto com GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655033a-06d5-495d-9237-5d4be1f88d80",
   "metadata": {},
   "source": [
    "### Carregar tokenizador\n",
    "Carregar o tokenizador pre-treinado GPT-2. O tokenizador pe respons√°vel por converter texto em tokens que o modelo consegue entender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43d255b8-1876-44fd-a471-a4d11cd57122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36a2cbf5f464fd198b1e58b010757bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59725bc1dc3d44dbbe2b40f52d0bf497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff61a99eb8194f7ba903803bcdf4eef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1817da9d8848849e77645912fc674e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759649e620b04787a3b8d19e4aa6abce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532accfc-2834-4b7b-ad96-f9c72617ec6f",
   "metadata": {},
   "source": [
    "Carregar o modelo GPT-2 pr√©-treinado com um head de modelagem de linguagem. O modelo gera texto com base nos tokens de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48b3b53f-c485-463c-a01f-a9f207ee0924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3916f7f18aba47599563d9a326050b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0c1ab624994b7d8639ce59a1c17350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd6e51-0d72-4759-8ac6-7e3a050fe0b7",
   "metadata": {},
   "source": [
    "## Pr√©-processe o texto de entrada\n",
    "Tokenize o texto de entrada e converta-o para um formato adequado ao modelo, como antes de ter os √≠ndices de token, ou seja, entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d855bb1-26d9-4e92-88ce-2746ba40e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0500202a-4dbb-4baa-87a8-36d84489c31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c482b0-519f-4228-b6bd-06b85b8ee5ff",
   "metadata": {},
   "source": [
    "### Executar infer√™ncia\n",
    "Gerar texto usando o modelo\n",
    "\n",
    "```inputs:``` IDs de token de entrada do tokenizador\n",
    "\n",
    "```attention_mask:``` M√°scara indicando quais tokens atender\n",
    "\n",
    "```pad_token_id:```ID de token de preenchimento definido para o ID de token de fim de sequ√™ncia\n",
    "\n",
    "```max_length:``` Comprimento m√°ximo das sequ√™ncias geradas\n",
    "\n",
    "```num_return_sequence:``` N√∫mero de sequ√™ncias a serem geradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2b50e2-bf21-4830-a183-f9d6e1f43ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "output_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a60f6d-8953-46f5-83e0-d3c39a9d55b1",
   "metadata": {},
   "source": [
    "## P√≥s-processar a sa√≠da\n",
    "Decodifique os tokens gerados para obter o texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a1c7283-3eb6-4723-bee9-b2aebe43148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output_ids[0],\n",
    "                                  skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65c66566-55b4-45a2-83b6-18a1daa65581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775c6c6-49a7-47c1-91d6-564be30f89ee",
   "metadata": {},
   "source": [
    "### Fun√ß√£o `pipeline()` do Hugging Face\n",
    "\n",
    "A fun√ß√£o `pipeline()` da biblioteca `transformers` do Hugging Face √© uma API de alto n√≠vel projetada para simplificar o uso de modelos pr√©-treinados para v√°rias tarefas de processamento de linguagem natural (NLP). Ela abstrai as complexidades do carregamento de modelos, tokeniza√ß√£o, infer√™ncia e p√≥s-processamento, permitindo que os usu√°rios realizem tarefas NLP complexas com apenas algumas linhas de c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b3181-4044-44ba-810e-4ab58f6e09e0",
   "metadata": {},
   "source": [
    "#### Defini√ß√£o\n",
    "\n",
    "```python\n",
    "transformers.pipeline(\n",
    "    task: str,\n",
    "    model: Optional = None,\n",
    "    config: Optional = None,\n",
    "    tokenizer: Optional = None,\n",
    "    feature_extractor: Optional = None,\n",
    "    framework: Optional = None,\n",
    "    revision: str = 'main',\n",
    "    use_fast: bool = True,\n",
    "    model_kwargs: Dict[str, Any] = None,\n",
    "    **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3fb92b-09b1-456a-925d-b9c0b739e11f",
   "metadata": {},
   "source": [
    "## Par√¢metros\n",
    "\n",
    "- **task**: `str`\n",
    "- A tarefa a ser executada, como \"classifica√ß√£o de texto\", \"gera√ß√£o de texto\", \"resposta a perguntas\", etc.\n",
    "- Exemplo: `\"text-classification\"`\n",
    "\n",
    "- **model**: `Opcional`\n",
    "- O modelo a ser usado. Pode ser uma string (identificador de modelo do hub de modelos Hugging Face), um caminho para um diret√≥rio contendo arquivos de modelo ou uma inst√¢ncia de modelo pr√©-carregada.\n",
    "- Exemplo: `\"distilbert-base-uncased-finetuned-sst-2-english\"`\n",
    "\n",
    "- **config**: `Opcional`\n",
    "- A configura√ß√£o a ser usada. Pode ser uma string, um caminho para um diret√≥rio ou um objeto de configura√ß√£o pr√©-carregado.\n",
    "- Exemplo: `{\"output_attentions\": True}`\n",
    "\n",
    "- **tokenizer**: `Opcional`\n",
    "- O tokenizador a ser usado. Pode ser uma string, um caminho para um diret√≥rio ou uma inst√¢ncia de tokenizador pr√©-carregada.\n",
    "- Exemplo: `\"bert-base-uncased\"`\n",
    "\n",
    "- **feature_extractor**: `Opcional`\n",
    "- O extrator de recursos a ser usado para tarefas que o exigem (por exemplo, processamento de imagens).\n",
    "- Exemplo: `\"facebook/detectron2\"`\n",
    "\n",
    "- **framework**: `Opcional`\n",
    "- O framework a ser usado, seja `\"pt\"` para PyTorch ou `\"tf\"` para TensorFlow. Se n√£o for especificado, ser√° inferido.\n",
    "- Exemplo: `\"pt\"`\n",
    "\n",
    "- **revision**: `str`, padr√£o `'main'`\n",
    "- A vers√£o espec√≠fica do modelo a ser usada (branch, tag ou hash de commit).\n",
    "- Exemplo: `\"v1.0\"`\n",
    "\n",
    "- **use_fast**: `bool`, padr√£o `True`\n",
    "- Se deve usar a vers√£o r√°pida do tokenizador, se dispon√≠vel.\n",
    "- Exemplo: `True`\n",
    "\n",
    "- **model_kwargs**: `Dict[str, Any]`, padr√£o `None`\n",
    "- Argumentos de palavra-chave adicionais passados ‚Äã‚Äãpara o modelo durante a inicializa√ß√£o.\n",
    "- Exemplo: `{\"output_hidden_states\": True}`\n",
    "\n",
    "- **kwargs**: `Any`\n",
    "- Argumentos de palavra-chave adicionais passados ‚Äã‚Äãpara os componentes do pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f12a7b-7f31-4f5a-aabb-3b4edf62eff0",
   "metadata": {},
   "source": [
    "## Tipos de tarefa\n",
    "\n",
    "A fun√ß√£o `pipeline()` suporta uma ampla gama de tarefas de PNL. Aqui est√£o algumas das tarefas comuns:\n",
    "\n",
    "1. **Classifica√ß√£o de texto**: `text-classification`\n",
    "- **Objetivo**: Classificar texto em categorias predefinidas.\n",
    "- **Casos de uso**: An√°lise de sentimento, detec√ß√£o de spam, classifica√ß√£o de t√≥picos.\n",
    "\n",
    "2. **Gera√ß√£o de texto**: `text-generation`\n",
    "- **Objetivo**: Gerar texto coerente com base em um prompt fornecido.\n",
    "- **Casos de uso**: Escrita criativa, gera√ß√£o de di√°logo, conclus√£o de hist√≥ria.\n",
    "\n",
    "3. **Resposta a perguntas**: `question-answering`\n",
    "- **Objetivo**: Responder a perguntas com base em um contexto fornecido.\n",
    "- **Casos de uso**: Construir sistemas de perguntas e respostas, recupera√ß√£o de informa√ß√µes de documentos.\n",
    "\n",
    "4. **Reconhecimento de Entidade Nomeada (NER)**: `ner` (ou `token-classification`)\n",
    "- **Objetivo**: Identificar e classificar entidades nomeadas (como pessoas, organiza√ß√µes, locais) no texto.\n",
    "- **Casos de Uso**: Extrair informa√ß√µes estruturadas de texto n√£o estruturado.\n",
    "\n",
    "5. **Resumo**: `summarization`\n",
    "- **Objetivo**: Resumir longos trechos de texto em resumos mais curtos e coerentes.\n",
    "- **Casos de Uso**: Resumo de documentos, resumo de not√≠cias.\n",
    "\n",
    "6. **Tradu√ß√£o**: `translation_xx_to_yy` (por exemplo, `translation_en_to_fr`)\n",
    "- **Objetivo**: Traduzir texto de um idioma para outro.\n",
    "- **Casos de Uso**: Tradu√ß√£o de idiomas, aplicativos multil√≠ngues.\n",
    "\n",
    "7. **Fill-Mask**: `fill-mask`\n",
    "- **Prop√≥sito**: Prever palavras mascaradas em uma frase (√∫til para modelagem de linguagem mascarada).\n",
    "- **Casos de uso**: Tarefas de modelagem de linguagem, compreens√£o de previs√µes de modelo.\n",
    "\n",
    "8. **Classifica√ß√£o Zero-Shot**: `zero-shot-classification`\n",
    "- **Prop√≥sito**: Classificar texto em categorias sem precisar de dados de treinamento para essas categorias.\n",
    "- **Casos de uso**: Tarefas de classifica√ß√£o flex√≠veis e adapt√°veis.\n",
    "\n",
    "9. **Extra√ß√£o de recursos**: `feature-extraction`\n",
    "- **Prop√≥sito**: Extrair recursos de estado ocultos do texto.\n",
    "- **Casos de uso**: Tarefas posteriores que exigem representa√ß√µes de texto, como agrupamento, similaridade ou treinamento de modelo personalizado adicional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a700e1-c989-463a-9695-310db9ee9724",
   "metadata": {},
   "source": [
    "### Exemplo 1: Classifica√ß√£o de texto usando `pipeline()`\n",
    "\n",
    "Neste exemplo, voc√™ usar√° a fun√ß√£o `pipeline()` para executar a classifica√ß√£o de texto. Voc√™ carregar√° um modelo de classifica√ß√£o de texto pr√©-treinado e o usar√° para classificar um texto de amostra.\n",
    "\n",
    "#### Carregue o modelo de classifica√ß√£o de texto:\n",
    "Inicializamos o pipeline para a tarefa `text-classification`, especificando o modelo `\"distilbert-base-uncased-finetuned-sst-2-english\"`. Este modelo √© ajustado para an√°lise de sentimento.\n",
    "\n",
    "#### Classifique o texto de amostra:\n",
    "Usamos o classificador para classificar um texto de amostra: \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\" A fun√ß√£o `classifier` retorna o resultado da classifica√ß√£o, que √© ent√£o impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cecb0339-5f83-4e0b-9597-55d13823c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a general text classification model\n",
    "classifier = pipeline(\"text-classification\",\n",
    "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                     framework='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32912722-cb7e-49fa-bec4-63665d570a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997586607933044}]\n"
     ]
    }
   ],
   "source": [
    "# Classify a sample text\n",
    "result = classifier(\"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17078e9d-0493-41ce-86ac-0658cd99ace8",
   "metadata": {},
   "source": [
    "### Exemplo 2: Detec√ß√£o de idioma usando `pipeline()`\n",
    "\n",
    "Neste exemplo, voc√™ usar√° a fun√ß√£o `pipeline()` para executar a detec√ß√£o de idioma. Voc√™ carregar√° um modelo de detec√ß√£o de idioma pr√©-treinado e o usar√° para identificar o idioma de um texto de amostra.\n",
    "\n",
    "#### Carregue o modelo de detec√ß√£o de idioma:\n",
    "Inicializamos o pipeline para a tarefa `text-classification`, especificando o modelo `\"papluca/xlm-roberta-base-language-detection\"`. Este modelo √© ajustado para detec√ß√£o de idioma.\n",
    "\n",
    "#### Classifique o texto de amostra:\n",
    "Usamos o classificador para detectar o idioma de um texto de amostra: \"Bonjour, comment √ßa va?\" A fun√ß√£o `classifier` retorna o resultado da classifica√ß√£o, que √© ent√£o impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7906900a-4209-41ad-ab26-85b2a498205e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6609bdfd56ad47639169014a6615465e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2122dbaf1d524ab6ba971617f1cc6ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab35f67944e42ca9c50c5a64dfdd2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac352afcc71a4e579619bda490a93ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651652c89d344aa18ce6b345ed93959e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b847e41736e4a9e8e2d1c3f7ad17ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", \n",
    "                      model=\"papluca/xlm-roberta-base-language-detection\",\n",
    "                     framework='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9f12b47-4704-48b9-96e3-bc098edc77f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'fr', 'score': 0.9934879541397095}]\n"
     ]
    }
   ],
   "source": [
    "result = classifier(\"Bonjour, comment √ßa va?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54273ba1-bbfe-4696-a713-a465dabb96fc",
   "metadata": {},
   "source": [
    "### Exemplo 3: Gera√ß√£o de texto usando `pipeline()`\n",
    "\n",
    "Neste exemplo, voc√™ usar√° a fun√ß√£o `pipeline()` para executar a gera√ß√£o de texto. Voc√™ carregar√° um modelo de gera√ß√£o de texto pr√©-treinado e o usar√° para gerar texto com base em um prompt fornecido.\n",
    "\n",
    "#### Inicialize o modelo de gera√ß√£o de texto:\n",
    "Inicializamos o pipeline para a tarefa `text-generation`, especificando o modelo `\"gpt2\"`. GPT-2 √© um modelo bem conhecido para tarefas de gera√ß√£o de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a2f68a5-61f8-4a17-bdf4-5b6c4bfef045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text generation pipeline with GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", framework='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877edb5c-ae45-40a8-aca5-987b53d72e66",
   "metadata": {},
   "source": [
    "#### Gerar texto com base em um prompt fornecido:\n",
    "Usamos o gerador para gerar texto com base em um prompt: \"Once upon a time\". Vamos especificar `max_length=50`, `truncation=True` para limitar o texto gerado a 50 tokens e `num_return_sequences=1` para gerar uma sequ√™ncia. A fun√ß√£o `generator` retorna o texto gerado, que √© ent√£o impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2edf1cb-b2ac-495c-bf5b-d2e786431c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text based on a given prompt\n",
    "prompt = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc53887f-edda-49a3-9840-a2242902144a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = generator(prompt, \n",
    "                   max_length=50,\n",
    "                   num_return_sequences=1,\n",
    "                   truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0dfd218-7db8-4dfb-9156-5781415c7450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time it was expected that the entire country would start the debate, but just six days later, the President decided that time had actually run out. The American people have done enough. They have demanded change. They have called for a new\n"
     ]
    }
   ],
   "source": [
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f21d7-274a-41de-8c0b-59a7a2ecc91b",
   "metadata": {},
   "source": [
    "### Exemplo 4: Gera√ß√£o de texto usando T5 com `pipeline()`\n",
    "\n",
    "Neste exemplo, voc√™ usar√° a fun√ß√£o `pipeline()` para executar a gera√ß√£o de texto para texto com o modelo T5. Voc√™ carregar√° um modelo T5 pr√©-treinado e o usar√° para traduzir uma frase do ingl√™s para o franc√™s com base em um prompt fornecido.\n",
    "\n",
    "#### Inicialize o modelo de gera√ß√£o de texto:\n",
    "Inicializamos o pipeline para a tarefa `text2text-generation, especificando o modelo \"t5-small\". O T5 √© um modelo vers√°til que pode executar v√°rias tarefas de gera√ß√£o de texto para texto, incluindo tradu√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cfac1f1-69bf-4dfd-a67d-058189cd9523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3c40e96be24eda8af8260bdabcc1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f97caba0d94161b571854db7d2ddb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dff4437626467496161a0151400965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2535cb09aafa40c9933693c948af6391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf13df383f3a4e24a28f43bee0805155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9eeedecaee442f8954cc8bd11167650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the text generation pipeline with T5\n",
    "generator = pipeline(\"text2text-generation\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bbcf67-1a1d-4868-b455-73d02cce5a56",
   "metadata": {},
   "source": [
    "#### Gerar texto com base em um prompt fornecido:\n",
    "Usamos o gerador para traduzir uma frase do ingl√™s para o franc√™s com base no prompt: \"translate English to French: How are you?\". Vamos especificar `max_length=50` para limitar o texto gerado a 50 tokens e `num_return_sequences=1` para gerar uma sequ√™ncia. A fun√ß√£o `generator` retorna o texto traduzido, que √© ent√£o impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a11eb48e-b54a-45f1-8a4f-7c88c280cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text based on a given prompt\n",
    "prompt = \"translate English to French: How are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1e226c5-8b77-4c43-8b4c-8d999db5efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generator(prompt,\n",
    "                   max_length=50,\n",
    "                   num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12e8cea2-a9e5-4f8a-8874-e4d9f3505893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment √™tes-vous?\n"
     ]
    }
   ],
   "source": [
    "# Print the genrated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8104f17-ccf1-47ab-b880-5a3062210c95",
   "metadata": {},
   "source": [
    "## Benef√≠cios de usar `pipeline()`\n",
    "\n",
    "- **C√≥digo Boilerplate Reduzido**: Simplifica o c√≥digo necess√°rio para executar tarefas de PNL.\n",
    "- **Legibilidade Aprimorada**: Torna o c√≥digo mais leg√≠vel e expressivo.\n",
    "- **Efici√™ncia de Tempo**: Economiza tempo ao manipular o carregamento de modelos, tokeniza√ß√£o, infer√™ncia e p√≥s-processamento automaticamente.\n",
    "- **API Consistente**: Fornece uma API consistente em diferentes tarefas, permitindo f√°cil experimenta√ß√£o e prototipagem r√°pida.\n",
    "- **Manipula√ß√£o Autom√°tica de Framework**: Manipula automaticamente o framework subjacente (TensorFlow ou PyTorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8f965-2741-48f9-a9eb-ca0815b5de13",
   "metadata": {},
   "source": [
    "## Quando usar `pipeline()`\n",
    "\n",
    "- **Prototipagem r√°pida**: quando voc√™ precisa prototipar rapidamente um aplicativo NLP ou experimentar diferentes modelos.\n",
    "- **Tarefas simples**: ao executar tarefas NLP simples ou comuns que s√£o bem suportadas pela fun√ß√£o `pipeline()`.\n",
    "- **Implanta√ß√£o**: ao implantar modelos NLP em ambientes onde simplicidade e facilidade de uso s√£o cruciais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b1f37-3d95-47e1-9858-4e45dbc8bb6c",
   "metadata": {},
   "source": [
    "## Quando evitar `pipeline()`\n",
    "\n",
    "- **Tarefas personalizadas**: quando voc√™ precisa executar tarefas altamente personalizadas que n√£o s√£o bem suportadas pela fun√ß√£o `pipeline()`.\n",
    "- **Otimiza√ß√£o de desempenho**: quando voc√™ precisa de controle refinado sobre o modelo e o processo de tokeniza√ß√£o para otimiza√ß√£o de desempenho ou casos de uso espec√≠ficos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf6793-c846-49e0-8fc1-5f41518fa6bc",
   "metadata": {},
   "source": [
    "### Exerc√≠cio: tarefa de m√°scara de preenchimento usando BERT com `pipeline()`\n",
    "\n",
    "Neste exerc√≠cio, voc√™ usar√° a fun√ß√£o `pipeline()` para executar uma tarefa de m√°scara de preenchimento usando o modelo BERT. Voc√™ carregar√° um modelo BERT pr√©-treinado e o usar√° para prever a palavra mascarada em uma determinada frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05e50f00-ff0b-41a0-9ffd-13027badd53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a1b5636e804f98b4d215fff664fe00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd5120c51b24f01bacfdff55bf5f8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.416788786649704, 'token': 3000, 'token_str': 'paris', 'sequence': 'the capital of france is paris.'}, {'score': 0.07141651213169098, 'token': 22479, 'token_str': 'lille', 'sequence': 'the capital of france is lille.'}, {'score': 0.06339280307292938, 'token': 10241, 'token_str': 'lyon', 'sequence': 'the capital of france is lyon.'}, {'score': 0.04444750398397446, 'token': 16766, 'token_str': 'marseille', 'sequence': 'the capital of france is marseille.'}, {'score': 0.03029724210500717, 'token': 7562, 'token_str': 'tours', 'sequence': 'the capital of france is tours.'}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the fill-mask pipeline with BERT\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Generate text by filling in the masked token\n",
    "prompt = \"The capital of France is [MASK].\"\n",
    "result = fill_mask(prompt)\n",
    "\n",
    "# Print the generated text\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b191c1-d29b-478a-a475-3d824a0c105e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
