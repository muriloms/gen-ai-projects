{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be2e307-01b5-42ed-9a92-b891a03253cb",
   "metadata": {},
   "source": [
    "# Modelos LLMs pré-treinados com Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa150dd-e9a2-4fa0-98de-97179c4738d8",
   "metadata": {},
   "source": [
    "Este projeto tem como objetivo apresentar o processo de pré-treinamento de modelos de linguagem grandes (LLMs) usando a popular biblioteca Hugging Face. Hugging Face é uma plataforma de código aberto líder para processamento de linguagem natural que fornece uma ampla gama de modelos pré-treinados e ferramentas para ajuste fino e implantação desses modelos.\n",
    "\n",
    "Vamos carregar modelos pré-treinados do Hugging Face e fazer inferências usando o módulo Pipeline. Além disso, vamos treinar LLMs pré-treinados em seus próprios dados (ajuste fino autossupervisionado). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd2910-8884-47c2-be29-4d40fd8a0201",
   "metadata": {},
   "source": [
    "### Objetivos\n",
    "\n",
    "- Carregar LLMs pré-treinados do Hugging Face e fazer inferências usando o módulo pipeline\n",
    "- Treinar LLMs pré-treinados em seus dados\n",
    "- Armazenar LLMs para ajustá-los para casos de uso específicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5cfea8-ce6b-429b-911b-1844a61f88fe",
   "metadata": {},
   "source": [
    "### Preparar setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8188e3a-5a3b-4dab-8ec8-8f8dfe34f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "    \n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install(package, pip_name=None):\n",
    "    if pip_name is None:\n",
    "        pip_name = package\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is None:\n",
    "        print(f\"{package} não está instalado. Instalando...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "    else:\n",
    "        print(f\"{package} já está instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1927df29-22c7-490b-8897-efee5d12c2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch já está instalado.\n",
      "transformers já está instalado.\n",
      "datasets já está instalado.\n",
      "portalocker já está instalado.\n",
      "accelerate já está instalado.\n",
      "torchvision já está instalado.\n",
      "protobuf não está instalado. Instalando...\n"
     ]
    }
   ],
   "source": [
    "check_and_install('torch')\n",
    "check_and_install('transformers')\n",
    "check_and_install('datasets')\n",
    "check_and_install('portalocker', 'portalocker>=2/0.0')\n",
    "check_and_install('accelerate')\n",
    "check_and_install('torchvision')\n",
    "check_and_install('protobuf', 'protobuf==3.20.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440e6cb-59f6-4e00-bc04-87728e242de6",
   "metadata": {},
   "source": [
    "### Carregar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1f05871-0dde-46f4-a2e6-a8a54230a47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\murilo.silvestrini\\.conda\\envs\\llms\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoConfig,AutoModelForCausalLM,AutoModelForSequenceClassification,BertConfig,BertForMaskedLM,TrainingArguments, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer,BertTokenizerFast,TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3439327-6be1-45fb-a08c-33e9ecf81de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tokenizer parallelism to avoid deadlocks.\n",
    "# Set the environment variable TOKENIZERS_PARALLELISM to 'false'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9707dac9-9aaf-40c2-bcd2-beeed0168e75",
   "metadata": {},
   "source": [
    "### Pré-treinamento e ajuste fino auto-supervisionado\n",
    "\n",
    "O pré-treinamento é uma técnica usada no processamento de linguagem natural (NLP) para treinar grandes modelos de linguagem (LLMs) em um vasto corpus de dados de texto não rotulados. O objetivo é capturar os padrões gerais e relacionamentos semânticos presentes na linguagem natural, permitindo que o modelo desenvolva uma compreensão profunda da estrutura e do significado da linguagem.\n",
    "\n",
    "A motivação por trás dos transformadores de pré-treinamento é abordar as limitações das abordagens tradicionais de NLP que geralmente exigem quantidades significativas de dados rotulados para cada tarefa específica. Ao alavancar a abundância de dados de texto não rotulados, o pré-treinamento permite que o modelo aprenda habilidades fundamentais de linguagem por meio de objetivos autossupervisionados, facilitando a aprendizagem por transferência.\n",
    "\n",
    "Os objetivos de pré-treinamento, como modelagem de linguagem mascarada (MLM) e predição da próxima frase (NSP), desempenham um papel crucial no sucesso dos modelos transformadores. Os modelos pré-treinados podem ser ajustados ainda mais treinando-os em dados não rotulados específicos do domínio, o que é conhecido como ajuste fino autossupervisionado.\n",
    "\n",
    "Além disso, o modelo pode ser ajustado em tarefas downstream específicas usando dados rotulados, um processo conhecido como ajuste fino supervisionado, melhorando ainda mais seu desempenho.\n",
    "\n",
    "Nas seções a seguir deste laboratório, você explorará objetivos de pré-treinamento, carregamento de modelos pré-treinados, preparação de dados e o processo de ajuste fino. Ao final, você terá uma compreensão sólida do pré-treinamento e do ajuste fino autossupervisionado, capacitando-o a aplicar essas técnicas para resolver problemas de PNL do mundo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d79ff3-8af9-4b1b-9a53-47e9b10ef75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4d0e636ef84c82a76660b7157c6c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc6c2183ffe4bdeb166dc5964f18719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4049f610a59d4e3299c4968c1af894e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35eaaad5bbc34e5db5b698ff7e468b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be1d34fd578489dac297f7db7def9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141570d09649457a9e1dc22594db6242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8be3b22988e4d5bb6838f752e03d147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading a pretrained model from Hugging Face and making an inference\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0eac3a8-3b20-4b3a-b4e7-b2a9c7fecd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab171bf-3211-407c-9636-25f4af598849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was really good. I was really surprised by how good it was.\n",
      "I was\n"
     ]
    }
   ],
   "source": [
    "print(pipe(\"This movie was really\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7b0c6-9544-4f47-9109-9cfbec8ab406",
   "metadata": {},
   "source": [
    "### Objetivos de pré-treinamento\n",
    "\n",
    "Objetivos de pré-treinamento são componentes cruciais do processo de pré-treinamento para transformadores. Esses objetivos definem as tarefas nas quais o modelo é treinado durante a fase de pré-treinamento, permitindo que ele aprenda representações contextuais significativas da linguagem. Três objetivos de pré-treinamento comumente usados ​​são modelagem de linguagem mascarada (MLM), previsão da próxima frase (NSP) e previsão do próximo Ttoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a1066-ae40-4bc6-9bda-1d18973c3646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
