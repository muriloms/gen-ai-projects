{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f47ca6-aa21-4c17-b3d6-1bb3b82f8d4c",
   "metadata": {},
   "source": [
    "# Transformers no PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c55b9d0-ca45-438f-af21-623c83b2bef6",
   "metadata": {},
   "source": [
    "Nesta seção, vamos criar modelos de transformadores usando a biblioteca `nn.torch`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adceef7c-08f2-4475-970c-5445c5a50710",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8546184-9245-4869-896f-b6d4b6751af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "    \n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install(package, pip_name=None):\n",
    "    if pip_name is None:\n",
    "        pip_name = package\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is None:\n",
    "        print(f\"{package} não está instalado. Instalando...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "    else:\n",
    "        print(f\"{package} já está instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da05b24d-3784-4632-9c2f-51c1359f5c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_and_install('Levenshtein')\n",
    "# check_and_install('torch','torch==2.3.0')\n",
    "# check_and_install('torchtext','torchtext==0.18.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80557e7b-5fa2-4cc0-a4f5-31e2e1358b7a",
   "metadata": {},
   "source": [
    "### Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e55bf85-5dc0-4e65-b6f2-f1749646e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "\n",
    "from Levenshtein import distance\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778780f5-1f26-45ae-beb6-a7dd9072ccbd",
   "metadata": {},
   "source": [
    "### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44c5fd0-7811-4fa4-9d7d-8bdb255114f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embdings(my_embdings,name,vocab):\n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "  # Plot the data points\n",
    "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
    "\n",
    "  # Label the points\n",
    "  for j, label in enumerate(name):\n",
    "      i=vocab.get_stoi()[label]\n",
    "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
    "\n",
    "  # Set axis labels\n",
    "  ax.set_xlabel('X Label')\n",
    "  ax.set_ylabel('Y Label')\n",
    "  ax.set_zlabel('Z Label')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298d527-072b-4ff9-af70-08542fb2ebb1",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "Este bloco de código cria uma instância do modelo Transformer a partir do módulo nn (rede neural) no PyTorch. O parâmetro nhead especifica o número de cabeças no mecanismo de atenção multicabeça, que é um componente crucial da arquitetura do Transformer. Neste caso, ele é definido como 16.\n",
    "\n",
    "O parâmetro num_encoder_layers determina o número de camadas do codificador no modelo Transformer. Aqui, ele é definido como 12.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62de534-0e6a-462a-829d-7dccffd34ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6005455-ea87-4146-8042-37280760a11f",
   "metadata": {},
   "source": [
    "Essas duas linhas criam tensores aleatórios para representar as sequências de origem e destino para o modelo Transformer.\n",
    "\n",
    "`src` representa 10 sequências de origem, cada uma com um comprimento de 32 e uma dimensão de recurso de 512.\n",
    "`tgt` representa 20 sequências de destino, cada uma com um comprimento de 32 e uma dimensão de recurso de 512.\n",
    "No contexto de tarefas de sequência para sequência, as sequências de origem são os dados de entrada (por exemplo, frases em um idioma) e as sequências de destino são a saída desejada (por exemplo, as frases correspondentes em outro idioma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56603fbc-d074-4211-82ba-3240843f5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fd117-66dd-4e1e-9590-9c4967a9c914",
   "metadata": {},
   "source": [
    "Em seguida, passe os tensores de origem e destino pelo modelo Transformer. A variável out conterá a saída do modelo Transformer, que deve ter o mesmo formato que o tensor tgt ((20, 32, 512)). Essa saída pode ser processada posteriormente ou usada para tarefas posteriores, como calcular uma função de perda para treinamento ou gerar texto para inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9cd0bc8-e47f-496f-90bc-6e81520def3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f62dba-9bad-4062-88ad-eaeafc68f868",
   "metadata": {},
   "source": [
    "## Atenção MultiHead (multi-head attention)\n",
    "\n",
    "`nn.MultiheadAttention` é um módulo no PyTorch que implementa o mecanismo de autoatenção multi-head, um componente-chave da arquitetura Transformer. Este mecanismo de atenção permite que o modelo se concentre em diferentes partes da sequência de entrada simultaneamente, capturando várias dependências contextuais e melhorando a capacidade do modelo de processar padrões complexos de linguagem natural.\n",
    "\n",
    "O módulo `nn.MultiheadAttention` tem três entradas principais: `query`, `key` e `value`, conforme ilustrado abaixo.\n",
    "<p style=\"text-align:center\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/MultiHeadAttention.png\" width=\"300\" alt=\"MultiHead\"/>\n",
    "</p>\n",
    "O mecanismo de atenção multi-cabeça funciona primeiro dividindo as entradas `query`, `key` e `value` em várias \"cabeças\", cada uma com seu próprio conjunto de pesos aprendíveis. Esse processo permite que o modelo aprenda diferentes padrões de atenção em paralelo.\n",
    "\n",
    "As saídas de todas as cabeças são concatenadas e passadas por uma camada linear, conhecida como projeção de saída, para combinar as informações aprendidas por cada cabeça. Essa saída final representa a sequência contextualmente enriquecida que pode ser usada em camadas subsequentes do modelo Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa18f9d7-293e-486c-be0b-d37438fe0e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should be zero: 0\n"
     ]
    }
   ],
   "source": [
    "# Embedding dimension\n",
    "embed_dim =4\n",
    "# Number of attention heads\n",
    "num_heads = 2\n",
    "print(\"should be zero:\",embed_dim %num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e13a8cd-6ba9-43e8-a1ff-746fb2deb3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MultiheadAttention\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, \n",
    "                                       num_heads=num_heads,\n",
    "                                       batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43e8377b-6241-4321-87b3-3956b9a18505",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 10 # Sequence length\n",
    "batch_size = 5 # Batch size\n",
    "query = torch.rand((seq_length, batch_size, embed_dim))\n",
    "key = torch.rand((seq_length, batch_size, embed_dim))\n",
    "value = torch.rand((seq_length, batch_size, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cca6eb5-e025-4f3b-b1db-ec4db306ddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output Shape: torch.Size([10, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Perform multi-head attention\n",
    "attn_output, _= multihead_attn(query, key, value)\n",
    "print(\"Attention Output Shape:\", attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4ff49a-59ce-4e66-986e-43f6a4db03ef",
   "metadata": {},
   "source": [
    "### TransformerEncoderLayer e TransformerEncoder\n",
    "\n",
    "O `TransformerEncoderLayer` e o `TransformerEncoder` são componentes essenciais da arquitetura do Transformer no PyTorch. Esses componentes trabalham juntos para criar uma rede neural multicamadas baseada em atenção.\n",
    "\n",
    "### TransformerEncoderLayer:\n",
    "Esta é uma única camada de codificação na arquitetura do Transformer, consistindo em duas subcamadas primárias, conforme mostrado abaixo: a camada Multi-head Self-Attention e a Feed-Forward Network (FFN). Cada uma dessas subcamadas é seguida por uma conexão residual e normalização de camada.\n",
    "<p style=\"text-align:center\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/TrLayer.png\" width=\"200\" alt=\"TrLayer\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7b058-ff6c-49f1-bf4b-88043fc5775d",
   "metadata": {},
   "source": [
    "### TransformerEncoder:\n",
    "O TransformerEncoder é uma pilha de múltiplas instâncias `TransformerEncoderLayer`. O codificador consiste em N camadas idênticas. N pode ser ajustado com base na complexidade desejada do modelo.\n",
    "\n",
    "O codificador pega uma sequência de entrada, aplica codificação posicional e a passa por cada um dos TransformerEncoderLayers sequencialmente. Isso permite que o modelo aprenda representações ricas e hierárquicas da sequência de entrada, capturando dependências locais e de longo alcance.\n",
    "\n",
    "O TransformerEncoder aceita os seguintes parâmetros:\n",
    "- `src` (obrigatório): A sequência para o codificador.\n",
    "\n",
    "- `mask` (opcional): O parâmetro mask é usado para restringir o mecanismo de atenção de considerar certas posições na sequência de entrada. É um tensor binário com o mesmo formato da sequência de entrada. Um valor de 1 indica que a atenção é permitida, enquanto um valor de 0 indica que a atenção deve ser desconsiderada. Esta máscara é particularmente útil ao trabalhar com máscaras de atenção triangulares, onde cada posição na sequência pode atender apenas às posições anteriores.\n",
    "\n",
    "- `src_key_padding_mask` (opcional): O parâmetro src_key_padding_mask é usado para especificar quais posições na sequência de entrada correspondem aos tokens de preenchimento. É um tensor binário com formato (batch_size, sequence_length). Um valor de 1 indica que a posição correspondente contém um token válido, enquanto um valor de 0 indica que a posição contém um token de preenchimento. Ao fornecer essa máscara, o mecanismo de atenção pode ignorar tokens de preenchimento e focar apenas nas partes significativas da sequência de entrada. Esse parâmetro é particularmente útil ao lidar com sequências de comprimento variável que foram preenchidas para um comprimento fixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "344893c5-6f27-4154-b819-46a34a2ce8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimension\n",
    "embed_dim = 4\n",
    "\n",
    "# Number of attention h\n",
    "num_heads = 2\n",
    "\n",
    "# Checking if the embedding dimension is divisible by the number of heads, print(\"should be zero\", embed_dim % num_h\n",
    "# Number of encoder layers\n",
    "num_layers = 6\n",
    "\n",
    "# Initialize the encoder layer with specified embedding dimension and number of heads.\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "\n",
    "# Build the transformer encoder by stacking the encoder layer 6 times.\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0739cf90-a921-4f30-a19f-140df065827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tensor Shape: torch.Size([10, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Define sequence length as 10 and batch size as 5 for the input data.\n",
    "seq_length = 10 # Sequence length\n",
    "batch_size = 5 # Batch size\n",
    "\n",
    "# Generate random input tensor to simulate input embeddings for the transformer encoder.\n",
    "x = torch.rand((seq_length, batch_size, embed_dim))\n",
    "\n",
    "# Apply the transformer encoder to the input\n",
    "encoded = transformer_encoder(x)\n",
    "\n",
    "# Output the shape of the encoded tensor to verify the transformation.\n",
    "print(\"Encoded Tensor Shape:\", encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d1072-c772-4593-a633-6da11d4bd5b4",
   "metadata": {},
   "source": [
    "## Exercício\n",
    "Criar um transformador multi-hear e o usar para codificar um vetor de entrada. Para conseguir isso, você aproveitará nn.TransformerEncoderLayer e nn.TransformerEncoder.\n",
    "\n",
    ">Observe que o `tamanho de incorporação` deve ser divisível pelo `número de cabeças de atenção`.\n",
    "\n",
    "1. **Crie um codificador de transformador com os seguintes parâmetros:**\n",
    "- `tamanho de incorporação` = 240\n",
    "- `número de camadas` = 12\n",
    "- `número de cabeças de atenção` = 12\n",
    "\n",
    "2. **Crie um tensor de entrada aleatório de comprimento 20 e tamanho de lote de 1**\n",
    "\n",
    "3. **Passe o tensor de entrada para modelar e imprima o formato de sua saída**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6627776f-db72-4f14-beeb-e6fda076a79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tensor Shape: torch.Size([20, 1, 240])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 240\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "seq_length = 20\n",
    "batch_size = 1\n",
    "x = torch.rand((seq_length, batch_size, embed_dim))\n",
    "encoded = transformer_encoder(x)\n",
    "print(\"Encoded Tensor Shape:\", encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c7289-db9f-47dc-8afa-75042749e41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
