{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f47ca6-aa21-4c17-b3d6-1bb3b82f8d4c",
   "metadata": {},
   "source": [
    "# Autoatenção e codificação posicional\n",
    "self-attention and positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c55b9d0-ca45-438f-af21-623c83b2bef6",
   "metadata": {},
   "source": [
    "### Objetivos\n",
    "- Entenda os conceitos básicos de tokenização e como os dados textuais são preparados para modelos de rede neural.\n",
    "- Aprenda o conceito de codificação one-hot e sua aplicação na representação de dados textuais para modelos de aprendizado de máquina.\n",
    "- Explore o mecanismo de autoatenção, um componente-chave dos modelos Transformer, que permite que o modelo se concentre dinamicamente em diferentes partes da sequência de entrada para fazer previsões.\n",
    "- Implemente um mecanismo básico de autoatenção e integre-o a um modelo de rede neural.\n",
    "- Entenda a importância da codificação posicional em modelos Transformer, fornecendo ao modelo as informações necessárias sobre a ordem das palavras em uma frase.\n",
    "- Implemente a codificação posicional e observe seu efeito no desempenho do modelo e na compreensão da ordem da sequência.\n",
    "- Aplique os conceitos aprendidos para construir um modelo de tradução simples ou tarefa de processamento de texto, demonstrando a aplicação prática da autoatenção e da codificação posicional.\n",
    "- Desenvolver uma intuição para o funcionamento dos modelos NLP modernos, particularmente a arquitetura Transformer, e entender suas vantagens sobre os modelos tradicionais de processamento de sequência, como RNNs e LSTMs.\n",
    "- Incentivar maior exploração e experimentação com diferentes arquiteturas de modelos, hiperparâmetros e aplicações em processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac2596-1d29-4270-9b47-9fefe0594b82",
   "metadata": {},
   "source": [
    "### Self-attention (autoatenção)\n",
    "Self-attention é um mecanismo usado em redes neurais para permitir que o modelo foque em diferentes partes dos dados de entrada ao gerar cada parte da saída. É um componente essencial da arquitetura Transformer, amplamente utilizado em tarefas de processamento de linguagem natural (NLP), como tradução automática, sumarização de texto e análise de sentimento.\n",
    "\n",
    "A ideia por trás do self-attention é permitir que o modelo atribua pesos às partes importantes dos tokens de entrada ao gerar cada token de saída. Isso é feito computando uma soma ponderada dos tokens de entrada, onde os pesos são determinados pelas relações entre todos os pares de tokens de entrada.\n",
    "\n",
    "#### Benefícios\n",
    "- Contextualização: O modelo pode levar em consideração a relação entre palavras, mesmo que estejam distantes na sequência.\n",
    "- Paralelismo: Ao contrário de RNNs, que processam sequências de forma sequencial, o self-attention pode ser calculado em paralelo, tornando o treinamento mais rápido e eficiente.\n",
    "- Versatilidade: Pode ser usado em diversas tarefas, desde tradução de texto até tarefas de classificação.\n",
    "\n",
    "Esse mecanismo forma a base do sucesso dos Transformers, como nos modelos BERT e GPT, que se destacam por sua habilidade em capturar relacionamentos contextuais em sequências de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd836e-df42-4904-8988-2823cd95685a",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Para este laboratório, vamos usar as seguintes bibliotecas:\n",
    "\n",
    "- [`torch`](https://pytorch.org/): A biblioteca principal para construir e treinar modelos de rede neural neste projeto, incluindo a implementação de mecanismos de autoatenção e codificações posicionais.\n",
    "- [`torch.nn`](https://pytorch.org/docs/stable/nn.html), [`torch.nn.functional`](https://pytorch.org/docs/stable/nn.functional.html): Esses submódulos do PyTorch são usados ​​para definir as camadas da rede neural e aplicar funções como ativações, que são essenciais na construção da arquitetura do modelo.\n",
    "- [`Levenshtein`](https://pypi.org/project/python-Levenshtein/): Esta biblioteca é usada para calcular a distância de Levenshtein, que pode ser útil para avaliar o desempenho do modelo em tarefas como geração ou tradução de texto, medindo a diferença entre as sequências de texto previstas e reais.\n",
    "- [`get_tokenizer`](https://pytorch.org/text/stable/data_utils.html), [`build_vocab_from_iterator`](https://pytorch.org/text/stable/vocab.html) de `torchtext`: Essas funções são cruciais para o pré-processamento de dados de texto, incluindo a tokenização de texto em palavras ou subpalavras e a construção de um vocabulário a partir do conjunto de dados, que são etapas fundamentais na preparação de dados para modelos de PNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8546184-9245-4869-896f-b6d4b6751af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "    \n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install(package, pip_name=None):\n",
    "    if pip_name is None:\n",
    "        pip_name = package\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is None:\n",
    "        print(f\"{package} não está instalado. Instalando...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "    else:\n",
    "        print(f\"{package} já está instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da05b24d-3784-4632-9c2f-51c1359f5c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein não está instalado. Instalando...\n"
     ]
    }
   ],
   "source": [
    "check_and_install('Levenshtein')\n",
    "# check_and_install('torch','torch==2.3.0')\n",
    "# check_and_install('torchtext','torchtext==0.18.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80557e7b-5fa2-4cc0-a4f5-31e2e1358b7a",
   "metadata": {},
   "source": [
    "### Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e55bf85-5dc0-4e65-b6f2-f1749646e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "\n",
    "from Levenshtein import distance\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd226318-34b1-421a-b7e8-002cf99d263e",
   "metadata": {},
   "source": [
    "- **Configuração do dispositivo**: Atribuímos os cálculos a uma GPU, se disponível, caso contrário, usamos a CPU. Utilizar uma GPU pode acelerar significativamente o treinamento de modelos de aprendizado profundo. Se CUDA estiver disponível, definiremos como `cuda`, caso contrário, definiremos como `cpu`. Compute Unified Device Architecture (CUDA) é uma plataforma de computação paralela e interface de programação de aplicativos (API) que permite que o software aproveite unidades de processamento gráfico (GPUs) específicas para processamento acelerado de propósito geral, conhecido como computação de propósito geral em GPUs.\n",
    "\n",
    "- **Parâmetros de treinamento**:\n",
    "- `learning_rate`: Este é o tamanho do passo em cada iteração enquanto se move em direção a um mínimo da função de perda. Definimos como `3e-4`, que é um ponto de partida comum para muitos modelos.\n",
    "- `batch_size`: O número de amostras que serão propagadas pela rede em uma passagem para frente/para trás. Aqui, é `64`.\n",
    "- `max_iters`: O número total de iterações de treinamento que planejamos executar. Defina como `5000` para permitir que o modelo tenha ampla oportunidade de aprender com os dados.\n",
    "- `eval_interval` e `eval_iters`: Parâmetros que definem a frequência com que avaliamos o desempenho do modelo em um número definido de lotes para aproximar a perda.\n",
    "\n",
    "- **Parâmetros de arquitetura**:\n",
    "- `max_vocab_size`: Isso representa o número máximo de tokens em nosso vocabulário. Ele é definido como `256`, o que significa que consideraremos apenas os 256 tokens mais frequentes.\n",
    "- `vocab_size`: O número real de tokens no vocabulário, que pode ser menor que o máximo devido ao comprimento variável de tokens na tokenização de subpalavras como BPE (Byte Pair Encoding).\n",
    "- `block_size`: O comprimento da sequência de entrada que o modelo foi projetado para manipular. Aqui é `16`.\n",
    "- `n_embd`: O tamanho de cada vetor de incorporação, definido como `32`. As incorporações convertem tokens em um espaço contínuo onde tokens semelhantes estão mais próximos uns dos outros.\n",
    "- `num_heads`: O número de cabeças no mecanismo de autoatenção de várias cabeças, `2` neste caso, que permite que o modelo atenda conjuntamente às informações de diferentes subespaços de representação.\n",
    "- `n_layer`: O número de camadas (ou profundidade) da rede. Aqui, `2` camadas são usadas.\n",
    "- `ff_scale_factor`: Um fator de escala para o tamanho das redes feed-forward, escolhido como `4` aqui.\n",
    "- `dropout`: A taxa de dropout usada para regularização para evitar overfitting, definida como `0.0`, indicando que não há dropout neste caso.\n",
    "\n",
    "Finalmente, você tem um cálculo `head_size` que é derivado do tamanho de incorporação e do número de cabeças, garantindo que cada cabeça tenha um pedaço igual do tamanho de incorporação para trabalhar. Também incluímos uma asserção para verificar se `head_size` vezes `num_heads` é igual a `n_embd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78a77809-4fd5-4118-969c-a7178fe96bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device for training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "split = 'train'\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "max_iters = 5000              # Maximum training iterations\n",
    "eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n",
    "eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n",
    "\n",
    "# Architecture parameters\n",
    "max_vocab_size = 256          # Maximum vocabulary size\n",
    "vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
    "block_size = 16               # Context length for predictions\n",
    "n_embd = 32                   # Embedding size\n",
    "num_heads = 2                 # Number of head in multi-headed attention\n",
    "n_layer = 2                   # Number of Blocks\n",
    "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
    "dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n",
    "\n",
    "head_size = n_embd // num_heads\n",
    "assert (num_heads * head_size) == n_embd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78305d-da6a-4a54-8c06-86b60225f649",
   "metadata": {},
   "source": [
    "### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec38d9b-42d1-4dd1-bc75-ab7e3629b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function designed to visualize the learned embeddings in a #D space\n",
    "# This function helps in understanding how the embeddings cluester and separate\n",
    "# diferrent tokens, providing insight into what the model has learned\n",
    "\n",
    "def plot_embdings(my_embdings,name,vocab):\n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "  # Plot the data points\n",
    "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
    "\n",
    "  # Label the points\n",
    "  for j, label in enumerate(name):\n",
    "      i=vocab.get_stoi()[label]\n",
    "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
    "\n",
    "  # Set axis labels\n",
    "  ax.set_xlabel('X Label')\n",
    "  ax.set_ylabel('Y Label')\n",
    "  ax.set_zlabel('Z Label')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2dd545-d719-4f95-82aa-59a16afb2b9a",
   "metadata": {},
   "source": [
    "### Programa para tradução literal\n",
    "\n",
    "Na próxima parte, vamos explorar os conceitos fundamentais de tokenização e tradução por meio de um programa simples para tradução literal do francês para o inglês:\n",
    "\n",
    "- Um `dicionário` é definido, mapeando palavras francesas para seus equivalentes em inglês, formando a base da nossa lógica de tradução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9baf3f7d-680c-4d9d-957c-e18d545acac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    'le': 'the'\n",
    "    , 'chat': 'cat'\n",
    "    , 'est': 'is'\n",
    "    , 'sous': 'under'\n",
    "    , 'la': 'the'\n",
    "    , 'table': 'table'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb4aed-921a-4e46-9740-e20228c6d66a",
   "metadata": {},
   "source": [
    "- A função `tokenize` é responsável por dividir uma frase em palavras individuais.\n",
    "- A função `translate` usa esta função `tokenize` para dividir a frase de entrada e então traduz cada palavra de acordo com o dicionário. As palavras traduzidas são concatenadas para formar a frase de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81f7a865-393e-43c9-91e5-6326ca230c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split a sentence into tokens (words)\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function takes a string of text as input and returns a list of words (tokens).\n",
    "    It uses the split method, which by default splits on any whitespace, to tokenize the text.\n",
    "    \"\"\"\n",
    "    return text.split()  # Split the input text on whitespace and return the list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5259d19b-89f2-4d24-8423-f90f2f2d9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate a sentence from source to target language word by word\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    This function translates a sentence by looking up each word's translation in a predefined dictionary.\n",
    "    It assumes that every word in the sentence is a key in the dictionary.\n",
    "    \"\"\"\n",
    "    out = ''  # Initialize the output string\n",
    "    for token in tokenize(sentence):  # Tokenize the sentence into words\n",
    "        # Append the translated word to the output string\n",
    "        # This line assumes the dictionary contains a translation for every word in the input\n",
    "        out += dictionary[token] + ' '\n",
    "    return out.strip()  # Return the translated sentence, stripping any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e007944f-9872-4c56-9365-681c60edca82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0941a-21c7-4a9e-a105-a36a67ae9a34",
   "metadata": {},
   "source": [
    "Este exemplo simples ilustra uma substituição palavra por palavra que, embora não seja sofisticada, fornece uma introdução aos métodos de tradução computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55848ca4-20de-4c7d-a941-914f9078484f",
   "metadata": {},
   "source": [
    "### Melhoria: E se a 'chave' não estiver no dicionário?\n",
    "\n",
    "O código apresenta uma melhoria para o programa de tradução, abordando o cenário quando uma palavra não existe em nosso dicionário:\n",
    "\n",
    "- **Função find_closest_key**: Esta nova função tem como objetivo encontrar a chave mais próxima no dicionário para uma determinada palavra de consulta. Ela usa a **distância de Levenshtein** (uma medida da diferença entre duas sequências) para encontrar a chave do dicionário com a distância mínima para a consulta, sugerindo uma palavra semelhante se uma correspondência exata não for encontrada.\n",
    "- **Função de tradução aprimorada**: A função `translate` foi atualizada para usar `find_closest_key`. Agora, em vez de traduzir tokens diretamente com base no dicionário, ela primeiro encontra a chave mais próxima para cada palavra tokenizada. Isso permite uma tradução mais robusta, especialmente ao encontrar palavras com pequenos erros de ortografia ou variações não presentes no dicionário.\n",
    "- **Demonstração**: A função de tradução melhorada é demonstrada com a entrada \"tables\". Embora \"tables\" não esteja no dicionário, espera-se que a função encontre e use a chave mais próxima \"table\" para a tradução, gerando \"table\" em inglês.\n",
    "\n",
    "Esta melhoria demonstra uma forma simples de tratamento de erros e correspondência fuzzy em sistemas de tradução, permitindo traduções mais flexíveis e tolerantes a falhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad9c8a1-2747-47a1-9d8b-9da31e0e20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the closest key in the dictionary to the given query word\n",
    "def find_closest_key(query):\n",
    "    \"\"\"\n",
    "    The function computes the Levenshtein distance between the query and each key in the dictionary.\n",
    "    The Levenshtein distance is a measure of the number of single-character edits required to change one word into the other.\n",
    "    \"\"\"\n",
    "    closest_key, min_dist = None, float('inf')  # Initialize the closest key and minimum distance to infinity\n",
    "    for key in dictionary.keys():\n",
    "        dist = distance(query, key)  # Calculate the Levenshtein distance to the current key\n",
    "        if dist < min_dist:  # If the current distance is less than the previously found minimum\n",
    "            min_dist, closest_key = dist, key  # Update the minimum distance and the closest key\n",
    "    return closest_key  # Return the closest key found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40b1283-619c-4e13-8f99-7a0bfb598c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate a sentence from source to target language using the dictionary\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input sentence into words and finds the closest translation for each word.\n",
    "    It constructs the translated sentence by appending the translated words together.\n",
    "    \"\"\"\n",
    "    out = ''  # Initialize the output string\n",
    "    for query in tokenize(sentence):  # Tokenize the sentence into words\n",
    "        key = find_closest_key(query)  # Find the closest key in the dictionary for each word\n",
    "        out += dictionary[key] + ' '  # Append the translation of the closest key to the output string\n",
    "    return out.strip()  # Return the translated sentence, stripping any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c97f8329-9d47-4624-bdfe-8832e38c3bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'table'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9823efb-2030-47de-a557-b7a667913932",
   "metadata": {},
   "source": [
    "### Converter para rede neural\n",
    "\n",
    "Transicionando da tradução básica para redes neurais, vamos começar definindo nossos vocabulários de entrada e saída e então passar para a codificação de nossos tokens:\n",
    "\n",
    "- **Definição de vocabulário**: Dois vocabulários são criados a partir do dicionário — `vocabulary_in` para o idioma de origem (francês) e `vocabulary_out` para o idioma de destino (inglês). Esses vocabulários são as listas de palavras únicas obtidas das chaves e valores do dicionário, respectivamente, e são classificadas para manter uma ordem consistente.\n",
    "- **Codificação one-hot**: A função `encode_one_hot` é introduzida para converter cada palavra no vocabulário em um vetor codificado one-hot. A codificação one-hot é um processo em que representa cada palavra como um vetor binário com um '1' na posição correspondente ao índice da palavra no vocabulário e '0's em outros lugares. Isso cria um vetor único de tamanho fixo para cada palavra, o que é essencial para o processamento de rede neural.\n",
    "- **Demonstração de codificação**: Demonstre o processo de codificação one-hot aplicando `encode_one_hot` ao nosso vocabulário de entrada (`vocabulary_in`) e mostrando os vetores codificados para cada palavra. O mesmo processo é então aplicado ao vocabulário de saída (`vocabulary_out`).\n",
    "\n",
    "Esta etapa é crítica no aprendizado de máquina, pois prepara nossos dados textuais para entrada em uma rede neural, permitindo que ela aprenda e faça previsões sobre nossos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167fe0af-4720-4103-9242-66134518ef43",
   "metadata": {},
   "source": [
    "### Definir 'vocabularies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80cb8ba6-af06-44e5-9ddb-06fd27d88ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary input (6): ['chat', 'est', 'la', 'le', 'sous', 'table']\n",
      "Vocabulary output (5): ['cat', 'is', 'table', 'the', 'under']\n"
     ]
    }
   ],
   "source": [
    "# Create and sort the input vocabulary from the dictionary's keys\n",
    "vocabulary_in = sorted(list(set(dictionary.keys())))\n",
    "# Display the size and the sorted vocabulary for the input language\n",
    "print(f\"Vocabulary input ({len(vocabulary_in)}): {vocabulary_in}\")\n",
    "\n",
    "# Create and sort the output vocabulary from the dictionary's values\n",
    "vocabulary_out = sorted(list(set(dictionary.values())))\n",
    "# Display the size and the sorted vocabulary for the output language\n",
    "print(f\"Vocabulary output ({len(vocabulary_out)}): {vocabulary_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8f2de-7e4c-4f42-a60a-8d7b9b69742f",
   "metadata": {},
   "source": [
    "### Tokens codificados utilizando o codificador 'one hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "350f2baa-575a-4750-aa6f-75b93e7a4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a list of vocabulary words into one-hot encoded vectors\n",
    "def encode_one_hot(vocabulary):\n",
    "    vocabulary_size = len(vocabulary) # Get the size of the vocabulary\n",
    "    one_hot = dict() # Initialize a dictionary to hold our one-hot encodings\n",
    "    LEN = len(vocabulary) # the length of each one-hot encoded will be equal to the vocabulary\n",
    "\n",
    "    # Iterate over the vocabulary to create a one-hot encoded vector for each word\n",
    "    for i, key in enumerate(vocabulary):\n",
    "        one_hot_vector = torch.zeros(LEN) # Start with a vector of zeros\n",
    "        one_hot_vector[i] = 1 # Set the i-th position on 1 for the current word\n",
    "        one_hot[key] = one_hot_vector # Map the word to its one-hot encoded vector\n",
    "        print(f\"{key}\\t: {one_hot[key]}\")\n",
    "    \n",
    "    # Return the dict of words and thei one-hot encoded vectors\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff684f9e-d6ca-4bee-b51b-fe000cba163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat\t: tensor([1., 0., 0., 0., 0., 0.])\n",
      "est\t: tensor([0., 1., 0., 0., 0., 0.])\n",
      "la\t: tensor([0., 0., 1., 0., 0., 0.])\n",
      "le\t: tensor([0., 0., 0., 1., 0., 0.])\n",
      "sous\t: tensor([0., 0., 0., 0., 1., 0.])\n",
      "table\t: tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Apply the one-hot encoding function to the input vocabulary and store the result\n",
    "one_hot_in = encode_one_hot(vocabulary_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b33f347-b269-4f06-9651-2aeaed8191ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_{ chat } =  tensor([1., 0., 0., 0., 0., 0.])\n",
      "E_{ est } =  tensor([0., 1., 0., 0., 0., 0.])\n",
      "E_{ la } =  tensor([0., 0., 1., 0., 0., 0.])\n",
      "E_{ le } =  tensor([0., 0., 0., 1., 0., 0.])\n",
      "E_{ sous } =  tensor([0., 0., 0., 0., 1., 0.])\n",
      "E_{ table } =  tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the one-hot encoded input vocabulary and print each vector\n",
    "# This visualizes the one-hot representation for each word in the input vocabulary\n",
    "for k, v in one_hot_in.items():\n",
    "    print(f\"E_{{ {k} }} = \", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f59503fa-9e6f-4048-b305-5013fdf9ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\t: tensor([1., 0., 0., 0., 0.])\n",
      "is\t: tensor([0., 1., 0., 0., 0.])\n",
      "table\t: tensor([0., 0., 1., 0., 0.])\n",
      "the\t: tensor([0., 0., 0., 1., 0.])\n",
      "under\t: tensor([0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Apply the one-hot encoding function to the output vocabulary and store the result\n",
    "# This time we're encoding the target language vocabulary\n",
    "one_hot_out = encode_one_hot(vocabulary_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1048d-ce5c-4392-97d1-a067f96eb82b",
   "metadata": {},
   "source": [
    "### Vamos criar um 'dicionário' usando multiplicação de matrizes\n",
    "\n",
    "Agora estamos ilustrando como criar uma representação do nosso dicionário adequada para operações de rede neural:\n",
    "\n",
    "- **Criação de matriz**: Usando `torch.stack` do PyTorch, converta os vetores codificados one-hot para vocabulários de entrada (`K`) e saída (`V`) em tensores. `K` é construído a partir dos vetores one-hot do vocabulário de entrada, e `V` a partir dos vetores do vocabulário de saída. Esses tensores podem ser pensados ​​como uma tabela de consulta que nosso modelo usará para associar tokens de entrada com tokens de saída.\n",
    "- **Dicionário como matrizes**: Esta etapa traduz efetivamente nosso mapeamento de dicionário palavra a palavra em um formato amigável à rede neural. Cada linha em `K` corresponde a uma palavra no idioma de entrada representada como um vetor one-hot, e cada linha em `V` corresponde à respectiva palavra traduzida no idioma de saída.\n",
    "- **Exemplo de consulta**: Um exemplo mostra como usar operações de matriz para encontrar uma tradução. Procure o vetor one-hot para a palavra \"sous\" do vocabulário de entrada (`q`). Em seguida, demonstre como encontrar sua tradução correspondente realizando a multiplicação de matriz com a transposição de `K` (ou seja, `q @ K.T`) para identificar o índice e, em seguida, use esse índice para selecionar a linha relevante de `V`. Este processo imita a pesquisa que você executaria em uma rede neural real durante tarefas de tradução.\n",
    "\n",
    "Esta representação de matriz é um precursor para entender como arquiteturas de rede neural mais complexas, como aquelas que usam autoatenção, gerenciam traduções de token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c091693-b180-4994-93f3-bfd2d0e5f166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Stacking the one-hot encoded vectors for input vocabulary to form a tensor\n",
    "K = torch.stack([one_hot_in[k] for k in dictionary.keys()])\n",
    "\n",
    "# K now represents a matrix of one-hot vectors for the input vocabulary\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c623d80-d1e0-46f1-8a20-e514370368ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Similarly, stack the one-hot encoded vectors for output vocabulary to form a tensor\n",
    "V = torch.stack([one_hot_out[k] for k in dictionary.values()])\n",
    "\n",
    "# V represents the corresponding matrix of one-hot vectors for the output vocabulary\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bca56573-7c48-4643-9d65-4f3b2f9ffe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query token:  tensor([0., 0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Demostrating how to look up a translation for a given word using matrix operations\n",
    "# Here, we take the one-hot representation of 'sous' from the input vocabulary\n",
    "\n",
    "q = one_hot_in['sous']\n",
    "print('Query token: ', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f0f40df-60ca-43a3-acef-13d273bdff34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select value (V): tensor([0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Use the index found from the key selection to fing the corresponding \n",
    "#  value vector in V (output dictionary matrix)\n",
    "# This operation selects the row from V that is the translation of 'sous' in the output vocab\n",
    "\n",
    "print(\"Select value (V):\", q @ K.T @ V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e439a86b-c84a-4e95-90a4-f1b102bab9a7",
   "metadata": {},
   "source": [
    "Query vector, K matrix, and V matrix:\n",
    "\n",
    "$$\n",
    "q = \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 0 & 1 & 0\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "; \\\n",
    "K = \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 1 & 0 & 0\\\\\\\\\n",
    "  1 & 0 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 1 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  0 & 0 & 1 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 1\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "; \\\n",
    "V = \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  1 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 1 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 1\\\\\\\\\n",
    "  0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  0 & 0 & 1 & 0 & 0\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefcb90f-631d-4dcf-9598-99de2c728ede",
   "metadata": {},
   "source": [
    "The operation $q \\cdot K^T \\cdot V$ allows us to build a dictionary-like structure from a set of vectors\n",
    "\n",
    "This is an example on how to select the value from a query:\n",
    "\n",
    "$$\n",
    "q \\cdot K^T \\cdot V =\n",
    "\\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 0 & 1 & 0\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "\\cdot\n",
    "\\left[\\begin{matrix}\n",
    "  0 & 1 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 1 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  1 & 0 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 1 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 1\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "\\cdot\n",
    "\\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  1 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 1 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 1\\\\\\\\\n",
    "  0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  0 & 0 & 1 & 0 & 0\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aceff0-30b3-44b6-acc2-f136995d22f5",
   "metadata": {},
   "source": [
    "$$\n",
    "q \\cdot K^T \\cdot V =\n",
    "%\\hspace{2cm}\n",
    "\\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 1 & 0 & 0\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "%\\hspace{2.5cm}\n",
    "\\cdot\n",
    "\\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  1 & 0 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 1 & 0 & 0 & 0\\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 1\\\\\\\\\n",
    "  0 & 0 & 0 & 1 & 0\\\\\\\\\n",
    "  0 & 0 & 1 & 0 & 0\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "\\hspace{4.5cm}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9360ba9-bb7d-4740-a1c6-8eae281520b9",
   "metadata": {},
   "source": [
    "$$\n",
    "q \\cdot K^T \\cdot V\n",
    "=\n",
    "%\\hspace{3.5cm}\n",
    "\\left[\\begin{matrix}\n",
    "0 & 0 & 0 & 0 & 1\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "%\\hspace{3.5cm}\n",
    "\\hspace{9cm}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e47e53-dae9-4132-ab74-e18f10b7c170",
   "metadata": {},
   "source": [
    "O código a seguri introduz uma função para decodificar vetores one-hot para tokens e atualiza a função de tradução para utilizar multiplicação de matrizes:\n",
    "\n",
    "### Decodificar vetor one-hot (decode one-hot vector)\n",
    "A função `decode_one_hot` é projetada para decodificar um vetor codificado one-hot de volta para o token correspondente (palavra). Ela faz isso encontrando o token cuja representação one-hot tem a maior similaridade de cosseno com o vetor fornecido, que é efetivamente apenas o produto escalar devido à natureza dos vetores one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f39322b7-3d30-4072-9015-be8b6d3e8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_one_hot(one_hot, vector):\n",
    "    \"\"\" \n",
    "    Decode a one-hot encoded vector to find the best matching token in the vocabulary.\n",
    "    \"\"\"\n",
    "    best_key, best_cosine_sim = None, 0\n",
    "    for k, v in one_hot.items():  # Iterate over the one-hot encoded vocabulary\n",
    "        cosine_sim = torch.dot(vector, v)  # Calculate dot product (cosine similarity)\n",
    "        if cosine_sim > best_cosine_sim:  # If this is the best similarity we've found\n",
    "            best_cosine_sim, best_key = cosine_sim, k  # Update the best similarity and token\n",
    "    return best_key  # Return the token corresponding to the one-hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8637de5-5c7a-4643-bdff-60678628c7bc",
   "metadata": {},
   "source": [
    "### Função de tradução baseada em matriz\n",
    "A função `translate` agora alavanca operações de matriz para executar a tradução. Para cada token na frase de entrada, ela encontra seu vetor one-hot, multiplica-o pelas matrizes `K.T` e `V` para encontrar o vetor one-hot correspondente no vocabulário de saída e, em seguida, decodifica esse vetor para obter a palavra traduzida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8afd0977-5b3b-4c91-aa75-e3aa94f22272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\" \n",
    "    Translate a sentence using matrix multiplication, treating the dictionaries as matrices.\n",
    "    \"\"\"\n",
    "    sentence_out = ''  # Initialize the output sentence\n",
    "    for token_in in tokenize(sentence):  # Tokenize the input sentence\n",
    "        q = one_hot_in[token_in]  # Find the one-hot vector for the token\n",
    "        out = q @ K.T @ V  # Multiply with the input and output matrices to find the translation\n",
    "        token_out = decode_one_hot(one_hot_out, out)  # Decode the output one-hot vector to a token\n",
    "        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n",
    "    return sentence_out.strip()  # Return the translated sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0baa62-6d09-4a98-8ade-526dea1b0b7f",
   "metadata": {},
   "source": [
    "### Teste de tradução\n",
    "A função de tradução aprimorada é testada com a frase \"le chat est sous la table\", verificando se ela é traduzida corretamente para \"o gato está debaixo da mesa\" usando as operações de matriz para uma tradução perfeita palavra por palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab0d405c-d5d2-453c-adcd-ac435d20e91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ce141-aaa0-4878-8368-c412a3d55488",
   "metadata": {},
   "source": [
    "Esta abordagem aprimorada mostra como modelos de redes neurais podem traduzir idiomas representando o dicionário de tradução como matrizes e usando operações vetoriais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d4676-d414-4d6d-85be-903f043d5110",
   "metadata": {},
   "source": [
    "**O próximo segmento de código apresenta conceitos que levam à implementação de \"Atenção\" em redes neurais:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235bc4c-a0fc-4897-aeae-1bccf1fbe95f",
   "metadata": {},
   "source": [
    "### Função Softmax para similaridade\n",
    "É explicado que tokens similares terão vetores similares, e uma função softmax é adicionada à equação. Esta função é aplicada à saída da multiplicação da matriz do vetor de consulta `q` e da transposição da matriz `K`. A função softmax converte esses valores em probabilidades, enfatizando o token mais similar enquanto ainda considera os outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4213701d-ee1b-4041-81bb-aa2980ebd2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_{table} =  tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print('E_{table} = ', one_hot_in['table'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c04e4-1de6-40d4-b342-88f9c778e8f9",
   "metadata": {},
   "source": [
    "$$\n",
    "E_{table} =  \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 0 & 0 & 1\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "\\ \\ \\\n",
    "$$\n",
    "\n",
    "$$\n",
    "E_{tables} =  \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 0 & 0 & 0.95\\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597c36e-ec60-4090-a0cc-6fcacc3de93c",
   "metadata": {},
   "source": [
    "Our new equation is:\n",
    "$$\n",
    "softmax(q \\cdot K^T) \\cdot V\n",
    "$$\n",
    "\n",
    "Let's adjust using by the dimensionality of the query vector, and you'll get:\n",
    "\n",
    "$$\n",
    "softmax\\left( \\frac{q \\cdot K^T}{\\sqrt{d}} \\right) \\cdot V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c9b0c7-a6ce-4d98-a63b-9e6f9ef1089f",
   "metadata": {},
   "source": [
    "### Tradução com mecanismo de atenção\n",
    "A função `translate` é modificada para usar a função softmax como uma forma de aplicar atenção. Primeiro, ela encontra o vetor one-hot para o token, depois aplica a função softmax ao produto escalar de `q` e `K.T`, dimensiona-o pela raiz quadrada da dimensionalidade (para fins de normalização) e, finalmente, multiplica isso por `V` para obter o vetor de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29f20189-c322-4851-93d3-d32482b6a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    Translate a sentence using the attention mechanism represented by th K and V matrices\n",
    "    The softmax function is used to calculate a weighted sum of the V vectors,\n",
    "    focusing on the most relevant vector for translation\n",
    "    \"\"\"\n",
    "    sentence_out = '' # Initialize the output sentence\n",
    "    for token_in in tokenize(sentence): # Tokenize the input sentence\n",
    "        q = one_hot_in[token_in] # Get the one-hot vector for the current token\n",
    "        # Apply softmax to the scaled dot product of q and K.T, \n",
    "        # then multiply by V\n",
    "        # This selects the most relevant translation vector from V\n",
    "        out = torch.softmax(q @ K.T, dim=0) @ V\n",
    "        token_out = decode_one_hot(one_hot_out, out) # Decode the output vector to a token\n",
    "        sentence_out += token_out + ' ' # Append the translated token to the output sentence\n",
    "    \n",
    "    return sentence_out.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0af92ddf-9343-478c-94c7-9dde57e53e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bdb6d6-7e24-4785-a515-98828ec4e735",
   "metadata": {},
   "source": [
    "**Teste de tradução**: A função de tradução atualizada é testada para garantir que processe corretamente a frase de exemplo \"le chat est sous la table\", traduzindo-a para \"the cat is under the table\". Isso verifica se o mecanismo de atenção implementado usando softmax funciona conforme o esperado.\n",
    "\n",
    "Esta etapa marca a progressão da tradução simples baseada em consulta para uma abordagem baseada em atenção, apresentando um componente-chave dos modelos modernos de tradução neural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f26a2-1345-4496-8ff5-85e58b4cf451",
   "metadata": {},
   "source": [
    "**A próxima parte do código demonstra uma melhoria no processo de tradução ao manipular todas as consultas em paralelo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872860eb-7c7f-4a5b-bf79-e1998208f3c3",
   "metadata": {},
   "source": [
    "### Criando a matriz 'Q'\n",
    "A matriz `Q` é construída empilhando os vetores codificados one-hot de todos os tokens na sentença de entrada. Isso paraleliza o processo de preparação dos vetores de consulta, o que é mais eficiente do que fazê-lo sequencialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f9258b5-9ff2-4df3-ba46-6cd16cb2ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentence we wnat to translate\n",
    "sentence = \"le chat est sous la table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14aff4c8-9294-40b4-a53b-f553bdc0dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the one-hot encoded vectors for the tokens in the sentence to form the Q Matrix\n",
    "Q = torch.stack([one_hot_in[token] for token in tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b01803c6-dd37-4059-912e-c93e13c615ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70abdd7-b901-4f92-86d8-337cbfa16805",
   "metadata": {},
   "source": [
    "$$\n",
    "Q = \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 & 1 & 0 & 0 \\\\\\\\\n",
    "  1 & 0 & 0 & 0 & 0 & 0 \\\\\\\\\n",
    "  0 & 1 & 0 & 0 & 0 & 0 \\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 1 & 0 \\\\\\\\\n",
    "  0 & 0 & 1 & 0 & 0 & 0 \\\\\\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 1 \\\\\\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d45098-277f-4f94-8789-a71753771b93",
   "metadata": {},
   "source": [
    "$$\n",
    "Attention(Q, K, V) = softmax\\left( \\frac{Q \\cdot K^T}{\\sqrt{d}} \\right) V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78e3e1-c6e6-4f49-8022-5897142b2116",
   "metadata": {},
   "source": [
    "### Função translate atualizada\n",
    "A função `translate` foi revisada para usar multiplicação de matrizes em toda a frase. Em vez de traduzir palavra por palavra, agora ela usa a matriz \"Q\" para executar a operação em paralelo para todas as palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8421314e-120b-4a81-9060-04fa77f7e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    Translate a sentence using matrix multiplication in parallel.\n",
    "    This function replaces the iterative approach with a single matrix multiplication step,\n",
    "    applying the attention mechanism across all tokens at once.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence and stack the one-hot vectors to form the Q matrix\n",
    "    Q = torch.stack([one_hot_in[token] for token in tokenize(sentence)])\n",
    "    \n",
    "    # Apply softmax to the dot product of Q and K.T and multiply by V\n",
    "    # This will give us the output vectors for all tokens in parallel\n",
    "    out = torch.softmax(Q @ K.T, 0) @ V\n",
    "    \n",
    "    # Decode each one-hot vector in the output to the corresponding token\n",
    "    # And join the tokens to form the translated sentence\n",
    "    return ' '.join([decode_one_hot(one_hot_out, o) for o in out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43a65143-69d6-4be8-961b-9e43f0196cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function to ensure it produces the correct translation\n",
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c74d36-6f7b-42b8-a10b-b467a01e0011",
   "metadata": {},
   "source": [
    "- **Melhoria de eficiência**: Ao aplicar operações à frase inteira de uma vez, essa abordagem simula um aspecto essencial do mecanismo de atenção real usado em redes neurais, que é processar vários componentes de dados de entrada em paralelo para uma computação mais rápida.\n",
    "\n",
    "- **Saída do teste**: A função atualizada traduz corretamente a frase francesa \"le chat est sous la table\" para \"o gato está debaixo da mesa\", confirmando que a paralelização funciona efetivamente.\n",
    "\n",
    "Essa otimização sugere as vantagens computacionais das operações de matriz em redes neurais, particularmente para tarefas como tradução, que se beneficiam do processamento paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de135f9-194f-4e42-a307-9fed0c7d79cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
