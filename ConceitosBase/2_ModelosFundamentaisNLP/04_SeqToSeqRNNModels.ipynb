{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8c7dc0-6f15-40a8-b819-b0eda0c4c256",
   "metadata": {},
   "source": [
    "# Modelos RNN de sequência para sequência: tarefa de tradução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac586b-249d-47fd-a1bc-807554cebc22",
   "metadata": {},
   "source": [
    "Neste projeto vamos explorar os fundamentos dos modelos de sequência para sequência e implementar um modelo baseado em RNN para uma tarefa de tradução usando o PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f465beb-e562-4cfe-aadd-db92f666e1e5",
   "metadata": {},
   "source": [
    "### Objetivos\n",
    "- Compreender a arquitetura de redes neurais recorrentes (RNN)\n",
    "- Criar um modelo codificador-decodificador para uma tarefa de tradução\n",
    "- Treinar e avaliar o modelo\n",
    "- Criar um gerador para a tarefa de tradução\n",
    "- Conceitos relacionados à Perplexidade e à pontuação BLEU e usá-los para avaliar traduções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fbe087-4cd3-44dd-bbff-4fac46299584",
   "metadata": {},
   "source": [
    "### Preparar setup - instalar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08df436f-968b-459c-b4ba-c7e34412a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "    \n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install(package, pip_name=None):\n",
    "    if pip_name is None:\n",
    "        pip_name = package\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is None:\n",
    "        print(f\"{package} não está instalado. Instalando...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "    else:\n",
    "        print(f\"{package} já está instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521f0a44-ab77-4d3b-b3f5-de0e1e1356a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchtext já está instalado.\n",
      "torch já está instalado.\n",
      "spacy já está instalado.\n",
      "torchdata já está instalado.\n",
      "portalocker já está instalado.\n",
      "nltk já está instalado.\n"
     ]
    }
   ],
   "source": [
    "check_and_install('torchtext')\n",
    "check_and_install('torch')\n",
    "check_and_install('spacy')\n",
    "check_and_install('torchdata')\n",
    "check_and_install('portalocker')\n",
    "check_and_install('nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ea553c-08d0-4bf9-be0a-d30a9ea4db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d7f1e-951a-4cb8-8e92-72011aedbba1",
   "metadata": {},
   "source": [
    "### Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef9935b5-3d4f-4f24-818a-fbc548580966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9798d4-f866-4d59-947c-a77b8396eee0",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5d751-f2ad-4a48-a66e-31006d92c913",
   "metadata": {},
   "source": [
    "Os modelos de sequência para sequência (Seq2seq) revolucionaram diversas tarefas de processamento de linguagem natural (NLP), como tradução automática, sumarização de texto e chatbots. Esses modelos utilizam redes neurais recorrentes (RNNs) para processar sequências de entrada de comprimento variável e gerar sequências de saída também de comprimento variável.\n",
    "#### História dos modelos de sequência para sequência\n",
    "Os modelos seq2seq foram introduzidos como uma extensão das redes neurais feedforward tradicionais. Pesquisadores perceberam a necessidade de modelos que pudessem lidar com sequências de entrada e saída de comprimentos variáveis, como ocorre na tradução automática. O trabalho pioneiro de Sutskever et al. (2014) introduziu o uso de RNNs para modelos seq2seq.\n",
    "\n",
    "Principais objetivos dos modelos seq2seq:\n",
    "- Tradução: Traduzir uma sequência de um domínio para outro (e.g., inglês para francês).\n",
    "- Resposta a perguntas: Gerar uma resposta em linguagem natural com base em uma frase de entrada (e.g., chatbots).\n",
    "- Sumarização: Resumir um documento longo em uma sequência mais curta de frases.\n",
    "- Outras aplicações: Qualquer tarefa que envolva geração de sequência, como geração de legendas para imagens, descrições automáticas e muito mais.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5cef02-ee02-4734-b49a-c32aaa8d0b3b",
   "metadata": {},
   "source": [
    "### Introdução as RNNs\n",
    "RNNs são uma classe de redes neurais projetadas para processar dados sequenciais.\n",
    "Elas mantêm uma memória interna ($h_t$) para capturar informações de etapas anteriores e usá-las para previsões atuais.\n",
    "RNNs têm uma conexão recorrente que permite que as informações fluam de uma etapa para a próxima.\n",
    "Redes Neurais Recorrentes (RNNs) operam em sequências e utilizam estados anteriores para influenciar o estado atual. Aqui está a formulação geral de uma RNN simples:\n",
    "\n",
    "Dado:\n",
    "\n",
    "-$ \\mathbf{x}_t $: vetor de entrada no passo de tempo $t$\n",
    "\n",
    "-$ \\mathbf{h}_{t-1} $: vetor de estado oculto do passo de tempo anterior\n",
    "\n",
    "-$ \\mathbf{W}_x $ e $ \\mathbf{W}_h $: matrizes de peso para o estado de entrada e oculto, respectivamente\n",
    "\n",
    "-$ \\mathbf{b} $: vetor de polarização\n",
    "\n",
    "-$\\sigma$: função de ativação (geralmente uma sigmoide ou tanh)\n",
    "\n",
    "As equações de atualização para o estado oculto $ \\mathbf{h}_t $ e a saída $ \\mathbf{y}_t $ são as seguintes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1185a40-2a2a-449d-8aee-af50ca8d35f7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_t &= \\sigma(\\mathbf{W}_x \\cdot \\mathbf{x}_t + \\mathbf{W}_h \\cdot \\mathbf{h}_{t-1} + \\mathbf{b})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300cff8a-fabf-4527-8c43-cd34c5425d78",
   "metadata": {},
   "source": [
    "Pode ser visto que a função de estado oculto depende do estado oculto anterior, bem como da entrada no tempo t, razão pela qual ela tem uma memória coletiva de passos de tempo anteriores.\n",
    "\n",
    "Para a saída (se você estiver fazendo uma previsão em cada passo de tempo):\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{y}_t &= \\text{softmax}(\\mathbf{W}_o \\cdot \\mathbf{h}_t + \\mathbf{b}_o)\n",
    "\\end{align*}\n",
    "$$\n",
    "Onde:\n",
    "\n",
    "$ \\mathbf{W}_o $: matriz de peso para a saída E $ \\mathbf{b}_o$: vetor de viés para a saída"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3fb96-3551-4d41-a6e9-595c853d028c",
   "metadata": {},
   "source": [
    "Dependendo da tarefa específica, uma célula RNN pode produzir uma saída de $h_t$ ou transferi-la somente para a célula seguinte, servindo como memória interna. Embora a capacidade da arquitetura de reter memória possa parecer ilusória à primeira vista, vamos elucidar isso implementando uma RNN simples para manipular o seguinte mecanismo de dados:\n",
    "![a title](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Screenshot%202023-10-19%20at%2011.29.23%E2%80%AFAM.png)\n",
    "O diagrama mostra uma máquina de estados ou modelo de transição com três estados distintos, representados pelos círculos roxos proeminentes. Cada estado é distintamente rotulado com um valor para $ h $: $ h = -1 $, $ h = 0 $ e $ h = 1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243350bd-bab5-4190-beef-200077d74c72",
   "metadata": {},
   "source": [
    "1. **Estado $ h = -1 $**:\n",
    "- Mantém-se quando $ x = 1 $ (ilustrado pelo loop amarelo).\n",
    "- Prossegue para o estado $ h = 0 $ ao receber $ x = -1 $ (destacado pela seta vermelha).\n",
    "\n",
    "2. **Estado $ h = 0 $**:\n",
    "- Move-se para o estado $ h = -1 $ quando $ x = 1 $ (ilustrado pela seta vermelha).\n",
    "- Avança para o estado $ h = 1 $ com $ x = -1 $ (marcado pela seta vermelha).\n",
    "\n",
    "3. **Estado $ h = 1 $**:\n",
    "- Mantém sua posição quando $ x = -1 $ (indicado pelo loop amarelo).\n",
    "- Transita para o estado $ h = 0 $ ao receber $ x = 1 $ (representado pela seta vermelha).\n",
    "\n",
    "Para encapsular, o diagrama retrata efetivamente transições entre três estados com base na entrada $ x $. Contingente no estado predominante e na entrada $ x $, a máquina de estados transita para um estado diferente ou permanece estacionária."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee9659-205b-473b-a75a-aa2ae61ac7ae",
   "metadata": {},
   "source": [
    "Podemos representar a máquina de estados mencionada anteriormente usando a camada detalhada abaixo. Use $tanh$, pois o valor $h$ deve cair entre [-1, 1]. Note que você excluiu a saída para simplificação:\n",
    "$$\\begin{align*}\n",
    "W_{xh} & = -10.0 \\\\\n",
    "W_{hh} & = 10.0 \\\\\n",
    "b_h & = 0.0 \\\\\n",
    "x_t & = 1 \\\\\n",
    "h_{\\text{prev}} & = 0.0 \\\\\n",
    "h_t & = \\tanh(x_t \\cdot W_{xh} + h_{\\text{prev}} \\cdot W_{hh} + b_h)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6400bb-4d02-491d-8b18-8a74b999f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_xh = torch.tensor(-10.0)\n",
    "W_hh = torch.tensor(10.0)\n",
    "b_h = torch.tensor(0.0)\n",
    "x_t = 1\n",
    "h_prev = torch.tensor(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd56d0-4325-4208-8edd-a6fa7400febc",
   "metadata": {},
   "source": [
    "Considerando a seguinte sequência $x_t$ para $t=0,1,..,7$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf78738-5cb4-465e-bce3-bae7b59e82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 1, -1, -1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f71a5f-7d0b-43b7-a4bd-7bba078a58c5",
   "metadata": {},
   "source": [
    "Supondo que começamos no estado inicial $h = 0$, com o vetor de entrada $x$ acima, o vetor de estado $h$ deve ficar assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb307e5-8016-4117-9960-2aeaea1ec6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=[-1,-1,0,1,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd70e2b5-163b-4b07-97a5-bd5765a72f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  1\n",
      "h_t-1 -1\n",
      "x_t 1\n",
      "h_t -1.0\n",
      "---\n",
      "t =  2\n",
      "h_t-1 -1.0\n",
      "x_t 1\n",
      "h_t -1.0\n",
      "---\n",
      "t =  3\n",
      "h_t-1 -1.0\n",
      "x_t -1\n",
      "h_t 0.0\n",
      "---\n",
      "t =  4\n",
      "h_t-1 0.0\n",
      "x_t -1\n",
      "h_t 1.0\n",
      "---\n",
      "t =  5\n",
      "h_t-1 1.0\n",
      "x_t 1\n",
      "h_t 0.0\n",
      "---\n",
      "t =  6\n",
      "h_t-1 0.0\n",
      "x_t 1\n",
      "h_t -1.0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the predicted state values\n",
    "H_hat = []\n",
    "\n",
    "# Loop through each data point in the input sequence X\n",
    "t = 1\n",
    "for x in X:\n",
    "    # Assign the current data point to x_t\n",
    "    print(\"t = \", t)\n",
    "    x_t = x\n",
    "\n",
    "    # Print the value of the previous state (h at time t-1)\n",
    "    print(\"h_t-1\", h_prev.item())\n",
    "\n",
    "    # Compute the current state (h at time t) using the RNN formula with tanh activation\n",
    "    h_t = torch.tanh(x_t * W_xh + h_prev * W_hh + b_h)\n",
    "\n",
    "    # Update h_prev to the current state value for the next iteration\n",
    "    h_prev = h_t\n",
    "\n",
    "    # Print the current input value (x at time t)\n",
    "    print(\"x_t\", x_t)\n",
    "\n",
    "    # Print the computed state value (h at time t)\n",
    "    print(\"h_t\", h_t.item())\n",
    "    \n",
    "    # Append the current state value to the H_hat list after converting it to integer\n",
    "    H_hat.append(int(h_t.item()))\n",
    "    t += 1\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d1a18-8482-4062-bd3b-273306050b26",
   "metadata": {},
   "source": [
    "Você pode avaliar a precisão do estado previsto ```H_hat``` comparando-o ao estado real ```H```. Em RNNs, o estado $ h_t $ é utilizado para prever uma sequência de saída $y_t $ com base na sequência de entrada fornecida $ x_t $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69136aa7-a0ca-45de-bde9-516cb32d1a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 0, 1, 0, -1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0128e940-af57-49b6-b7be-b88825a4540d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 0, 1, 0, -1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6798e8-6173-4176-a916-a10eb57e2adf",
   "metadata": {},
   "source": [
    "Embora tenhamos predefinido $W_{xh}$, $W_{hh}$ e $b_h$, na prática esses valores precisam ser identificados por meio de treinamento em dados.\n",
    "\n",
    "Na prática, modificações e melhorias, como Long Short-Term Memory (LSTM) e Gated Recurrent Units (GRU), são frequentemente usadas para resolver problemas como o problema do gradiente de desaparecimento em RNNs básicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b2b81-86a4-412a-8106-6165f2127971",
   "metadata": {},
   "source": [
    "Uma célula LSTM tem três componentes principais: uma porta de entrada (input gate), uma porta de esquecimento (forget gate) e uma porta de saída (output gate).\n",
    "- A **porta de entrada** controla quanta informação nova deve ser armazenada na memória da célula. Ela olha para a entrada atual e o estado oculto anterior e decide quais partes da nova entrada lembrar.\n",
    "- A **porta de esquecimento** determina quais informações devem ser descartadas ou esquecidas da memória da célula. Ela considera a entrada atual e o estado oculto anterior e decide quais partes da memória anterior não são mais relevantes.\n",
    "- A **porta de saída** determina quais informações devem ser emitidas da célula. Ela olha para a entrada atual e o estado oculto anterior e decide quais partes da memória da célula incluir na saída.\n",
    "\n",
    "A ideia-chave por trás das células LSTM é que elas têm um estado de memória separado que pode reter ou esquecer informações seletivamente ao longo do tempo. Isso as ajuda a lidar com dependências de longo alcance e lembrar informações importantes de etapas anteriores em uma sequência."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce84e9-a2e9-4108-970d-7208cdca5296",
   "metadata": {},
   "source": [
    "### Arquitetura de sequência para sequência (sequence-to-sequence)\n",
    "\n",
    "Os modelos Seq2seq têm uma estrutura Encoder-Decoder. O codificador codifica a sequência de entrada em uma representação de dimensão fixa, geralmente chamada de vetor de contexto($h_t$). O decodificador gera a sequência de saída com base no vetor de contexto codificado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d6704-feaa-42df-a2e4-f2696ed098eb",
   "metadata": {},
   "source": [
    "Vamos olhar mais de perto as caixas codificadora e decodificadora no vídeo abaixo. A tradução é uma tarefa típica de sequência para sequência. A entrada é uma sequência de palavras no idioma original (\"Eu amo viajar\"), enquanto a saída é sua tradução no idioma de destino (\"J'adore voyager\"). Conforme mostrado no vídeo, a entrada é alimentada na parte decodificadora, uma palavra após a outra. Cada célula RNN recebe uma palavra ($x_t$) e tem uma memória interna ($h_t$). Após processar a entrada e $h_t$, a célula RNN passa um vetor de contexto atualizado ($h_{t+1}$) para a próxima célula RNN. Quando o fim da frase é alcançado, o vetor de contexto é passado para a parte decodificadora. As células decodificadoras também são células RNN que recebem o vetor de contexto e geram a saída palavra por palavra. Cada RNN recebe a palavra gerada, bem como o vetor de contexto atualizado de sua célula anterior e gera a próxima palavra ($y_t$). Essa arquitetura permite gerar texto sem restrições de comprimento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31948589-2a3d-4a38-99be-7aabb85724f2",
   "metadata": {},
   "source": [
    "<video width=\"640\" height=\"480\"\n",
    "src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Translation_RNN.mp4\"\n",
    "controls>\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b615fb3-6608-42da-b4cc-bc2e7749c5b0",
   "metadata": {},
   "source": [
    "## Implementação do codificador (Encoder) no PyTorch\n",
    "\n",
    "Para implementar a parte do codificador (encoder) usando o Pytorch, vamos criar a subclasse da classe torch.nn.Module e definirá os métodos __init__() e __forward__().\n",
    "\n",
    "Primeiro definimos os parâmetros que são usados na função __init__():\n",
    "- O `vocab_len` nada mais é do que o número de palavras únicas presentes no vocabulário. Após o pré-processamento dos dados, você pode contar o número de palavras únicas no seu vocabulário e usar essa contagem aqui. Esta será a dimensão da entrada do modelo.\n",
    "- O embedding_dim é a dimensão de saída do vetor de incorporação que você precisa. Uma boa prática é usar 256-512 para um aplicativo de demonstração de exemplo como o que você está construindo aqui.\n",
    "- O LSTM pode de fato ser empilhado, permitindo múltiplas camadas. Na implementação inicial, você usará apenas uma camada. No entanto, para acomodar a flexibilidade futura, você passará o parâmetro `n_layers` para especificar o número de camadas no LSTM.\n",
    "- `hid_dim` é a dimensionalidade dos estados oculto e de célula.\n",
    "- `dropout` é a quantidade de dropout a ser usada. Este é um parâmetro de regularização para evitar overfitting.\n",
    "\n",
    "Camadas:\n",
    "- A camada Embedding pega os dados de entrada e gera o vetor de embedding, portanto, a dimensão deles precisa ser definida como `vocab_len` e `embedding_dim`.\n",
    "- A camada LSTM pega o `embedding_dim` como os dados de entrada e cria um total de 3 saídas: `hidden`, `cell` e `output`. Aqui você precisa definir o número de neurônios que precisa no LSTM, que é definido usando o `hid_dim`.\n",
    "\n",
    "Na função __forward__(), a camada Embedding é definida, que utiliza o `vocab_len` para converter internamente o input_batch em uma representação one-hot. Em seguida, a camada LSTM recebe a entrada incorporada e produz três vetores: Output, Hidden e cell. Quanto ao codificador, você não precisa do vetor de saída do LSTM, pois você passa apenas o contexto vector(`hidden`+`cell`) para o bloco decoder. Portanto, forward() retorna apenas hidden e cell.\n",
    "\n",
    "Nota: Ao usar um LSTM, você tem um estado de célula adicional. No entanto, se você estivesse usando uma GRU, você teria apenas o estado hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5264a7-27cb-4080-acc7-8d93e89f6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, droput_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = droput_prob)\n",
    "        self.dropout = nn.Dropout(droput_prob)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        # input_batch = [src len, batch size]\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "        embed = embed.to(device)\n",
    "\n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embed)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c5ced2b-ff44-4b5a-884e-a0d721bb607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder instance\n",
    "vocab_len = 8\n",
    "emb_dim = 10\n",
    "hid_dim=8\n",
    "n_layers=1\n",
    "dropout_prob=0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc37f0-ad5d-4721-ae83-6d3d0a5520f9",
   "metadata": {},
   "source": [
    "Testar com um exemplo simples onde o método de encaminhamento do codificador transforma a frase `src` em estados `hidden` e `cell`. tensor([[0],[3],[4],[2],[1]]) é igual a `src` = 0,3,4,2,1 em que cada número representa um token no vocabulário `src`. Por exemplo, 0:`<bos>`,3:\"Das\", 4:\"ist\",2:\"schön\", 1:`<eos>`. Observe que aqui temos um tamanho de lote de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5cf70a-c5d5-42f5-a11f-6b9708158f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_batch = torch.tensor([[0,3,4,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d053fb19-32aa-45c6-a3dd-b2ef925c8906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input(src) tensor: torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# you need to transpose the input tensor as the encoder LSTM is in Sequence_first mode by default\n",
    "src_batch = src_batch.t().to(device)\n",
    "print(\"Shape of input(src) tensor:\", src_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36594b7b-8432-4c7b-9e7b-8300f132b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden tensor from encoder: tensor([[[ 0.0073,  0.2685, -0.0821, -0.1150,  0.1230, -0.0823, -0.1276,\n",
      "           0.0178]]], grad_fn=<StackBackward0>) \n",
      "Cell tensor from encoder: tensor([[[ 0.0381,  0.3959, -0.3196, -0.1924,  0.1916, -0.1427, -0.2526,\n",
      "           0.0671]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden_t , cell_t = encoder_t(src_batch)\n",
    "print(\"Hidden tensor from encoder:\",hidden_t ,\"\\nCell tensor from encoder:\", cell_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b954fd5-ab17-4ad5-b1c2-57fe75471d80",
   "metadata": {},
   "source": [
    "O codificador pega toda a sequência de origem como entrada, que consiste em uma sequência de palavras ou tokens. O codificador LSTM processa toda a sequência de entrada e atualiza seus estados ocultos a cada passo de tempo. Os estados ocultos da rede LSTM agem como uma forma de memória e capturam as informações contextuais da sequência de entrada. Após processar toda a sequência de entrada, o estado oculto final do codificador LSTM captura a representação resumida do contexto da sequência de entrada. Este estado oculto final é algumas vezes chamado de \"vetor de contexto\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4eb0ed-3501-45f4-adfa-15eb75958baa",
   "metadata": {},
   "source": [
    "### Implementação do decodificador (Decoder) no PyTorch\n",
    "Para entender melhor o mecanismo interno da parte do decodificador, vamos analisar o video abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc97bf9-78b0-49fe-a07c-5b74e10658bd",
   "metadata": {},
   "source": [
    "<video width=\"640\" height=\"480\"\n",
    "       src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/decoder_RNN.mp4\"\n",
    "       controls>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a941b-968d-4d43-8d78-f46c5e42308b",
   "metadata": {},
   "source": [
    "A classe decoder herda de nn.Module, que é uma classe base para todos os módulos de rede neural no PyTorch.\n",
    "O construtor (método __init__) inicializa os parâmetros e camadas do decodificador.\n",
    "- `output_dim` é o número de valores de saída possíveis (comprimento do vocabulário alvo).\n",
    "- `emb_dim` é a dimensionalidade da camada de incorporação.\n",
    "- `hid_dim` é a dimensionalidade do estado oculto no LSTM.\n",
    "- `n_layers` é o número de camadas no LSTM.\n",
    "- `dropout` é a probabilidade de abandono.\n",
    "\n",
    "O decodificador contém as seguintes camadas:\n",
    "- `embedding`: Uma camada de embedding que mapeia os valores de saída para vetores densos de tamanho emb_dim.\n",
    "- `lstm`: Uma camada LSTM que pega a entrada incorporada e produz estados ocultos de tamanho hid_dim.\n",
    "- `fc_out`: Uma camada linear que mapeia a saída LSTM para a dimensão de saída output_dim.\n",
    "- `softmax`: Uma função de ativação log-softmax aplicada à saída para obter uma distribuição de probabilidade sobre os valores de saída.\n",
    "- `dropout`: Uma camada de dropout que aplica dropout à entrada incorporada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03277d3f-f519-4f9d-ace9-a9cdd4e33156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "\n",
    "        #input = [batch size]\n",
    "\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        prediction_logit = self.fc_out(output.squeeze(0))\n",
    "        prediction = self.softmax(prediction_logit)\n",
    "        #prediction = [batch size, output dim]\n",
    "\n",
    "\n",
    "        return prediction, hidden, cell       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b974aa8-5562-460c-aea2-f1a3ae495f54",
   "metadata": {},
   "source": [
    "Podemos criar uma instância de decodificador. A dimensão de saída é definida como o comprimento do vocabulário de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a96cb947-abd9-45ba-8426-bd49b5070b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 6\n",
    "emb_dim=10\n",
    "hid_dim = 8\n",
    "n_layers=1\n",
    "dropout=0.5\n",
    "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9ab7d-f84d-41f0-8a92-8b4983ee4ff7",
   "metadata": {},
   "source": [
    "Agora que temos instâncias de encoder e decoder, podemos conectá-los (a caixa vermelha no diagrama abaixo). Primeiro, vamos ver como você pode passar o Hidden e o Cell (a célula rosa dentro da caixa vermelha) do encoder (o contêiner de caixas verdes) para o decoder (o contêiner de caixas laranja). Olhando para o diagrama, você pode ver que o decoder também recebe uma entrada que é a palavra anterior que ele previu. Para a primeira célula do decoder, essa entrada é o token `<bos>`. Cada célula do decoder emite uma previsão e atualiza a célula e o estado para passar para a próxima célula do decoder. A previsão é uma distribuição de probabilidade sobre possíveis tokens de destino (comprimento do vocabulário de destino).\n",
    "![connection](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/ED_connection.JPG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77c42ff4-974a-4755-b11a-4a21b6144977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tensor([[-2.1463, -1.4403, -1.8858, -1.6018, -1.9691, -1.8747]],\n",
      "       grad_fn=<LogSoftmaxBackward0>) \n",
      "\n",
      "Hidden: tensor([[[ 0.1940,  0.2322, -0.1090, -0.2850, -0.1056,  0.2018,  0.1688,\n",
      "           0.1350]]], grad_fn=<StackBackward0>) \n",
      "\n",
      "Cell: tensor([[[ 0.4253,  0.3850, -0.2375, -0.7614, -0.1726,  0.2291,  0.3454,\n",
      "           0.4532]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_t = torch.tensor([0]).to(device) #<bos>\n",
    "input_t.shape\n",
    "prediction, hidden, cell = decoder_t(input_t, hidden_t , cell_t)\n",
    "print(\"Prediction:\", prediction, '\\n\\nHidden:',hidden,'\\n\\nCell:', cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71871734-95cf-42d3-8691-1faed74a345f",
   "metadata": {},
   "source": [
    "### Conectar Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1a13d-3888-46fd-bb00-e149ef143204",
   "metadata": {},
   "source": [
    "Vimos como criar módulos codificadores e decodificadores e como passar entrada para eles. Agora precisamos criar a conexão para que o modelo possa processar pares (`src`,`trg`) e gerar a tradução. Suponha que `trg` seja tensor ([[0],[2],[3],[5],[1]]) que é igual à sequência 0,2,3,5,1 na qual cada número representa um token no vocabulário alvo. Por exemplo, 0:`<bos>`,2:\"this\", 3:\"is\",5:\"beautiful\", 1:`<eos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8451c02-6aed-44b4-86bb-890ac5965574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trg = [trg len, batch size]\n",
    "#teacher_forcing_ratio is probability to use teacher forcing\n",
    "#e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
    "teacher_forcing_ratio = 0.5\n",
    "trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e4902cc-c029-442b-a1a1-88491039f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = trg.shape[1]\n",
    "trg_len = trg.shape[0]\n",
    "trg_vocab_size = decoder_t.output_dim\n",
    "\n",
    "#tensor to store decoder outputs\n",
    "outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e347e5d-d944-439b-b43b-745cc405c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send to device\n",
    "\n",
    "hidden_t = hidden_t.to(device)\n",
    "cell_t = cell_t.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46b937b6-e6e0-4866-83b7-7baa1eaeb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first input to the decoder is the <bos> tokens\n",
    "input = trg[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdfd2a8c-46ec-44db-a8d7-8af65fc1b7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-2.1373, -1.4752, -1.8961, -1.5600, -1.9611, -1.8819]],\n",
      "\n",
      "        [[-2.1009, -1.4650, -1.8953, -1.5649, -2.0019, -1.8831]],\n",
      "\n",
      "        [[-2.0453, -1.5394, -1.8731, -1.5384, -2.0219, -1.8619]],\n",
      "\n",
      "        [[-1.9978, -1.6298, -1.7415, -1.6078, -1.9532, -1.8906]]],\n",
      "       grad_fn=<CopySlices>) torch.Size([5, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "for t in range(1, trg_len):\n",
    "\n",
    "    #you loop through the trg len and generate tokens\n",
    "    #decoder receives previous generated token, cell and hidden\n",
    "    # decoder outputs it prediction(probablity distribution for the next token) and updates hidden and cell\n",
    "    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n",
    "\n",
    "    #place predictions in a tensor holding predictions for each token\n",
    "    outputs_t[t] = output_t\n",
    "\n",
    "    #decide if you are going to use teacher forcing or not\n",
    "    teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "    #get the highest predicted token from your predictions\n",
    "    top1 = output_t.argmax(1)\n",
    "\n",
    "\n",
    "    #if teacher forcing, use actual next token as next input\n",
    "    #if not, use predicted token\n",
    "    #input = trg[t] if teacher_force else top1\n",
    "    input = trg[t] if teacher_force else top1\n",
    "\n",
    "print(outputs_t,outputs_t.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ac35e-4c3d-4102-ad1a-10fdb1e6b4ca",
   "metadata": {},
   "source": [
    "O tamanho do tensor de saída é (trg_len, batch_size, trg_vocab_size). Isso ocorre porque para cada token `trg` (comprimento de `trg`) o modelo produz uma distribuição de probabilidade sobre todos os tokens possíveis (comprimento do vocabulário trg). Portanto, para gerar os tokens previstos ou a tradução da frase `src`, você precisa obter a probabilidade máxima para cada token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa2e4ec4-288b-4ee6-9b2a-d7ff2a8a76a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# Note that you need to get the argmax from the second dimension as **outputs** is an array of **output** tensors\n",
    "pred_tokens = outputs_t.argmax(2)\n",
    "print(pred_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5ba24-d0dd-4514-ba15-4f471ee1c29c",
   "metadata": {},
   "source": [
    "Não é surpresa que a tradução não esteja correta (trg = tensor([[0],[2],[3],[5],[1]]), pois o modelo ainda não passou por nenhum treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9834aa32-1d40-4f5d-8055-fff0970145dc",
   "metadata": {},
   "source": [
    "### Implementação do modelo sequência-a-sequência (seq2seq) no PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a7501-b454-4042-9b05-6d2cf44ff70f",
   "metadata": {},
   "source": [
    "Vamos conectar os componentes do codificador e do decodificador para criar o modelo seq2seq.\n",
    "\n",
    "Você define a classe seq2seq que herda de nn.Module, que é a classe base para todos os módulos de rede neural no PyTorch.\n",
    "As entradas são:\n",
    "- `encoder` e `decoder` são instâncias das redes de codificador e decodificador que você já definiu.\n",
    "- `device` especifica o dispositivo (por exemplo, CPU ou GPU) no qual os cálculos serão realizados.\n",
    "- `trg_vocab` representa o vocabulário do idioma de destino. É usado para determinar o tamanho do vocabulário de saída."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963ba2f-a2ff-4443-b6f7-eff18a5935e9",
   "metadata": {},
   "source": [
    "O método **forward** define o passe para frente do modelo seq2seq. Ele recebe três argumentos: `src`, `trg` e `teacher_forcing_ratio`.:\n",
    "\n",
    "- `src` representa as sequências de origem e `trg` representa as sequências de destino.\n",
    "- `teacher_forcing_ratio` é uma probabilidade que determina se o forçamento do professor será usado apenas durante o treinamento. O forçamento do professor é uma técnica em que a sequência de destino verdadeira é alimentada como entrada para o decodificador em cada passo de tempo, em vez de usar a saída prevista do passo de tempo anterior.\n",
    "\n",
    "O método **forward** inicializa algumas variáveis ​​necessárias para o passe para frente, como `batch_size`, `trg_len` e `trg_vocab_size`. Ele também cria um tensor vazio chamado `outputs` para armazenar as saídas do decodificador para cada passo de tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecfe2f2-974d-44eb-92b7-080dd67e5e04",
   "metadata": {},
   "source": [
    "Os estados `hidden` e `cell` do codificador são obtidos chamando o método encoder (src). Esses estados são então usados ​​como estados iniciais para o decodificador.\n",
    "\n",
    "A entrada para o decodificador no primeiro passo de tempo é o token <bos> das sequências de destino.\n",
    "\n",
    "O decodificador é iterado para cada passo de tempo nas sequências de destino (`for t in range(1, trg_len)`). A entrada, junto com os estados hidden e cell anteriores, é passada para o decodificador e produz um tensor de saída. O tensor `output` é armazenado no tensor `outputs`.\n",
    "\n",
    "Em cada passo de tempo, há uma decisão tomada sobre usar força do professor ou não com base na probabilidade teacher_forcing_ratio. Se a força do professor for usada, o próximo token verdadeiro das sequências de destino (`trg[t]`) é usado como entrada para o próximo passo de tempo. Caso contrário, o token previsto do passo de tempo anterior (`top1 = output.argmax(1)`) é usado.\n",
    "\n",
    "Finalmente, o tensor `outputs` contendo as saídas previstas para cada passo de tempo é retornado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5d9d280-f1e1-4eeb-972c-27aea9d9df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, trg_vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        hidden = hidden.to(device)\n",
    "        cell = cell.to(device)\n",
    "\n",
    "        # First input to the decoder is the <bos> tokens\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "\n",
    "            # decide if you are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # get the highest predicted token from your predictions\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            #input = trg[t] if teacher_force else top1\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e04b6-f885-40a6-8c39-f00723bceb3b",
   "metadata": {},
   "source": [
    "### Modelo de treinamento no PyTorch\n",
    "Agora que o modelo está definido, vamos construir uma função de treinamento, o modelo seq2seq. Seus componentes:\n",
    "\n",
    "1. `train(model, iterator, optimizer, criteria, clip)` recebe cinco argumentos:\n",
    "\n",
    "- `model` é o modelo que será treinado.\n",
    "- `iterator` é um objeto iterável que fornece os dados de treinamento em lotes.\n",
    "- `optimizer` é o algoritmo de otimização usado para atualizar os parâmetros do modelo.\n",
    "- `criterion` é a função de perda que mede o desempenho do modelo.\n",
    "- `clip` é um valor usado para recortar os gradientes para evitar que eles se tornem muito grandes durante a retropropagação.\n",
    "\n",
    "2. A função começa definindo o modelo para o modo de treinamento com `model.train()`. Isso é necessário para habilitar certas camadas (por exemplo, dropout) que se comportam de forma diferente durante o treinamento e a avaliação.\n",
    "\n",
    "3. Inicializa uma variável `epoch_loss` para manter o controle da perda acumulada durante a época.\n",
    "\n",
    "4. A função itera sobre os dados de treinamento fornecidos pelo `iterator`. Cada iteração recupera um lote de sequências de entrada (`src`) e sequências de destino (`trg`).\n",
    "\n",
    "5. As sequências de entrada (`src`) e sequências de destino (`trg`) são movidas para o dispositivo apropriado (por exemplo, GPU) usando `src = src.to(device)` e `trg = trg.to(device)`.\n",
    "\n",
    "6. Os gradientes dos parâmetros do modelo são limpos usando `optimizer.zero_grad()` para preparar o novo lote.\n",
    "\n",
    "7. O modelo é então chamado com `output = model(src, trg)` para obter as previsões do modelo para as sequências de destino.\n",
    "\n",
    "8. O tensor `output` tem dimensões `[trg len, batch size, output dim]`. Para calcular a perda, o tensor é remodelado para `[trg len - 1, batch size, output dim]` para remover o token `<bos>` inicial, que não é usado para calcular a perda.\n",
    "\n",
    "9. As sequências de destino (`trg`) também são remodeladas para `[trg len - 1]` removendo o token `<bos>` inicial e tornando-o um tensor contíguo. Isso corresponde ao formato do tensor `output` remodelado.\n",
    "\n",
    "10. A perda entre os tensores `output` e `trg` remodelados é calculada usando o `criterion` especificado.\n",
    "\n",
    "11. Os gradientes da perda com relação aos parâmetros do modelo são computados usando `loss.backward()`.\n",
    "\n",
    "12. Os gradientes são então recortados para um valor máximo especificado por `clip` usando `torch.nn.utils.clip_grad_norm_(model.parameters(), clip)`. Isso evita que os gradientes se tornem muito grandes, o que pode causar problemas durante a otimização.\n",
    "\n",
    "13. O método `step()` do otimizador é chamado para atualizar os parâmetros do modelo usando os gradientes computados.\n",
    "\n",
    "14. A perda atual do lote (`loss.item()`) é adicionada à variável `epoch_loss`.\n",
    "\n",
    "15. Após todos os lotes terem sido processados, a função retorna a perda média por lote para toda a época, calculada como `epoch_loss / len(list(iterator))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9498ac5f-ddfd-473c-895b-7e82d9ea57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Wrap iterator with tqdm for progress logging\n",
    "    #train_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "\n",
    "    for i, (src,trg) in enumerate(iterator):\n",
    "\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "\n",
    "        trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tqdm progress bar with the current loss\n",
    "        #train_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "    return epoch_loss / len(list(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc019d-d3e5-40cf-be85-9e0803eefdb7",
   "metadata": {},
   "source": [
    "### Avaliando modelo no PyTorch\n",
    "Você também precisa definir uma função para avaliar o modelo. Vamos analisar o código e entender seus componentes:\n",
    "\n",
    "1. `evaluate(model, iterator, criteria)` recebe três argumentos:\n",
    "- `model` é o modelo de rede neural que será avaliado.\n",
    "- `iterator` é um objeto iterável que fornece os dados de avaliação em lotes.\n",
    "- `criterion` é a função de perda que mede o desempenho do modelo.\n",
    "* Observe que a função de avaliação não realiza nenhuma otimização no modelo.\n",
    "\n",
    "2. A função começa definindo o modelo para o modo de avaliação com `model.eval()`.\n",
    "\n",
    "3. Ela inicializa uma variável `epoch_loss` para manter o controle da perda acumulada durante a avaliação.\n",
    "\n",
    "4. A função entra em um bloco `with torch.no_grad()`, que garante que nenhum gradiente seja computado durante a avaliação. Isso economiza memória e acelera o processo de avaliação, pois os gradientes não são necessários para atualizações de parâmetros.\n",
    "\n",
    "5. A função itera sobre os dados de avaliação fornecidos pelo `iterator`. Cada iteração recupera um lote de sequências de entrada (`src`) e sequências de destino (`trg`).\n",
    "\n",
    "6. As sequências de entrada (`src`) e sequências de destino (`trg`) são movidas para o dispositivo apropriado (por exemplo, GPU) usando `src = src.to(device)` e `trg = trg.to(device)`.\n",
    "\n",
    "7. O modelo é então chamado com `output = model(src, trg, 0)` para obter as previsões do modelo para as sequências alvo. O terceiro argumento `0` é passado para indicar que o forçamento do professor é desativado durante a avaliação. Durante a avaliação, o forçamento do professor é normalmente desativado para avaliar a capacidade do modelo de gerar sequências com base em suas próprias previsões.\n",
    "\n",
    "8. O tensor `output` tem dimensões `[trg len, batch size, output dim]`. Para calcular a perda, o tensor é remodelado para `[trg len - 1, batch size, output dim]` para remover o token inicial `<bos>` (início da sequência), que não é usado para calcular a perda.\n",
    "\n",
    "9. As sequências alvo (`trg`) também são remodeladas para `[trg len - 1]` removendo o token `<bos>` inicial e tornando-o um tensor contíguo. Isso corresponde ao formato do tensor `output` remodelado.\n",
    "\n",
    "10. A perda entre os tensores `output` e `trg` remodelados é calculada usando o `criterion` especificado.\n",
    "\n",
    "11. A perda do lote atual (`loss.item()`) é adicionada à variável `epoch_loss`.\n",
    "\n",
    "12. Após todos os lotes terem sido processados, a função retorna a perda média por lote para toda a avaliação, calculada como `epoch_loss / len(list(iterator))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e84bfdba-2cc3-4d1d-af1b-5a3de2debf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Wrap iterator with tqdm for progress logging\n",
    "    valid_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (src,trg) in enumerate(iterator):\n",
    "\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "\n",
    "            trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            # Update tqdm progress bar with the current loss\n",
    "            valid_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(list(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2ae6f-c2e6-4a05-b571-251bcb781ac4",
   "metadata": {},
   "source": [
    "### Processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82a861-09d2-43cb-9b1e-6b907a5e9c69",
   "metadata": {},
   "source": [
    "Buscar um conjunto de dados de tradução de idioma chamado Multi30k, o agrupará (tokenização, numericização e adição de BOS/EOS e preenchimento) e criará lotes iteráveis de tensores src e trg.\n",
    "\n",
    "Isso aproveita o collate_fn predefinido para curar e preparar lotes de forma eficiente para treinar o modelo do transformador. O objetivo principal é se aprofundar nas complexidades dos componentes do codificador e decodificador RNN.\n",
    "\n",
    "Foi criado um arquivo \"Multi30K_de_en_dataloader.py\" que contém todos os processos de transformação em dados. Aqui, você só baixa o arquivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2462037a-979f-474c-893b-317a1773ce62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: Multi30K_de_en_dataloader.py\n"
     ]
    }
   ],
   "source": [
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'\n",
    "\n",
    "import requests\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'\n",
    "filename = 'Multi30K_de_en_dataloader.py'\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"File downloaded: {filename}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2315d706-c080-4f08-ab74-566f603cfadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d483cf69-b8a1-4b1e-adf1-072fd916f737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    print(\"Modelo carregado com sucesso!\")\n",
    "except OSError as e:\n",
    "    print(f\"Erro ao carregar o modelo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "18190e9d-d936-42ae-a1bb-7762f2d15647",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Multi30K_de_en_dataloader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f143d8d2-63ad-4a8b-9f77-bfcdae0bbac7",
   "metadata": {},
   "source": [
    "chamar a função `get_translation_dataloaders(batch_size = N,flip=True)` com um tamanho de lote arbitrário `N` e definir flip como True para que o codificador LSTM receba a sequência de entrada na ordem inversa. Isso pode ajudar no treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "da5f245e-d8f2-42c2-98dd-36820d7467a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size = 4)#,flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7ad6a0e0-2d8a-4bc7-a1ca-c415fbc14adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    2,     2,     2,     2],\n",
       "         [    3,  5510,  5510, 12642],\n",
       "         [    1,     3,     3,     8],\n",
       "         [    1,     1,     1,  1701],\n",
       "         [    1,     1,     1,     3]]),\n",
       " tensor([[   2,    2,    2,    2],\n",
       "         [   3, 6650,  216,    6],\n",
       "         [   1, 4623,  110, 3398],\n",
       "         [   1,  259, 3913,  202],\n",
       "         [   1,  172, 1650,  109],\n",
       "         [   1, 9953, 3823,   37],\n",
       "         [   1,  115,   71,    3],\n",
       "         [   1,  692, 2808,    1],\n",
       "         [   1, 3428, 2187,    1],\n",
       "         [   1,    5,    5,    1],\n",
       "         [   1,    3,    3,    1]]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the src and trg tensors\n",
    "src, trg = next(iter(train_dataloader))\n",
    "src,trg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445dd7c-d872-442f-b61a-82779e7d3190",
   "metadata": {},
   "source": [
    "Podemos obter as strings em inglês e alemão usando as funções `index_to_eng` e `index_to_german` fornecidas no arquivo .py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7adaf8e0-79c6-432d-9bf5-ab07ec388401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________\n",
      "german\n",
      "<bos> Personen mit schwarzen Hüten in der Innenstadt . <eos>\n",
      "<bos> Eine Gruppe Menschen protestiert in einer Stadt . <eos>\n",
      "<bos> Eine Gruppe teilt ihre politischen Ansichten mit . <eos>\n",
      "<bos> Mehrere Personen sitzen an einem felsigen Strand . <eos>\n",
      "________________\n",
      "english\n",
      "<bos> People in black hats gathered together downtown . <eos> <pad> <pad> <pad>\n",
      "<bos> A group of people protesting in a city . <eos> <pad> <pad>\n",
      "<bos> A group is letting their political opinion be known . <eos> <pad>\n",
      "<bos> A group of people are sitting on a rocky beach . <eos>\n",
      "________________\n",
      "german\n",
      "<bos> Zwei sitzende Personen mit Hüten und Sonnenbrillen . <eos>\n",
      "<bos> Ein kleiner Junge mit Hut beim Angeln . <eos>\n",
      "<bos> Diese zwei Frauen haben Spaß im Giorgio's . <eos>\n",
      "<bos> Zwei kleine Kinder schlafen auf dem Sofa . <eos>\n",
      "________________\n",
      "english\n",
      "<bos> Two people sitting in hats and shades . <eos> <pad> <pad> <pad>\n",
      "<bos> A young boy in a hat is fishing by himself . <eos>\n",
      "<bos> These two women is at Giorgio 's having fun . <eos> <pad>\n",
      "<bos> Two young children are asleep on a couch . <eos> <pad> <pad>\n",
      "________________\n",
      "german\n",
      "<bos> Zwei junge Mädchen marschieren in einem Umzug . <eos>\n",
      "<bos> Eine Frau läuft vor einer gestreiften Wand . <eos>\n",
      "<bos> Ein Mann fährt Jet-Ski auf dem Ozean . <eos>\n",
      "<bos> Die städtischen Straßenbahnen an einem sonnigen Tag . <eos>\n",
      "________________\n",
      "english\n",
      "<bos> Two young girls walk in a parade . <eos> <pad> <pad> <pad> <pad>\n",
      "<bos> A woman is running in front of a striped wall . <eos> <pad>\n",
      "<bos> A man rides a jet ski across the ocean . <eos> <pad> <pad>\n",
      "<bos> The urban trolly 's of a city on a sunny day . <eos>\n"
     ]
    }
   ],
   "source": [
    "data_itr = iter(train_dataloader)\n",
    "# moving forward in the dataset to reach sequences of longer length for illustration purpose. (Remember the dataset is sorted on sequence len for optimal padding)\n",
    "for n in range(1000):\n",
    "    german, english= next(data_itr)\n",
    "\n",
    "for n in range(3):\n",
    "    german, english=next(data_itr)\n",
    "    german=german.T\n",
    "    english=english.T\n",
    "    print(\"________________\")\n",
    "    print(\"german\")\n",
    "    for g in german:\n",
    "        print(index_to_german(g))\n",
    "    print(\"________________\")\n",
    "    print(\"english\")\n",
    "    for e in english:\n",
    "        print(index_to_eng(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672ad1e-c798-4f6a-9fc3-86b40c8a2080",
   "metadata": {},
   "source": [
    "* Nota: Ao trabalhar com tensores PyTorch que representam dados, é importante entender as convenções em torno da representação de sequências. Na maioria dos casos, as linhas (a primeira dimensão) em um tensor PyTorch representam amostras individuais, enquanto as colunas (a segunda dimensão) representam recursos ou etapas de tempo no caso de sequências. Ao lidar com sequências no PyTorch, é comum usar funções como `pad_sequence` para garantir que todas as sequências tenham o mesmo comprimento. Surpreendentemente, a operação de preenchimento é aplicada ao longo da segunda dimensão (colunas), embora as sequências sejam tipicamente representadas na primeira dimensão (linhas). Isso pode ser confuso no início devido à maneira como os lotes de sequências são representados. Em muitas tarefas relacionadas a sequências no PyTorch, especialmente ao trabalhar com modelos recorrentes como RNNs, LSTMs e GRUs, lotes de sequências são geralmente representados com a forma [sequence_length, batch_size, feature_size], onde `sequence_length` se refere ao comprimento da sequência mais longa dentro do lote (aqui é equivalente a `src_len` ou `trg_len`). Se você verificar o tensor src acima, poderá ver que a primeira palavra de todas as frases está na primeira linha, a segunda palavra de todas as frases está na segunda linha, etc. É por isso que a primeira dimensão é o comprimento da sequência.\n",
    "\n",
    "* Quando você usa `pad_sequence`, ele adiciona preenchimento às sequências em um lote para que todas tenham o mesmo comprimento, correspondendo ao comprimento da sequência mais longa. Como as sequências são representadas na primeira dimensão, o preenchimento é aplicado ao longo dessa dimensão. Como resultado, o tensor de saída de `pad_sequence` terá o formato [sequence_length, batch_size]. (Verifique a saída para `src` e `trg` da célula acima.) Essa convenção é comumente usada porque modelos como LSTMs esperam que os dados estejam neste formato. No entanto, se você estiver acostumado a trabalhar com dados tabulares mais tradicionais no PyTorch, isso pode causar confusão inicialmente. É importante estar ciente dessa convenção para evitar erros potenciais e entender como preparar e formatar adequadamente os dados de sequência para seus modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d27c0-476e-449b-aca2-0c27816c17db",
   "metadata": {},
   "source": [
    "### Treinar o modelo\n",
    "#### Inicializações\n",
    "Este código define a semente aleatória para várias bibliotecas e módulos. Isso é feito para tornar os resultados reproduzíveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "94ba217d-2032-4f03-937a-c7c1e8e2d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce3fc6-bb65-4c01-aac5-0f8f0fcc2d3f",
   "metadata": {},
   "source": [
    "### Treinamento\n",
    "Agora, vamos definir a instância do modelo:\n",
    "\n",
    "- `enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)`: Esta linha cria uma instância da classe `Encoder`, que representa o componente codificador do modelo Seq2Seq. A classe `Encoder` pega a dimensão de entrada, dimensão de incorporação, dimensão oculta, número de camadas e probabilidade de abandono como argumentos.\n",
    "\n",
    "- `dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)`: Esta linha cria uma instância da classe `Decoder`, que representa o componente decodificador do modelo Seq2Seq. A classe `Decoder` usa a dimensão de saída, dimensão de incorporação, dimensão oculta, número de camadas e probabilidade de abandono como argumentos.\n",
    "\n",
    "- `model = Seq2Seq(enc, dec, device,trg_vocab = vocab_transform['en']).to(device)`: Esta linha cria uma instância da classe `Seq2Seq`, que representa todo o modelo Seq2Seq. A classe `Seq2Seq` usa o codificador, o decodificador e o dispositivo (por exemplo, CPU ou GPU) como argumentos. Ela combina o codificador e o decodificador para formar a arquitetura Seq2Seq completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "23e09103-5081-459a-a9ee-a8d4917e9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform['de'])\n",
    "OUTPUT_DIM = len(vocab_transform['en'])\n",
    "ENC_EMB_DIM = 128 #256\n",
    "DEC_EMB_DIM = 128 #256\n",
    "HID_DIM = 256 #512\n",
    "N_LAYERS = 1 #2\n",
    "ENC_DROPOUT = 0.3 #0.5\n",
    "DEC_DROPOUT = 0.3 #0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device,trg_vocab = vocab_transform['en']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba8194-44a1-403e-b10e-d292a33a2512",
   "metadata": {},
   "source": [
    "`def init_weights(m)`define uma função chamada `init_weights` que recebe um módulo `m` como entrada. O propósito desta função é inicializar os pesos do módulo da rede neural.\n",
    "\n",
    "A próxima linha `for name, param in m.named_parameters():` inicia um loop que itera sobre os parâmetros nomeados do módulo `m`. Cada parâmetro é acessado como `param` e seu nome correspondente é acessado como `name`.\n",
    "\n",
    "`nn.init.uniform_(param.data, -0.08, 0.08)`inicializa os dados do parâmetro com valores extraídos de uma distribuição uniforme entre `-0.08` e `0.08`. A função `nn.init.uniform_` é fornecida pela biblioteca PyTorch e é usada para inicializar os pesos dos parâmetros da rede neural.\n",
    "\n",
    "Finalmente, `model.apply(init_weights)` aplica a função `init_weights` à instância `model`. Isso garante que os pesos de todos os parâmetros no modelo sejam inicializados usando a distribuição uniforme especificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "41b0717d-633e-4a17-8e12-092010d22d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(19214, 128)\n",
       "    (lstm): LSTM(128, 256, dropout=0.3)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(10837, 128)\n",
       "    (lstm): LSTM(128, 256, dropout=0.3)\n",
       "    (fc_out): Linear(in_features=256, out_features=10837, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (trg_vocab): Vocab()\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98423382-6b4e-462e-80bb-510f4b8f2c47",
   "metadata": {},
   "source": [
    "Este código define uma função `count_parameters` que conta o número de parâmetros treináveis ​​em um dado modelo. Ele então imprime a contagem de parâmetros treináveis ​​em uma string formatada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4f3d12b3-4462-4789-8860-1bd106911557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,422,165 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee263d0b-cfea-4cfb-9c67-95363b17c9e6",
   "metadata": {},
   "source": [
    "A célula a seguir configura o otimizador e a função de perda para treinar o modelo.\n",
    "\n",
    "1. `optimizer = optim.Adam(model.parameters())`: Esta linha cria uma instância do otimizador Adam e passa os parâmetros do modelo (`model.parameters()`) como os parâmetros a serem otimizados. O otimizador Adam é um algoritmo de otimização popular comumente usado para treinar redes neurais profundas. Ele ajusta os parâmetros do modelo com base nos gradientes computados durante a retropropagação para minimizar a função de perda.\n",
    "\n",
    "2. `PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']`: ​​Esta linha recupera o índice do token `<pad>` no vocabulário de destino.\n",
    "\n",
    "3. `criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)`: Esta linha cria uma instância do critério CrossEntropyLoss. CrossEntropyLoss é uma função de perda comumente usada para tarefas de classificação multiclasse. Nesse caso, ela é usada para treinar o modelo para prever a próxima palavra na sequência traduzida. O parâmetro `ignore_index` é definido como `PAD_IDX`, o que indica que a perda deve ser ignorada para quaisquer previsões em que o alvo seja o token de preenchimento. Isso é útil para excluir tokens de preenchimento de contribuir para a perda durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5efdfa74-cbf9-4707-b770-e31859c1e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8623bd-ec97-4a43-aae4-f878dca5f62a",
   "metadata": {},
   "source": [
    "A função auxiliar a seguir fornece uma maneira conveniente de calcular o tempo decorrido em minutos e segundos, dados os horários de início e fim. Ela será usada para medir o tempo gasto para cada época durante o treinamento ou quaisquer outros cálculos relacionados ao tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d525db71-a321-4486-8200-2a7eacc0a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b9d36-12a7-428f-9075-c1a2de8f5c41",
   "metadata": {},
   "source": [
    "Esteja ciente de que treinar o modelo usando CPUs pode ser um processo demorado. Se você não tiver acesso a GPUs, pode pular para \"carregando o modelo salvo\" e prosseguir com o carregamento do modelo pré-treinado usando o código fornecido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb706921-b7b8-4749-ae1c-7057faafb1ad",
   "metadata": {},
   "source": [
    "Vamos começar as épocas de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "91552c2a-2cf5-48bb-bae5-3c0f8d9f8861",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                               | 0/3 [13:34<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Call train function\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m train_ppl \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(train_loss)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Call evaluate function\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[110], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     13\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#trg = [trg len, batch size]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#output = [trg len, batch size, output dim]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[87], line 40\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[1;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m trg[\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, trg_len):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m#insert input token embedding, previous hidden and previous cell states\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m#receive output tensor (predictions) and new hidden and cell states\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     output, hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# place predictions in a tensor holding predictions for each token\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     outputs[t] \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[1;32m~\\.conda\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 31\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, input, hidden, cell)\u001b[0m\n\u001b[0;32m     28\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#embedded = [1, batch size, emb dim]\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#output = [seq len, batch size, hid dim * n directions]\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#hidden = [n layers * n directions, batch size, hid dim]\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#cell = [n layers * n directions, batch size, hid dim]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#hidden = [n layers, batch size, hid dim]\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#cell = [n layers, batch size, hid dim]\u001b[39;00m\n\u001b[0;32m     40\u001b[0m prediction_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\.conda\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:911\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    908\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 911\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    915\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 3  # Number of epochs\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_PPLs = []\n",
    "valid_PPLs = []\n",
    "\n",
    "# Initialize tqdm for progress logging\n",
    "progress_bar = tqdm(range(N_EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call train function\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    train_ppl = math.exp(train_loss)\n",
    "\n",
    "    # Call evaluate function\n",
    "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
    "    valid_ppl = math.exp(valid_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'RNN-TR-model.pt')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_PPLs.append(train_ppl)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_PPLs.append(valid_ppl)\n",
    "\n",
    "    # Update the description of the tqdm bar dynamically\n",
    "    progress_bar.set_description(\n",
    "        f\"Epoch {epoch+1}/{N_EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.3f}, Train PPL: {train_ppl:.3f} | \"\n",
    "        f\"Val Loss: {valid_loss:.3f}, Val PPL: {valid_ppl:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba49a4d4-578c-4f8e-a3ee-2422101289d4",
   "metadata": {},
   "source": [
    "Visualizar as perdas do modelo de treinamento e validação ao longo dos períodos de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cee75b-1955-43d0-8eb7-4a608b367fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Create a list of epoch numbers\n",
    "epochs = [epoch+1 for epoch in range(N_EPOCHS)]\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "ax1.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(epochs, valid_losses, label='Validation Loss', color='orange')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss/PPL')\n",
    "\n",
    "# Plotting the training and validation perplexity\n",
    "ax2.plot(epochs, train_PPLs, label='Train PPL', color='green')\n",
    "ax2.plot(epochs, valid_PPLs, label='Validation PPL', color='red')\n",
    "ax2.set_ylabel('Perplexity')\n",
    "\n",
    "# Adjust the y-axis scaling for PPL plot\n",
    "ax2.set_ylim(bottom=min(min(train_PPLs), min(valid_PPLs)) - 10, top=max(max(train_PPLs), max(valid_PPLs)) + 10)\n",
    "\n",
    "# Set the legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "lines = lines1 + lines2\n",
    "labels = labels1 + labels2\n",
    "ax1.legend(lines, labels, loc='upper right')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdfb7e4-61e8-43aa-b0f4-9dd697511ae0",
   "metadata": {},
   "source": [
    "### Carregando o modelo salvo\n",
    "Se você quiser pular o treinamento e carregar o modelo pré-treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "18df8b65-f1a5-4673-a236-5e8e3b80c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: RNN-TR-model.pt\n"
     ]
    }
   ],
   "source": [
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/RNN-TR-model.pt'\n",
    "filename = 'RNN-TR-model.pt'\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"File downloaded: {filename}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6859c2bb-ba22-49a7-b0e6-8461401d56fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('RNN-TR-model.pt',\n",
    "                                 map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50623be-eaec-4eb0-8658-4e1daecc5a4b",
   "metadata": {},
   "source": [
    "### Inferência de modelo\n",
    "\n",
    "Em seguida, crie uma função geradora que gere traduções para frases de fonte de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ebe86e7d-23b8-463e-a73b-bf8018f5cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_translation(model, src_sentence, src_vocab, trg_vocab, max_len=50):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_tensor = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1).to(device)\n",
    "\n",
    "        # Pass the source tensor through the encoder\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "        # Create a tensor to store the generated translation\n",
    "        # get_stoi() maps tokens to indices\n",
    "        trg_indexes = [trg_vocab.get_stoi()['<bos>']]  # Start with <bos> token\n",
    "\n",
    "        # Convert the initial token to a PyTorch tensor\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension\n",
    "\n",
    "        # Move the tensor to the same device as the model\n",
    "        trg_tensor = trg_tensor.to(model.device)\n",
    "\n",
    "\n",
    "        # Generate the translation\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            # Pass the target tensor and the previous hidden and cell states through the decoder\n",
    "            output, hidden, cell = model.decoder(trg_tensor[-1], hidden, cell)\n",
    "\n",
    "            # Get the predicted next token\n",
    "            pred_token = output.argmax(1)[-1].item()\n",
    "\n",
    "            # Append the predicted token to the translation\n",
    "            trg_indexes.append(pred_token)\n",
    "\n",
    "\n",
    "            # If the predicted token is the <eos> token, stop generating\n",
    "            if pred_token == trg_vocab.get_stoi()['<eos>']:\n",
    "                break\n",
    "\n",
    "            # Convert the predicted token to a PyTorch tensor\n",
    "            trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension\n",
    "\n",
    "            # Move the tensor to the same device as the model\n",
    "            trg_tensor = trg_tensor.to(model.device)\n",
    "\n",
    "        # Convert the generated tokens to text\n",
    "        # get_itos() maps indices to tokens\n",
    "        trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]\n",
    "\n",
    "        # Remove the <sos> and <eos> from the translation\n",
    "        if trg_tokens[0] == '<bos>':\n",
    "            trg_tokens = trg_tokens[1:]\n",
    "        if trg_tokens[-1] == '<eos>':\n",
    "            trg_tokens = trg_tokens[:-1]\n",
    "\n",
    "        # Return the translation list as a string\n",
    "\n",
    "        translation = \" \".join(trg_tokens)\n",
    "\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c6b42-b8c5-4d65-9635-6034362c0690",
   "metadata": {},
   "source": [
    "Verificar a saída do modelo para uma frase de exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "267db7c4-55eb-43e9-a3e7-a7768a17a0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Asian man is on the sidewalk .\n"
     ]
    }
   ],
   "source": [
    "# Actual translation: Asian man sweeping the walkway.\n",
    "src_sentence = 'Ein asiatischer Mann kehrt den Gehweg.'\n",
    "\n",
    "\n",
    "generated_translation = generate_translation(model, src_sentence=src_sentence, src_vocab=vocab_transform['de'], trg_vocab=vocab_transform['en'], max_len=12)\n",
    "#generated_translation = \" \".join(generated_translation_list).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "print(generated_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97b3e4-49a0-4d09-a498-2c2b4ff702c2",
   "metadata": {},
   "source": [
    "### Métrica de pontuação BLEU para avaliação\n",
    "Embora a peplexity sirva como uma métrica geral para avaliar o desempenho do modelo de linguagem na previsão do próximo token correto, a pontuação BLEU é útil para avaliar a qualidade da tradução final gerada.\n",
    "Validar os resultados usando a pontuação BLEU é útil quando há mais de uma tradução válida para uma frase, pois você pode incluir muitas versões de tradução na lista de referência e comparar a tradução gerada com diferentes versões de traduções.\n",
    "\n",
    "A pontuação BLEU (Bilingual Evaluation Understudy) é uma métrica comumente usada para avaliar a qualidade de traduções geradas por máquina, comparando-as a uma ou mais traduções de referência. Ela mede a similaridade entre a tradução gerada e as traduções de referência com base na correspondência de n-gramas.\n",
    "\n",
    "A pontuação BLEU é calculada usando as seguintes fórmulas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73163b0e-e12d-4712-a391-c61af4cc2205",
   "metadata": {},
   "source": [
    "1. **Precisão**:\n",
    "- A precisão mede a proporção de n-gramas na tradução gerada que aparecem nas traduções de referência.\n",
    "- A precisão é calculada para cada ordem de n-gramas (1 a N) e então combinada usando uma média geométrica.\n",
    "- A precisão para uma ordem de n-gramas específica é calculada como:\n",
    "   $$\\text{Precision}_n(t) = \\frac{\\text{CountClip}_n(t)}{\\text{Count}_n(t)}$$\n",
    "  onde:\n",
    "- $\\text{CountClip}_n(t)$ é a contagem de n-gramas na tradução gerada que aparecem em qualquer tradução de referência, cortada pela contagem máxima desse n-grama em qualquer tradução de referência única.\n",
    "- $\\text{Count}_n(t)$ é a contagem de n-gramas na tradução gerada.\n",
    "\n",
    "2. . **Penalidade de brevidade**:\n",
    "- A penalidade de brevidade é responsável pelo fato de que traduções mais curtas tendem a ter pontuações de precisão mais altas.\n",
    "- Ela incentiva traduções que são mais próximas em comprimento das traduções de referência.\n",
    "- A penalidade de brevidade é calculada como:\n",
    "\n",
    "$$\\text{BP} = \\begin{cases} 1 & \\text{if } c > r \\\\\\\\ e^{(1 - \\frac{r}{c})} & \\text{if } c \\leq r \\end{cases}$$\n",
    "\n",
    "onde:\n",
    "- $c$ é o comprimento total da tradução gerada.\n",
    "- $r$ é o comprimento total das traduções de referência.\n",
    "\n",
    "3. **Pontuação BLEU**:\n",
    "- A pontuação BLEU é a média geométrica das precisões, ponderada pela penalidade de brevidade.\n",
    "- É calculada como:\n",
    "\n",
    "$$\\text{BLEU} = \\text{BP} \\cdot \\exp(\\sum_{n=1}^{N}w_n \\log(\\text{Precisão}_n(t)))$$\n",
    "\n",
    "onde:\n",
    "- $N$ é a ordem máxima de n-gramas.\n",
    "- $w_n$ é o peso atribuído à precisão na ordem de n-gramas $n$, comumente definido como $\\frac{1}{N}$ para pesos iguais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "99f6eb97-cbaf-47f0-8729-c652a9bf90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(generated_translation, reference_translations):\n",
    "    # Convert the generated translations and reference translations into the expected format for sentence_bleu\n",
    "    references = [reference.split() for reference in reference_translations]\n",
    "    hypothesis = generated_translation.split()\n",
    "\n",
    "    # Calculate the BLEU score\n",
    "    bleu_score = sentence_bleu(references, hypothesis)\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e0a348e0-b639-4636-b3ba-27475e4db5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "reference_translations = [\n",
    "    \"Asian man sweeping the walkway .\",\n",
    "    \"An asian man sweeping the walkway .\",\n",
    "    \"An Asian man sweeps the sidewalk .\",\n",
    "    \"An Asian man is sweeping the sidewalk .\",\n",
    "    \"An asian man is sweeping the walkway .\",\n",
    "    \"Asian man sweeping the sidewalk .\"\n",
    "]\n",
    "\n",
    "bleu_score = calculate_bleu_score(generated_translation, reference_translations)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f5cbe443-3967-4706-9e30-b1d2e27f8cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original German text: Menschen gehen auf der Straße\n",
      "Translated English text: People are walking on the street .\n"
     ]
    }
   ],
   "source": [
    "german_text = \"Menschen gehen auf der Straße\"\n",
    "\n",
    "# The function should be defined to accept the text, the model, source and target vocabularies, and the device as parameters.\n",
    "english_translation = generate_translation(\n",
    "    model, \n",
    "    src_sentence=german_text, \n",
    "    src_vocab=vocab_transform['de'], \n",
    "    trg_vocab=vocab_transform['en'], \n",
    "    max_len=50\n",
    ")\n",
    "\n",
    "# Display the original and translated text\n",
    "print(f\"Original German text: {german_text}\")\n",
    "print(f\"Translated English text: {english_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c733f9-7002-4729-8509-aa2c63d6d891",
   "metadata": {},
   "source": [
    "____\n",
    "Esse material tem como referência o curso [Gen AI Foundational Models for NLP & Language Understanding](https://www.coursera.org/learn/gen-ai-foundational-models-for-nlp-and-language-understanding?specialization=generative-ai-engineering-with-llms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816eb324-ead0-4436-a581-bd80b2ef9d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
