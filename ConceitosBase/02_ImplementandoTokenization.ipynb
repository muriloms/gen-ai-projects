{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54c181d-ecb2-43a0-8e1c-4ae474afd7a7",
   "metadata": {},
   "source": [
    "## Implementando Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9b22f-1aa2-4796-b62c-50dcff9f769a",
   "metadata": {},
   "source": [
    "\n",
    "Tokenizadores são ferramentas essenciais no processamento de linguagem natural que dividem o texto em unidades menores chamadas tokens. Esses tokens podem ser palavras, caracteres ou subpalavras, tornando o texto complexo compreensível para computadores. Ao dividir o texto em partes gerenciáveis, os tokenizadores permitem que as máquinas processem e analisem a linguagem humana, possibilitando diversas aplicações relacionadas a linguagem, como tradução, análise de sentimentos e chatbots. Essencialmente, os tokenizadores fazem a ponte entre a linguagem humana e a compreensão da máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2e1d9-f939-42e4-a0ae-05f12a4dfc4f",
   "metadata": {},
   "source": [
    "#### 1. Instalar bibliotecas\n",
    "\n",
    "- nltk (Natural Language Toolkit): será empregada para tarefas de gerenciamento de dados. Ela oferece ferramentas e recursos abrangentes para o processamento de texto em linguagem natural, tornando-se uma escolha valiosa para tarefas como pré-processamento e análise de texto.\n",
    "\n",
    "- spaCy: uma biblioteca de software de código aberto para processamento avançado de linguagem natural em Python. O spaCy é renomado por sua velocidade e precisão no processamento de grandes volumes de dados textuais.\n",
    "\n",
    "- BertTokenizer: parte da biblioteca Transformers da Hugging Face, amplamente usada para trabalhar com modelos de linguagem pré-treinados de última geração. O BertTokenizer é especificamente projetado para tokenizar texto de acordo com as especificações do modelo BERT.\n",
    "\n",
    "- XLNetTokenizer: outro componente da biblioteca Transformers da Hugging Face, adaptado para tokenizar texto em conformidade com os requisitos do modelo XLNet.\n",
    "\n",
    "- torchtext: faz parte do ecossistema PyTorch e lida com várias tarefas de processamento de linguagem natural. Simplifica o trabalho com dados textuais e oferece funcionalidades para pré-processamento de dados, tokenização, gerenciamento de vocabulário e criação de lotes (batching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e3a179c-d65f-4cd1-b97a-4dbf06eada1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "    \n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install(package, pip_name=None):\n",
    "    if pip_name is None:\n",
    "        pip_name = package\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is None:\n",
    "        print(f\"{package} não está instalado. Instalando...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "    else:\n",
    "        print(f\"{package} já está instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aac22831-e9a5-40fd-8de5-4fb778524ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install transformers\n",
    "#!pip install sentencepiece\n",
    "#!pip install spacy\n",
    "#!pip install numpy==1.24\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download de_core_news_sm\n",
    "#!pip install numpy scikit-learn\n",
    "#!pip install torch==2.0.3\n",
    "#!pip install torchtext==0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a1705d2-73b5-4e24-a68f-dc1f34f2f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk já está instalado.\n",
      "transformers já está instalado.\n",
      "sentencepiece já está instalado.\n",
      "spacy já está instalado.\n",
      "numpy já está instalado.\n",
      "scikit-learn não está instalado. Instalando...\n",
      "torch já está instalado.\n",
      "torchtext já está instalado.\n"
     ]
    }
   ],
   "source": [
    "# Checando e instalando pacotes\n",
    "check_and_install('nltk')\n",
    "check_and_install('transformers')\n",
    "check_and_install('sentencepiece')\n",
    "check_and_install('spacy')\n",
    "check_and_install('numpy', 'numpy==1.24')\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"de_core_news_sm\"])\n",
    "check_and_install('scikit-learn')\n",
    "check_and_install('torch', 'torch==2.0.3')\n",
    "check_and_install('torchtext', 'torchtext==0.15.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0425b-0977-4443-b221-9825306ebbe8",
   "metadata": {},
   "source": [
    "#### 2. Impotar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e422cdfa-fadc-4df3-bbb5-7bd162a1d5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\murilo.silvestrini\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\murilo.silvestrini\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3110a-72f9-47dd-a838-4d1997275cae",
   "metadata": {},
   "source": [
    "#### O que é um tokenizador e por que o usamos?\n",
    "Tokenizadores desempenham um papel fundamental no processamento de linguagem natural, segmentando o texto em unidades menores conhecidas como tokens. Esses tokens são, então, transformados em representações numéricas chamadas índices de tokens, que são diretamente utilizados por algoritmos de aprendizado profundo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b081f47-fd47-47f4-8678-702808df899f",
   "metadata": {},
   "source": [
    "#### Tipos de tokenizadores\n",
    "A representação significativa pode variar dependendo do modelo utilizado. Diversos modelos empregam algoritmos de tokenização distintos, e você abordará amplamente as seguintes abordagens. Transformar texto em valores numéricos pode parecer simples à primeira vista, mas envolve várias considerações que devem ser levadas em conta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7781d-fe36-4cc5-899a-4b5498b0b624",
   "metadata": {},
   "source": [
    "#### Tokenizador baseado em palavras\n",
    "##### nltk\n",
    "Como o nome sugere, esse tipo de tokenização divide o texto com base nas palavras. Existem diferentes regras para tokenizadores baseados em palavras, como dividir pelo espaço ou pela pontuação. Cada opção atribui um ID específico a cada palavra dividida. Aqui, você usa o word_tokenize do nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49efc398-0623-4d87-b51f-7ad89ac836ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', 'for', 'word', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample sentence for word tokenization.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98078e6b-07ea-4426-9469-785e5a579f6f",
   "metadata": {},
   "source": [
    "Bibliotecas gerais como nltk e spaCy frequentemente dividem palavras como \"don't\" e \"couldn't\", que são contrações, em palavras individuais distintas. Não há uma regra universal, e cada biblioteca possui suas próprias regras de tokenização para tokenizadores baseados em palavras. No entanto, a diretriz geral é preservar o formato de entrada após a tokenização para que corresponda à forma como o modelo foi treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20990a0e-b72c-4669-a9a4-ef427519cfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154518b3-d3f3-4102-8539-eeed4bb333d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f24a5410-e33c-4ec2-8f87-0cdb8a671752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
     ]
    }
   ],
   "source": [
    "# Making a list of the tokens\n",
    "token_list = [token.text for token in doc]\n",
    "print(\"Tokens:\", token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d237148d-2c10-4c3d-9f76-31f5548f176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  | Details:  PRON nsubj\n",
      "could  | Details:  AUX aux\n",
      "n't  | Details:  PART neg\n",
      "help  | Details:  VERB ROOT\n",
      "the  | Details:  DET det\n",
      "dog  | Details:  NOUN dobj\n",
      ".  | Details:  PUNCT punct\n",
      "Ca  | Details:  AUX aux\n",
      "n't  | Details:  PART neg\n",
      "you  | Details:  PRON nsubj\n",
      "do  | Details:  VERB ROOT\n",
      "it  | Details:  PRON dobj\n",
      "?  | Details:  PUNCT punct\n",
      "Do  | Details:  AUX aux\n",
      "n't  | Details:  PART neg\n",
      "be  | Details:  AUX ROOT\n",
      "afraid  | Details:  ADJ acomp\n",
      "if  | Details:  SCONJ mark\n",
      "you  | Details:  PRON nsubj\n",
      "are  | Details:  AUX advcl\n",
      ".  | Details:  PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "# Showing token details\n",
    "for token in doc:\n",
    "    print(token.text,\" | Details: \", token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1f8cc-8731-4ea3-ab10-8d138483c5da",
   "metadata": {},
   "source": [
    "Explicação de algumas linhas:\n",
    "\n",
    "- I PRON nsubj: \"I\" é um pronome (PRON) e é o sujeito nominal (nsubj) da sentença.\n",
    "- help VERB ROOT: \"help\" é um verbo (VERB) e é a ação principal (ROOT) da sentença.\n",
    "- afraid ADJ acomp: \"afraid\" é um adjetivo (ADJ) e é um complemento adjetival (acomp), que dá mais informações sobre um estado ou qualidade relacionado ao verbo.\n",
    "\n",
    "O problema com esse algoritmo é que palavras com significados semelhantes recebem IDs diferentes, fazendo com que sejam tratadas como palavras completamente separadas, com significados distintos. Por exemplo, a forma plural de uma palavra é considerada separada da forma singular, mas um tokenizador baseado em palavras as trataria como palavras independentes, o que pode levar o modelo a perder a relação semântica entre elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbc2e412-76d8-46aa-a189-6772370da001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Unicorns are real. I saw a unicorn yesterday.\"\n",
    "token = word_tokenize(text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ef761-0445-49bc-b483-185d429bdc75",
   "metadata": {},
   "source": [
    "Cada palavra é dividida em um token, o que leva a um aumento significativo no vocabulário total do modelo. Cada token é mapeado para um grande vetor que contém os significados da palavra, resultando em parâmetros de modelo volumosos.\n",
    "\n",
    "Como as línguas geralmente possuem um grande número de palavras, os vocabulários baseados nelas sempre serão extensos. No entanto, o número de caracteres em uma língua é sempre menor em comparação com o número de palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c48b4-429f-4a6c-b220-5f2189c9315d",
   "metadata": {},
   "source": [
    "#### Tokenizador baseado em caracteres\n",
    "Como o nome sugere, a tokenização baseada em caracteres divide o texto em caracteres individuais. A vantagem desse método é que os vocabulários resultantes são inerentemente pequenos. Além disso, como as línguas possuem um conjunto limitado de caracteres, o número de tokens fora do vocabulário também é reduzido, diminuindo o desperdício de tokens.\n",
    "\n",
    "Por exemplo:\n",
    "Texto de entrada: \"This is a sample sentence for tokenization.\"\n",
    "\n",
    "Saída da tokenização baseada em caracteres:\n",
    "['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 's', 'a', 'm', 'p', 'l', 'e', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', ' ', 'f', 'o', 'r', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf165b6-750e-40b3-ac38-17c590f8dc31",
   "metadata": {},
   "source": [
    "No entanto, é importante observar que a tokenização baseada em caracteres tem suas limitações. Caracteres isolados podem não transmitir as mesmas informações que palavras inteiras, e o comprimento total dos tokens aumenta significativamente, o que pode causar problemas com o tamanho do modelo e perda de desempenho.\n",
    "\n",
    "Os transformers empregam a tokenização baseada em subpalavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a8f66-1adb-4e5f-867e-b4536faf2952",
   "metadata": {},
   "source": [
    "#### Tokenizador baseado em subpalavras\n",
    "O tokenizador baseado em subpalavras permite que palavras de uso frequente permaneçam intactas, enquanto palavras menos comuns são divididas em subpalavras significativas. Técnicas como SentencePiece ou WordPiece são amplamente utilizadas para tokenização de subpalavras. Esses métodos aprendem unidades de subpalavras a partir de um corpus de texto, identificando prefixos, sufixos e raízes comuns como tokens de subpalavras com base em sua frequência de ocorrência. Essa abordagem permite representar uma gama mais ampla de palavras e adaptar-se aos padrões específicos de linguagem de um corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c7206-0e7e-47f3-84a5-55791beb28a2",
   "metadata": {},
   "source": [
    "##### WordPiece\n",
    "Inicialmente, o WordPiece inicia seu vocabulário incluindo cada caractere presente nos dados de treinamento e, gradualmente, aprende um número especificado de regras de combinação. O WordPiece não seleciona o par de símbolos mais frequente, mas sim aquele que maximiza a probabilidade dos dados de treinamento ao ser adicionado ao vocabulário. Em essência, o WordPiece avalia o que \"sacrifica\" ao combinar dois símbolos, garantindo que seja uma escolha vantajosa.\n",
    "\n",
    "Atualmente, o tokenizador WordPiece é implementado no BertTokenizer. Observe que o BertTokenizer trata palavras compostas como tokens separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20632e0c-1413-463c-89ea-0d2c48e3bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.txt: 100%|████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 2.39MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<?, ?B/s]\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 570/570 [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aadbc3b8-44f7-427c-9e74-4d36de37f542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ibm', 'and', 'course', '##ra', 'taught', 'me', 'token', '##ization', '!']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"IBM and Coursera taught me tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e93b5-b06d-4f2c-98ce-49a140364462",
   "metadata": {},
   "source": [
    "- 'ibm': \"IBM\" é tokenizado como 'ibm'. O BERT converte tokens em letras minúsculas, pois não mantém a informação de maiúsculas/minúsculas ao usar o modelo \"bert-base-uncased\".\n",
    "- 'taught', 'me', '.': Esses tokens são iguais às palavras ou pontuação originais, apenas em letras minúsculas (exceto a pontuação).\n",
    "- 'token', '##ization': \"Tokenization\" é dividida em dois tokens. \"Token\" é uma palavra inteira, e \"##ization\" é uma parte da palavra original. O \"##\" indica que \"ization\" deve ser reconectado a \"token\" ao detokenizar (transformar tokens de volta em palavras)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03b8dc-13cf-4a4e-b7da-9db19b765066",
   "metadata": {},
   "source": [
    "#### Unigram e SentencePiece\n",
    "Unigram é um método para dividir palavras ou textos em partes menores. Ele faz isso começando com uma lista ampla de possibilidades e, em seguida, a reduz gradualmente com base na frequência dessas partes no texto. Essa abordagem facilita uma tokenização de texto eficiente.\n",
    "\n",
    "SentencePiece é uma ferramenta que divide o texto em partes menores e mais gerenciáveis, atribui IDs a esses segmentos e garante consistência. Assim, ao usar SentencePiece no mesmo texto repetidamente, você obterá os mesmos subwords e IDs de forma consistente.\n",
    "\n",
    "Unigram e SentencePiece funcionam juntos implementando o método de tokenização de subpalavras do Unigram dentro da estrutura do SentencePiece. O SentencePiece gerencia a segmentação de subpalavras e a atribuição de IDs, enquanto os princípios do Unigram orientam a redução de vocabulário para criar uma representação mais eficiente dos dados de texto. Essa combinação é especialmente valiosa para várias tarefas de NLP, nas quais a tokenização de subpalavras pode melhorar o desempenho dos modelos de linguagem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54e1c543-0018-4029-bc04-914c9ef0583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spiece.model: 100%|█████████████████████████████████████████████████████████████████| 798k/798k [00:00<00:00, 2.08MB/s]\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 760/760 [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44eb0ba6-f379-4291-96a2-78bcafa0c9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁IBM', '▁and', '▁Course', 'ra', '▁taught', '▁me', '▁token', 'ization', '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"IBM and Coursera taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae5ba6-ece2-4e64-9d24-e9d61d374448",
   "metadata": {},
   "source": [
    "- '▁IBM': O \"▁\" (frequentemente chamado de \"caractere de espaço\") antes de \"IBM\" indica que esse token é precedido por um espaço no texto original. \"IBM\" é mantido como está, pois é reconhecido como um token completo pelo XLNet, preservando a capitalização, pois você está usando o modelo \"xlnet-base-cased\".\n",
    "- '▁taught', '▁me', '▁token': Da mesma forma, esses tokens são prefixados com \"▁\" para indicar que são novas palavras precedidas por um espaço no texto original, preservando as palavras inteiras e mantendo a capitalização original.\n",
    "- 'ization': Diferente do BertTokenizer, o XLNetTokenizer não usa \"##\" para indicar tokens de subpalavras. \"ization\" aparece como seu próprio token, sem um prefixo, pois segue diretamente a palavra anterior \"token\" sem espaço no texto original.\n",
    "- '.': O ponto final é tokenizado como um token separado, uma vez que a pontuação é tratada separadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45472a-747c-4343-8a0c-7dcd4c2f0fcf",
   "metadata": {},
   "source": [
    "#### Tokenização com PyTorch\n",
    "No PyTorch, especialmente com a biblioteca torchtext, o tokenizador divide o texto de um conjunto de dados em palavras ou subpalavras individuais, facilitando sua conversão para formato numérico. Após a tokenização, o vocab (vocabulário) mapeia esses tokens para inteiros únicos, permitindo que sejam alimentados em redes neurais. Esse processo é essencial, pois modelos de aprendizado profundo operam com dados numéricos e não conseguem processar texto bruto diretamente. Assim, a tokenização e o mapeamento de vocabulário servem como uma ponte entre o texto legível para humanos e os dados numéricos operáveis pelas máquinas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27657e3c-b7dd-45b0-9396-f2fd1e92a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3f4f9e5-e819-4f99-b586-74c18fb3021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934479c-11c6-40cb-a9f4-411d585f0752",
   "metadata": {},
   "source": [
    "Na biblioteca torchtext, a função get_tokenizer é utilizada para obter um tokenizador pelo nome. Ela oferece suporte a uma variedade de métodos de tokenização, incluindo divisão básica de strings, e retorna diferentes tokenizadores com base no argumento fornecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e5db076-dc3f-44f2-82f2-016f5aa66e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "863b2b44-e1a8-4d41-b979-fdb69b87f9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', 'to', 'nlp']\n",
      "['basics', 'of', 'pytorch']\n",
      "['nlp', 'techniques', 'for', 'text', 'classification']\n",
      "['named', 'entity', 'recognition', 'with', 'pytorch']\n",
      "['sentiment', 'analysis', 'using', 'pytorch']\n",
      "['machine', 'translation', 'with', 'pytorch']\n",
      "['nlp', 'named', 'entity', ',', 'sentiment', 'analysis', ',', 'machine', 'translation']\n",
      "['machine', 'translation', 'with', 'nlp']\n",
      "['named', 'entity', 'vs', 'sentiment', 'analysis', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(dataset)):\n",
    "    print(tokenizer(dataset[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3710ea-4ba3-4fa6-8dd7-056fd90a22d2",
   "metadata": {},
   "source": [
    "#### Índices de tokens\n",
    "Palavras são representadas como números, pois algoritmos de NLP conseguem processar e manipular números de forma mais eficiente e rápida do que texto bruto. Para isso, você utiliza a função build_vocab_from_iterator, cujo resultado é normalmente chamado de \"índices de tokens\" ou simplesmente \"índices\". Esses índices representam as representações numéricas dos tokens no vocabulário.\n",
    "\n",
    "A função build_vocab_from_iterator, quando aplicada a uma lista de tokens, atribui um índice único a cada token com base em sua posição no vocabulário. Esses índices servem como uma forma de representar os tokens em um formato numérico que pode ser facilmente processado por modelos de aprendizado de máquina.\n",
    "\n",
    "Por exemplo, dado um vocabulário com os tokens [\"apple\", \"banana\", \"orange\"], os índices correspondentes poderiam ser [0, 1, 2], onde \"apple\" é representado pelo índice 0, \"banana\" pelo índice 1 e \"orange\" pelo índice 2.\n",
    "\n",
    "O dataset é um iterável. Assim, você usa uma função geradora yield_tokens para aplicar o tokenizador. O objetivo da função geradora yield_tokens é gerar textos tokenizados um de cada vez. Em vez de processar todo o conjunto de dados e retornar todos os textos tokenizados de uma só vez, a função geradora processa e gera cada texto tokenizado individualmente conforme solicitado. O processo de tokenização é realizado de forma lazy (preguiçosa), o que significa que o próximo texto tokenizado é gerado apenas quando necessário, economizando memória e recursos computacionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29034c7d-37b8-47d9-a5de-b56d13ac2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab98c91c-53ec-4456-8c2b-0314ef6f4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iterator = yield_tokens(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ee210ac-482e-4656-bcc1-2c39cb35e00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object yield_tokens at 0x000002DDF7C28BA0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc33c9-9734-4ac6-a2d8-391e85707309",
   "metadata": {},
   "source": [
    "Isso cria um iterador chamado my_iterator usando o generator. Para começar a avaliação do generator e recuperar os valores, você pode iterar sobre my_iterator usando um loop for ou recuperar valores dele usando a função next()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e399e725-c54a-4666-ab54-cffcef51ac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp', 'techniques', 'for', 'text', 'classification']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "967c972d-db5e-43fa-8921-602079ac37d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['named', 'entity', 'recognition', 'with', 'pytorch']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8251e2-c247-4151-ba65-bc36fc7313bd",
   "metadata": {},
   "source": [
    "Você constrói um vocabulário a partir dos textos tokenizados gerados pela função geradora yield_tokens, que processa o conjunto de dados. A função build_vocab_from_iterator() constrói o vocabulário, incluindo um token especial <unk> para representar palavras fora do vocabulário (out-of-vocabulary ou OOV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba94fb-f78b-498a-9d49-279afb495f28",
   "metadata": {},
   "source": [
    "#### Out-of-vocabulary (OOV)\n",
    "Ao tokenizar dados textuais, pode haver palavras que não estão presentes no vocabulário porque são raras ou não foram vistas durante o processo de construção do vocabulário. Ao encontrar essas palavras OOV durante tarefas reais de processamento de linguagem, como geração de texto ou modelagem de linguagem, o modelo pode usar o token <unk> para representá-las.\n",
    "\n",
    "Por exemplo, se a palavra \"apple\" está presente no vocabulário, mas \"pineapple\" não, \"apple\" será usada normalmente no texto, enquanto \"pineapple\" (sendo uma palavra OOV) será substituída pelo token <unk>.\n",
    "\n",
    "Ao incluir o token <unk> no vocabulário, você fornece uma maneira consistente de lidar com palavras fora do vocabulário em seu modelo de linguagem ou outras tarefas de processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2a4037c-3664-4f2b-aa1e-e172d69d8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(dataset),\n",
    "                                  specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358e454-8ec1-4a37-9236-7c7100541e94",
   "metadata": {},
   "source": [
    "Este código demonstra como buscar uma frase tokenizada de um iterador, converter seus tokens em índices usando um vocabulário fornecido e, então, imprimir a frase original e seus índices correspondentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08691aac-251b-4ab7-bf4b-167ca3439e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8370a42-42e8-4bea-9c6e-cf220299aff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence:  ['sentiment', 'analysis', 'using', 'pytorch']\n",
      "Token indices:  [7, 3, 20, 2]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "\n",
    "print(\"Tokenized sentence: \", tokenized_sentence)\n",
    "print(\"Token indices: \", token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e674af1c-88da-4a0f-bfe9-612f1bcb73ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence:  ['nlp', 'named', 'entity', ',', 'sentiment', 'analysis', ',', 'machine', 'translation']\n",
      "Token indices:  [1, 6, 4, 10, 7, 3, 10, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "\n",
    "print(\"Tokenized sentence: \", tokenized_sentence)\n",
    "print(\"Token indices: \", token_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d6e87-aa77-42ce-bb74-71ac3bf86026",
   "metadata": {},
   "source": [
    "#### Construção de vocabulário no PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "102cad7c-f2b5-4084-9898-5d61745e9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\"IBM taught me tokenization\", \n",
    "         \"Special tokenizers are ready and they will blow your mind\", \n",
    "         \"just saying hi!\"]\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c314923-b6da-460a-91a2-5221ef731c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8391ae91-e70b-4b04-8d65-09d7ff419383",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b7e24c8-946b-4a53-83c3-0390b0237012",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f493bce5-b9d0-4264-8dd7-54aa64d72d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines after adding special tokens:\n",
      " [['<bos>', 'IBM', 'taught', 'me', 'tokenization', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'Special', 'tokenizers', 'are', 'ready', 'and', 'they', 'will', 'blow', 'your', 'mind', '<eos>'], ['<bos>', 'just', 'saying', 'hi', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lines after adding special tokens:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bfbe64bf-e01e-464d-aafc-a3857ef6e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary without unk_init\n",
    "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "028b3da7-a2b3-4d8c-ada7-29b061a1e2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " ['<unk>', '<pad>', '<bos>', '<eos>', '!', 'IBM', 'Special', 'and', 'are', 'blow', 'hi', 'just', 'me', 'mind', 'ready', 'saying', 'taught', 'they', 'tokenization', 'tokenizers', 'will', 'your']\n",
      "\n",
      "Token IDs for 'tokenization':\n",
      " {'<bos>': 2, 'blow': 9, '<unk>': 0, 'and': 7, '<eos>': 3, '<pad>': 1, '!': 4, 'will': 20, 'are': 8, 'IBM': 5, 'Special': 6, 'hi': 10, 'just': 11, 'me': 12, 'mind': 13, 'ready': 14, 'saying': 15, 'taught': 16, 'they': 17, 'your': 21, 'tokenization': 18, 'tokenizers': 19}\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary and Token IDs\n",
    "print(\"Vocabulary:\\n\", vocab.get_itos())\n",
    "print(\"\\nToken IDs for 'tokenization':\\n\", vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa10b8-afff-413f-ae30-9182fa1b1f68",
   "metadata": {},
   "source": [
    "##### a. Tokens Especiais:\n",
    "\n",
    "- Token: \"<unk>\", Índice: 0 – <unk> representa \"desconhecido\" e é usado para palavras que não foram vistas durante a construção do vocabulário, geralmente durante a inferência em um novo texto.\n",
    "- Token: \"<pad>\", Índice: 1 – <pad> é um token de \"preenchimento\" usado para igualar o comprimento de sequências de palavras ao agrupá-las em batches.\n",
    "- Token: \"<bos>\", Índice: 2 – <bos> é um acrônimo de \"início da sequência\" e é usado para denotar o começo de uma sequência de texto.\n",
    "- Token: \"<eos>\", Índice: 3 – <eos> é um acrônimo de \"fim da sequência\" e é usado para denotar o final de uma sequência de texto.\n",
    "\n",
    "##### b. Tokens de Palavras:\n",
    "O restante dos tokens são palavras ou pontuações extraídas das frases fornecidas, cada uma com um índice único:\n",
    "- Token: \"IBM\", Índice: 5\n",
    "- Token: \"taught\", Índice: 16\n",
    "- Token: \"me\", Índice: 12\n",
    "… e assim por diante.\n",
    "\n",
    "##### c. Vocabulário:\n",
    "Denota o número total de tokens nas frases sobre as quais o vocabulário é construído.\n",
    "\n",
    "##### d. IDs de Tokens para 'tokenization'::\n",
    "Representa os IDs de tokens atribuídos no vocabulário, onde um número representa sua presença na sentença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d92c3173-1e73-4d4a-9fde-2789c7f55d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line = \"I learned about embeddings and attention mechanisms.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6aa745c8-07f2-43eb-bb93-614a636ab035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the new line\n",
    "tokenized_new_line = tokenizer_en(new_line)\n",
    "tokenized_new_line = ['<bos>'] + tokenized_new_line + ['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4b4c7f7-47ef-4a19-b15d-4d5dc3b3892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the new line to match the maximum length of previous lines\n",
    "new_line_padded = tokenized_new_line + ['<pad>'] * (max_length - len(tokenized_new_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8d88de46-f88a-49de-921c-88574e48ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to IDs and handle unknown words\n",
    "new_line_ids = [vocab[token] if token in vocab else vocab['<unk>'] for token in new_line_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f1c0091a-49eb-42ed-9366-59e6dfc66132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs for new line: [2, 0, 0, 0, 0, 7, 0, 0, 0, 3, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(\"Token IDs for new line:\", new_line_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b0542-1daf-45f3-9643-16dd717bef98",
   "metadata": {},
   "source": [
    "#### Tokens Especiais:\n",
    "\n",
    "- Token: \"<unk>\", Índice: 0 – <unk> significa \"desconhecido\" e representa palavras que não foram vistas durante a construção do vocabulário, geralmente durante a inferência em um novo texto.\n",
    "- Token: \"<pad>\", Índice: 1 – <pad> é um token de \"preenchimento\" usado para padronizar o comprimento das sequências de palavras ao agrupá-las em batches.\n",
    "- Token: \"<bos>\", Índice: 2 – <bos> é um acrônimo para \"início da sequência\" e é usado para marcar o início de uma sequência de texto.\n",
    "- Token: \"<eos>\", Índice: 3 – <eos> é um acrônimo para \"fim da sequência\" e é usado para marcar o final de uma sequência de texto.\n",
    "\n",
    "O token \"and\" é reconhecido na sentença e recebe o token_id de 7.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15528dd-fbe6-41d2-bf82-543a27181df1",
   "metadata": {},
   "source": [
    "____\n",
    "Esse material tem como referência o curso [Generative AI and LLMs: Architecture and Data Preparation](https://www.coursera.org/learn/generative-ai-llm-architecture-data-preparation?specialization=generative-ai-engineering-with-llms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83ee6e-c9fb-47ae-bb35-05668ae0feff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
