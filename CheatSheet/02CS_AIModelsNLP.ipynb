{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2a01e3-1bcd-4883-bc02-9142bb6fad4c",
   "metadata": {},
   "source": [
    "# Modelos de IA para NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b04f44-296d-4fc6-b13e-3ac5b10ec20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3818e-d279-452f-9578-711fda90badf",
   "metadata": {},
   "source": [
    "___\n",
    "### PyTorch/Embedding and EmbeddingBag\n",
    "[site](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html)\n",
    "\n",
    "Embedding é uma classe que representa uma camada de embedding. Ela aceita índices de token e produz vetores de embedding. EmbeddingBag é uma classe que agrega embeddings usando operações de média ou soma. Embedding e EmbeddingBag são parte do módulo torch.nn. \n",
    "\n",
    "O exemplo de código mostra como você pode usar Embedding e EmbeddingBag no PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "453a152b-5dce-468b-a43e-6252b9040650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ac59df-c854-4bd0-a96f-1ee9431b70f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a data set\n",
    "dataset = [\n",
    "\"I like cats\",\n",
    "\"I hate dogs\",\n",
    "\"I'm impartial to hippos\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69720788-f43a-4763-90c0-76b2e2251b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the tokenizer, iterator from the data set, and vocabulary\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "def yield_tokens(data_iter):\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample)\n",
    "data_iter = iter(dataset)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c52bb64-bb92-4a25-9c40-a70173f07ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 7, 2]), tensor([0, 4, 3]), tensor([0, 1, 6, 8, 5])]\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing and generating indices\n",
    "input_ids=lambda x:[torch.tensor(vocab(tokenizer(data_sample))) for data_sample in dataset]\n",
    "index=input_ids(dataset)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c52ac71e-d27c-404b-8006-9e27d2150af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating the embedding layer, specifying the dimension size for the embeddings, \n",
    "#determining the count of unique tokens present in the vocabulary, and creating the embedding layer\n",
    "embedding_dim = 3\n",
    "n_embedding = len(vocab)\n",
    "n_embedding:9\n",
    "embeds = nn.Embedding(n_embedding, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e9f596e-5521-47d6-8ec0-ef1e6970cd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4136, -0.1322, -0.8446],\n",
       "        [ 1.0880, -1.7253, -0.0407],\n",
       "        [-0.2187, -1.1263, -1.0967],\n",
       "        [ 1.1725,  0.0905,  1.0090],\n",
       "        [ 0.5685, -1.2016,  1.2934]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying the embedding object\n",
    "i_like_cats=embeds(index[0])\n",
    "i_like_cats\n",
    "impartial_to_hippos=embeds(index[-1])\n",
    "impartial_to_hippos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bead74d-e009-4305-ae51-dde225e7a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the embedding bag layer\n",
    "embedding_dim = 3\n",
    "n_embedding = len(vocab)\n",
    "n_embedding:9\n",
    "embedding_bag = nn.EmbeddingBag(n_embedding, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "185d2741-9f48-486a-a44f-47465b1186d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0653, -0.2848, -1.0250]], grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the embedding bag\n",
    "dataset = [\"I like cats\",\"I hate dogs\",\"I'm impartial to hippos\"]\n",
    "i_like_cats=embedding_bag(index[0],offsets=torch.tensor([0]))\n",
    "i_like_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a556578-1c8c-44a5-ae97-e15f416fc466",
   "metadata": {},
   "source": [
    "___\n",
    "### Batch function\n",
    "\n",
    "Define o número de amostras que serão propagadas pela rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ab1f7ef-680f-402c-b021-436c01e34d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d4a438b-d682-47f1-bd81-46f3deabb530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    target_list, context_list, offsets = [], [], [0]\n",
    "    for _context, _target in batch:\n",
    "        target_list.append(vocab[_target]) \n",
    "        processed_context = torch.tensor(text_pipeline(_context), dtype=torch.int64)\n",
    "        context_list.append(processed_context)\n",
    "        offsets.append(processed_context.size(0))\n",
    "        target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "        context_list = torch.cat(context_list)\n",
    "    return target_list.to(device), context_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be5c03b1-be4a-4fda-9437-63385cbddf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = \"\"\"I wish I was little bit taller\n",
    "I wish I was a baller\n",
    "She wore a small black dress to the party\n",
    "The dog chased a big red ball in the park\n",
    "He had a huge smile on his face when he won the race\n",
    "The tiny kitten played with a fluffy toy mouse\n",
    "The team celebrated their victory with a grand parade\n",
    "She bought a small, delicate necklace for her sister\n",
    "The mountain peak stood majestic and tall against the clear blue sky\n",
    "The toddler took small, careful steps as she learned to walk\n",
    "The house had a spacious backyard with a big swimming pool\n",
    "He felt a sense of accomplishment after completing the challenging puzzle\n",
    "The chef prepared a delicious, flavorful dish using fresh ingredients\n",
    "The children played happily in the small, cozy room\n",
    "The book had an enormous impact on readers around the world\n",
    "The wind blew gently, rustling the leaves of the tall trees\n",
    "She painted a beautiful, intricate design on the small canvas\n",
    "The concert hall was filled with thousands of excited fans\n",
    "The garden was adorned with colorful flowers of all sizes\n",
    "I hope to achieve great success in my chosen career path\n",
    "The skyscraper towered above the city, casting a long shadow\n",
    "He gazed in awe at the breathtaking view from the mountaintop\n",
    "The artist created a stunning masterpiece with bold brushstrokes\n",
    "The baby took her first steps, a small milestone that brought joy to her parents\n",
    "The team put in a tremendous amount of effort to win the championship\n",
    "The sun set behind the horizon, painting the sky in vibrant colors\n",
    "The professor gave a fascinating lecture on the history of ancient civilizations\n",
    "The house was filled with laughter and the sound of children playing\n",
    "She received a warm, enthusiastic welcome from the audience\n",
    "The marathon runner had incredible endurance and determination\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93d11890-3c25-4ff2-a2c7-c186be88350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def tokenize_data(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "tokenized_toy_data = tokenizer(toy_data)\n",
    "\n",
    "# slide over the sequence and create training data:\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "cobow_data = []\n",
    "for i in range(1, len(tokenized_toy_data) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [tokenized_toy_data[i - j -1] for j in range(CONTEXT_SIZE)]\n",
    "        + [tokenized_toy_data[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = tokenized_toy_data[i]\n",
    "    cobow_data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "219f3866-cc22-4013-8b51-d73cf8b9b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # batch size for training\n",
    "dataloader_cbow = DataLoader(cobow_data, \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             shuffle=True, \n",
    "                             collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a12dfa-a571-4156-959d-c21fe0ba166c",
   "metadata": {},
   "source": [
    "___\n",
    "### Passe para frente (Forward pass)\n",
    "\n",
    "Refere-se ao cálculo e armazenamento de variáveis ​​intermediárias (incluindo saídas) para uma rede neural, da camada de entrada para a camada de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd0c5bad-3682-46d2-b9cb-8538dc9d466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    # Initialize the CBOW model\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        # Define the embedding layer using nn.EmbeddingBag\n",
    "        # It outputs the average of context words embeddings\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse = False)\n",
    "\n",
    "        # Define the first linear layer with input size embed_dim and output size embed_dim/2\n",
    "        self.linear1 = nn.Linear(embed_dim, embed_dim//2)\n",
    "\n",
    "        # Define the fully connected layer with input size embed_dim/2 and output size vocab_size\n",
    "        self.fc = nn.Linear(embed_dim//2, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    # Initialize the weights of the model's parameters\n",
    "    def init_weights(self):\n",
    "        # Initialize the weights of the embedding layer\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        # Initialize the weights of the fully connected layer\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        # Initialize the biases of the fully connected layer to zeros\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # Pass the input text and offsets through the embeddings layer\n",
    "        out = self.embedding(text, offsets)\n",
    "\n",
    "        # Apply the ReLU activation function to the output of the first linear layer\n",
    "        out = torch.relu(self.linear1(out))\n",
    "\n",
    "        # Pass the output of the ReLU activation through the fully connected layer\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7e158-2555-4987-8379-8fee488122d2",
   "metadata": {},
   "source": [
    "___\n",
    "### GloVe pré-treinado de Stanford\n",
    "[site](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "Aproveita dados em larga escala para embeddings de palavras. Pode ser integrado ao PyTorch para tarefas de PNL aprimoradas, como classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52dc1eb5-7ab2-40dd-8ba7-c46ccaa09f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5939fb99-c8ba-498e-adb8-2b7eed9636c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [03:35, 4.00MB/s]                                                                    \n",
      "100%|███████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:49<00:00, 8078.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating an instance of the 6B version of Glove() model\n",
    "glove_vectors_6B = GloVe(name ='6B') # you can specify the model with the following format: GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c415a566-809c-4553-9f32-216a2390dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab from glove_vectors\n",
    "vocab = vocab(glove_vectors_6B.stoi, 0,specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708b8bd-80b5-43ce-82ec-5252647d4d48",
   "metadata": {},
   "source": [
    "___\n",
    "### vocab\n",
    "[site](https://pytorch.org/text/main/vocab.html)\n",
    "\n",
    "O objeto vocab faz parte da biblioteca torchtext do PyTorch. Ele mapeia tokens para índices. O exemplo de código mostra como você pode aplicar o objeto vocab a tokens diretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dff2bf8-0e2d-45dd-b430-2f9536bb02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceIterator:\n",
    "    def __init__(self, text):\n",
    "        # Divide o texto em sentenças\n",
    "        self.sentences = text.split(\".\")\n",
    "        self.index = 0  # Índice para rastrear a próxima sentença\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Retorna o próprio iterador\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Retorna a próxima sentença ou levanta StopIteration\n",
    "        if self.index < len(self.sentences):\n",
    "            sentence = self.sentences[self.index].strip()\n",
    "            self.index += 1\n",
    "            return sentence\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7e1d1de-5646-45b3-be68-883c8569430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple\n",
    "text = \"Python é uma linguagem poderosa. Ela é amplamente usada em IA. Fácil de aprender.\"\n",
    "my_iterator = SentenceIterator(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "459b8905-a80a-401c-949b-23666e4d4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes an iterator as input and extracts the next tokenized sentence. Creates a list of token indices using the vocab dictionary for each token.\n",
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b794011-c261-4aa3-b777-e8de2140233b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ela é amplamente usada em IA'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the tokenized sentences and the corresponding token indices. Repeats the process.\n",
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c55a6b69-099a-46d9-abb8-f745fe9fdb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: Python é uma linguagem poderosa\n",
      "Token Indices: [0, 3526, 2161, 5920, 4870, 3816, 0, 67614, 0, 6481, 1995, 9, 0, 5027, 43, 3816, 3412, 6481, 9, 3412, 1112, 1995, 0, 3422, 4870, 1970, 1112, 1913, 4870, 1536, 9]\n"
     ]
    }
   ],
   "source": [
    "# Prints the tokenized sentence and its corresponding token indices.\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31f9c6-1543-43bc-be71-7a5fae3ba1be",
   "metadata": {},
   "source": [
    "___\n",
    "### Special tokens in PyTorch: `<eos>` and `<bos>`\n",
    "\n",
    "\n",
    "Tokens introduzidos em sequências de entrada para transmitir informações específicas ou atender a um propósito particular durante o treinamento. O exemplo de código mostra o uso de `<bos>` e `<eos>` durante a tokenização. O token `<bos>` denota o início da sequência de entrada, e o token `<eos>` denota o fim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3baed580-1989-4419-92c6-af0a8fba1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de texto\n",
    "text = \"\"\"\n",
    "Python é uma linguagem de programação poderosa.\n",
    "É amplamente usada em inteligência artificial.\n",
    "Este é um exemplo simples para demonstrar a tokenização.\n",
    "\"\"\"\n",
    "\n",
    "# `lines` é uma lista de strings, onde cada string é uma linha do texto\n",
    "lines = text.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68419cd8-66b6-4c3a-956f-cab1decdda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends <bos> at the beginning and <eos> at the end of the tokenized sentences \n",
    "# using a loop that iterates over the sentences in the input data\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = []\n",
    "max_length = 0\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "707c7b38-d41a-4da8-b1d5-9c49bba35cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>',\n",
       "  'Python',\n",
       "  'é',\n",
       "  'uma',\n",
       "  'linguagem',\n",
       "  'de',\n",
       "  'programação',\n",
       "  'poderosa',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " ['<bos>',\n",
       "  'É',\n",
       "  'amplamente',\n",
       "  'usada',\n",
       "  'em',\n",
       "  'inteligência',\n",
       "  'artificial',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " ['<bos>',\n",
       "  'Este',\n",
       "  'é',\n",
       "  'um',\n",
       "  'exemplo',\n",
       "  'simples',\n",
       "  'para',\n",
       "  'demonstrar',\n",
       "  'a',\n",
       "  'tokenização',\n",
       "  '.',\n",
       "  '<eos>']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388ee35-b4e2-4499-9c53-534f2431a272",
   "metadata": {},
   "source": [
    "___\n",
    "### Special tokens in PyTorch: `<pad>` \n",
    "\n",
    "\n",
    "Tokens introduzidos em sequências de entrada para transmitir informações específicas ou atender a um propósito particular durante o treinamento. O exemplo de código mostra o uso do token `<pad>` para garantir que todas as sentenças tenham o mesmo comprimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43f3674a-5073-481b-be68-2d0e57ca5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads the tokenized lines\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ee38c12-232f-4d35-9cc5-3dc04afa2525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>',\n",
       "  'Python',\n",
       "  'é',\n",
       "  'uma',\n",
       "  'linguagem',\n",
       "  'de',\n",
       "  'programação',\n",
       "  'poderosa',\n",
       "  '.',\n",
       "  '<eos>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " ['<bos>',\n",
       "  'É',\n",
       "  'amplamente',\n",
       "  'usada',\n",
       "  'em',\n",
       "  'inteligência',\n",
       "  'artificial',\n",
       "  '.',\n",
       "  '<eos>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " ['<bos>',\n",
       "  'Este',\n",
       "  'é',\n",
       "  'um',\n",
       "  'exemplo',\n",
       "  'simples',\n",
       "  'para',\n",
       "  'demonstrar',\n",
       "  'a',\n",
       "  'tokenização',\n",
       "  '.',\n",
       "  '<eos>']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91330a-986b-4d80-a4da-f0bf964f93c0",
   "metadata": {},
   "source": [
    "___\n",
    "### Perda de entropia cruzada (Cross entropy loss) \n",
    "\n",
    "Uma métrica usada em machine learning (ML) para avaliar o desempenho de um modelo de classificação. A perda é medida como o valor de probabilidade entre 0 (modelo perfeito) e 1. Normalmente, o objetivo é trazer o modelo o mais próximo possível de 0.\n",
    "\n",
    "#### O que é nn.CrossEntropyLoss\n",
    "A função CrossEntropyLoss combina duas operações:\n",
    "\n",
    "- Log-Softmax: Converte os logits (saída da última camada do modelo) em probabilidades logarítmicas.\n",
    "- Negative Log-Likelihood (NLL): Calcula a distância entre as distribuições de probabilidade predita pelo modelo e os rótulos verdadeiros.\n",
    "\n",
    "Isso significa que você pode passar diretamente os logits para CrossEntropyLoss, sem aplicar softmax antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29df352c-6af1-415c-b3b0-5ba2da5c8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4ca9ad2-011f-458e-9d4f-dbe7fa6a3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_size, sparse=True)\n",
    "        self.fc = nn.Linear(embed_size, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Inicializa os pesos das camadas\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # Calcula a representação embutida\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        # Passa pela camada linear para prever as classes\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8586820a-2760-44e0-890e-a26269b216b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros do modelo\n",
    "vocab_size = 5000  # Tamanho do vocabulário\n",
    "embed_size = 64    # Dimensão dos embeddings\n",
    "num_class = 3      # Número de classes de saída\n",
    "\n",
    "# Exemplo de texto e offsets\n",
    "text = torch.tensor([1, 2, 3, 4, 5, 6])  # Índices dos tokens\n",
    "offsets = torch.tensor([0, 3])  # Offset indicando onde as sequências começam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d7b547a-e9a2-48c3-bb50-a880a94e1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de rótulos\n",
    "label = torch.tensor([1, 0])  # Rótulos das sequências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c635ce9-dc10-497f-a2fa-06d893a364f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassificationModel(vocab_size,embed_size,num_class)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "predicted_label = model(text, offsets)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(predicted_label, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4d3b31f-a2aa-4def-8873-ca6093ffeffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0009, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498e33c-fd49-4a9a-8afe-54f6111a1190",
   "metadata": {},
   "source": [
    "___\n",
    "### Otimização (Optimization) \n",
    "\n",
    "Método para reduzir perdas em um modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c616e26-8a36-41ce-b292-82f3bd810834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an iterator object\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae6d01e-8531-4b57-bddb-63f67c7d9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = model(text, offsets)\n",
    "loss = criterion(predicted_label, label)\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4ff16-f1d7-4f9e-825e-63f4de76fbde",
   "metadata": {},
   "source": [
    "___\n",
    "### sentence_bleu()\n",
    "[site](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)\n",
    "\n",
    "NLTK (ou Natural Language Toolkit) fornece esta função para avaliar uma sentença de hipótese em relação a uma ou mais sentenças de referência. As sentenças de referência devem ser apresentadas como uma lista de sentenças onde cada referência é uma lista de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bfcb4902-7ba5-4855-9169-c2e1f2e8fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67689c3d-fe42-4635-94c3-4047c77a199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(generated_translation, reference_translations):\n",
    "    # Convert the generated translations and reference translations into the expected format for sentence_bleu\n",
    "    references = [reference.split() for reference in reference_translations]\n",
    "    hypothesis = generated_translation.split()\n",
    "    # Calculate the BLEU score\n",
    "    bleu_score = sentence_bleu(references, hypothesis)\n",
    "    \n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d7cc2ba-98fb-4a92-b2ea-d1601e4015e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_translations = [\"Asian man sweeping the walkway .\",\n",
    "                          \"An asian man sweeping the walkway .\",\n",
    "                          \"An Asian man sweeps the sidewalk .\",\n",
    "                          \"An Asian man is sweeping the sidewalk .\",\n",
    "                          \"An asian man is sweeping the walkway .\",\n",
    "                          \"Asian man sweeping the sidewalk .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760da85e-03a3-4e3c-b32f-7bdc3a88d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu_score(generated_translation, \n",
    "                                  reference_translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca58ed-751c-4161-b017-a1efbcedd2ea",
   "metadata": {},
   "source": [
    "___\n",
    "### Modelo Encoder RNN\n",
    "O modelo seq2seq codificador-decodificador trabalha em conjunto para transformar uma sequência de entrada em uma sequência de saída. O codificador é uma série de RNNs que processam a sequência de entrada individualmente, passando seus estados ocultos para sua próxima RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "245e2eed-e8ce-4561-98ba-81b446ef9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    def forward(self, input_batch):\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "        embed = embed.to(device)\n",
    "        outputs, (hidden, cell) = self.lstm(embed)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d382cb-9807-4353-a458-83503c5e8115",
   "metadata": {},
   "source": [
    "___\n",
    "### Modelo Decoder RNN\n",
    "\n",
    "O modelo seq2seq codificador-decodificador trabalha em conjunto para transformar uma sequência de entrada em uma sequência de saída. O módulo decodificador é uma série de RNNs que geram autorregressivamente a tradução como um token por vez. Cada token gerado volta para o próximo RNN junto com o estado oculto para gerar o próximo token da sequência de saída até que o token final seja gerado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5db7d142-698b-4cd5-8e02-5b8525760fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction_logit = self.fc_out(output.squeeze(0))\n",
    "        prediction = self.softmax(prediction_logit)\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f6507-fc53-40c6-9d2a-61f6943510fb",
   "metadata": {},
   "source": [
    "___\n",
    "### Modelo Skip-gram\n",
    "\n",
    "Prevê palavras de contexto circundantes a partir de uma palavra-alvo específica. Ele prevê uma palavra de contexto por vez a partir de uma palavra-alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c48e21ac-d307-4792-ab4b-1fda51f55cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SkipGram_Model, self).__init__()\n",
    "        # Define the embeddings layer\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(in_features=embed_dim, out_features=vocab_size)\n",
    "        \n",
    "    # Perform the forward pass\n",
    "    def forward(self, text):\n",
    "        # Pass the input text through the embeddings layer\n",
    "        out = self.embeddings(text)\n",
    "        # Pass the output of the embeddings layer through the fully connected layer\n",
    "        # Apply the ReLU activation function\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc34986-f5bb-4285-8c10-979127e72a3e",
   "metadata": {},
   "source": [
    "___\n",
    "### collate_fn\n",
    "\n",
    "Processa a lista de amostras para formar um lote. O argumento `batch` é uma lista de todas as suas amostras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c875f58a-90e9-4b89-b0be-ea5549ec3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    target_list, context_list = [], []\n",
    "    for _context, _target in batch:\n",
    "        target_list.append(vocab[_target])\n",
    "        context_list.append(vocab[_context])\n",
    "        target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "        context_list = torch.tensor(context_list, dtype=torch.int64)\n",
    "    return target_list.to(device), context_list.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a64338a-d56e-47bf-84f5-870638474361",
   "metadata": {},
   "source": [
    "___\n",
    "### Função de treinamento\n",
    "\n",
    "Treina o modelo para um número especificado de épocas. Também inclui uma condição para verificar se a entrada é para skip-gram ou CBOW. A saída desta função inclui o modelo treinado e uma lista de perdas médias para cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c8623ea-794e-4a43-a1b3-9284fc7277c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=1000):\n",
    "    # List to store running loss for each epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Storing running loss values for the current epoch\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop through the data loader\n",
    "        for idx, samples in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Check for EmbeddingBag layer in the model (CBOW)\n",
    "            if any(isinstance(module, nn.EmbeddingBag) for _, module in model.named_modules()):\n",
    "                target, context, offsets = samples\n",
    "                predicted = model(context, offsets)\n",
    "\n",
    "            # Check for Embedding layer in the model (Skip-gram)\n",
    "            elif any(isinstance(module, nn.Embedding) for _, module in model.named_modules()):\n",
    "                target, context = samples\n",
    "                predicted = model(context)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(predicted, target)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Append average loss for the epoch\n",
    "        epoch_losses.append(running_loss / len(dataloader))\n",
    "\n",
    "    return model, epoch_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f417f2-fb22-47a2-9186-69cf02a84ba0",
   "metadata": {},
   "source": [
    "___\n",
    "### Modelo CBOW\n",
    "\n",
    "Utiliza palavras de contexto para prever uma palavra-alvo e gerar sua incorporação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5046fef4-5811-41fb-ab4b-27f4409b45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    # Initialize the CBOW model\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(CBOW, self).__init__()\n",
    "        # Define the embedding layer using nn.EmbeddingBag\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # Pass the input text and offsets through the embedding layer\n",
    "        out = self.embedding(text, offsets)\n",
    "        # Apply the ReLU activation function to the output of the embedding layer\n",
    "        out = torch.relu(out)\n",
    "        # Pass the output of the ReLU activation through the fully connected layer\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db6c6159-7f94-4921-8ac9-ed306e03d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the CBOW model\n",
    "vocab_size = len(vocab)  # Assuming 'vocab' is defined\n",
    "emsize = 24  # Size of the embedding dimension\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the CBOW model\n",
    "model_cbow = CBOW(vocab_size, emsize, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2319c-066e-4c2a-9225-3427bc920edd",
   "metadata": {},
   "source": [
    "___\n",
    "### Training loop\n",
    "\n",
    "Enumera dados do DataLoader e, em cada passagem do loop, obtém um lote de dados de treinamento do DataLoader, zera os gradientes do otimizador e executa uma inferência (obtém previsões do modelo para um lote de entrada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cbe308-d7b5-4c41-8ad2-afb20f7338bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    model.train()\n",
    "    cum_loss=0\n",
    "    for idx, (label, text, offsets) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        cum_loss+=loss.item()\n",
    "    cum_loss_list.append(cum_loss)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    acc_epoch.append(accu_val)\n",
    "    if accu_val > acc_old:\n",
    "        acc_old= accu_val\n",
    "        torch.save(model.state_dict(), 'my_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642ac19-18e8-43f6-9cdf-840c554769ef",
   "metadata": {},
   "source": [
    "____\n",
    "Esse material tem como referência o curso [Gen AI Foundational Models for NLP & Language Understanding](https://www.coursera.org/learn/gen-ai-foundational-models-for-nlp-and-language-understanding?specialization=generative-ai-engineering-with-llms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe38d9-2584-4277-889e-593f1d1e70d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
