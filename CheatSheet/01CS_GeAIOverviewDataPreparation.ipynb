{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e65dfb-a969-45e3-b3da-1c8a8e59924e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e839e98-587a-4581-89c6-be2234938f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280efac1-212c-4ee2-939a-389ce5c485fd",
   "metadata": {},
   "source": [
    "___\n",
    "### NLTK\n",
    "[site](https://www.nltk.org/)\n",
    "\n",
    "NLTK é uma biblioteca Python usada em processamento de linguagem natural (NLP) para tarefas como tokenização e processamento de texto.\n",
    "\n",
    "O exemplo de código mostra como você pode tokenizar texto usando o tokenizador baseado em palavras do NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc590867-de85-4662-8bf6-1527429e1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9274a89c-2855-4fa9-8c48-73d7313189cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc9a551-3928-4f26-8c5e-505cb6e0e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e094ed83-f425-419d-99f1-8c54a012927c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.', 'I', 'could', \"n't\", 'see', 'it', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "token = word_tokenize(text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b2a56e-4482-4dc8-b712-159c60597a8e",
   "metadata": {},
   "source": [
    "___\n",
    "### spaCy\n",
    "[site](https://spacy.io/)\n",
    "\n",
    "spaCy é uma biblioteca de código aberto usada em NLP. Ela oferece ferramentas para tarefas como tokenização e embeddings de palavras.\n",
    "\n",
    "O exemplo de código mostra como você pode tokenizar texto usando o tokenizador baseado em palavras do spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa50643-92b3-419e-bda7-1a40361cb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77961bb0-64e0-42a3-9414-3f2509dade81",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fefc965-6f66-4319-8142-923302710e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f6c7857-6603-4c9b-9cf5-d9c35b6fa361",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54f61b82-2272-4aba-a918-467cf2a8f33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.', 'I', 'could', \"n't\", 'see', 'it', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "token_list = [token.text for token in doc]\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad215a-b0a0-403b-8cdf-9f95068f3dca",
   "metadata": {},
   "source": [
    "___\n",
    "### BertTokenizer\n",
    "[site](https://huggingface.co/transformers/v3.0.2/model_doc/bert.html)\n",
    "\n",
    "BertTokenizer é um tokenizador baseado em subpalavras que utiliza o algoritmo WordPiece.\n",
    "\n",
    "O exemplo de código mostra como você pode tokenizar texto usando o BertTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70913837-6f5e-4a5d-8ecf-6e9c80b75707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df93c943-dda7-47dc-b9f5-976049c33987",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e6d85d-f704-4981-8338-e9467c20bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8419166-f6d6-49df-a741-8946c4d98724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unicorn',\n",
       " '##s',\n",
       " 'are',\n",
       " 'real',\n",
       " '.',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'unicorn',\n",
       " 'yesterday',\n",
       " '.',\n",
       " 'i',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'see',\n",
       " 'it',\n",
       " 'today',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07580d67-2eba-4330-a02a-17e864225eec",
   "metadata": {},
   "source": [
    "___\n",
    "### XLNetTokenizer\n",
    "[site](https://huggingface.co/docs/transformers/model_doc/xlnet)\n",
    "\n",
    "XLNetTokenizer tokeniza texto usando algoritmos Unigram e SentencePiece.\n",
    "    \n",
    "O exemplo de código mostra como você pode tokenizar texto usando XLNetTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d530db12-8231-4663-a668-47745e742886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6a04e54-5572-466d-a15e-05e69caa0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63041bc0-0a11-449d-a963-8a6aa8df345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "925b8126-6abe-44b1-a9a5-516f66827b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Uni',\n",
       " 'corn',\n",
       " 's',\n",
       " '▁are',\n",
       " '▁real',\n",
       " '.',\n",
       " '▁I',\n",
       " '▁saw',\n",
       " '▁a',\n",
       " '▁',\n",
       " 'uni',\n",
       " 'corn',\n",
       " '▁yesterday',\n",
       " '.',\n",
       " '▁I',\n",
       " '▁couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " '▁see',\n",
       " '▁it',\n",
       " '▁today',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179831f8-28e6-427d-8266-078184c3ecb9",
   "metadata": {},
   "source": [
    "___\n",
    "### torchtext\n",
    "[site](https://pytorch.org/text/stable/index.html)\n",
    "\n",
    "A biblioteca torchtext faz parte do ecossistema PyTorch e fornece as ferramentas e funcionalidades necessárias para PNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a48f4971-4c72-45cb-9f74-ddcc15ac8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cd834aa-e7d7-4d5b-b859-a0fcb9ef2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a dataset\n",
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\"NLP Named Entity,Sentiment Analysis, Machine Translation\"),\n",
    "    (1,\"Machine Translation with NLP\"),\n",
    "    (1,\"Named Entity vs Sentiment Analysis NLP\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33482703-bace-4713-951c-80068c36e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the tokenizer to th text to get the tokens as a list\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75716f5a-a357-4fe1-8a83-f60b09cfd3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction', 'to', 'nlp']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80d09062-0fe7-4ed4-a862-abcdb10c63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a data iterator as input, processes text from the iterator, and yields the tokenized output individually\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "765ea523-317b-4e09-93d1-b0af1e78e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an iterator\n",
    "my_iterator = yield_tokens(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3549a2f-dd70-497f-bed4-f3a406bd7ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction', 'to', 'nlp']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetches the next set of tokens from the data set\n",
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00274750-ceaf-40ef-a041-b8030d14e645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basics', 'of', 'pytorch']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8905945a-0085-42b8-88f0-ba33e64e5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts tokens to indices and sets <unk> as the default word if a word is not found in the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), \n",
    "                                  specials=[\"<unk>\"]\n",
    "                                 )\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3f793e6-b54c-44d6-af2e-e7c46ec71d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " 'nlp': 1,\n",
       " 'pytorch': 2,\n",
       " 'analysis': 3,\n",
       " 'sentiment': 7,\n",
       " 'entity': 4,\n",
       " 'named': 6,\n",
       " 'techniques': 17,\n",
       " 'machine': 5,\n",
       " 'translation': 8,\n",
       " 'with': 9,\n",
       " ',': 10,\n",
       " 'basics': 11,\n",
       " 'classification': 12,\n",
       " 'for': 13,\n",
       " 'introduction': 14,\n",
       " 'of': 15,\n",
       " 'recognition': 16,\n",
       " 'text': 18,\n",
       " 'to': 19,\n",
       " 'using': 20,\n",
       " 'vs': 21}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fives a dictionary that maps words to their corresponding numerical indices\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e6294-ebb2-43f6-9ae3-2a9c7680d5da",
   "metadata": {},
   "source": [
    "___\n",
    "### vocab\n",
    "[site](https://pytorch.org/text/main/vocab.html)\n",
    "\n",
    "O objeto vocab faz parte da biblioteca torchtext do PyTorch. Ele mapeia tokens para índices.\n",
    "\n",
    "O exemplo de código mostra como você pode aplicar o objeto vocab a tokens diretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c058cc0e-333b-45bb-9ac2-0118271d0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes an iterator as input and extracts the next tokenized sentence. \n",
    "# Creates a list of token indices using the vocab dictionary for each token\n",
    "\n",
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e74759ba-ddf2-4168-9bfa-7327a4613878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['named', 'entity', 'recognition', 'with', 'pytorch']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the tokenized sentences and the corresponding token indices. Repeats the process\n",
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70247d8b-3443-4f07-8129-7cff65fd54ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['nlp', 'techniques', 'for', 'text', 'classification']\n",
      "Token indices: [1, 17, 13, 18, 12]\n"
     ]
    }
   ],
   "source": [
    "# Prints the tokenized sentence and its corresponding token indices\n",
    "print(\"Tokenized sentence:\", tokenized_sentence)\n",
    "print(\"Token indices:\", token_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36064010-bb59-4121-a271-ad7ac56c0a3d",
   "metadata": {},
   "source": [
    "___\n",
    "### Special tokens in PyTorch: `<eos> and <bos>`\n",
    "\n",
    "Tokens Especiais são tokens introduzidos nas sequências de entrada para transmitir informações específicas ou cumprir uma função particular durante o treinamento.\n",
    "\n",
    "O exemplo de código mostra o uso de <bos> e <eos> durante a tokenização. O token <bos> indica o início da sequência de entrada, enquanto o token <eos> indica o final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "897b2cc8-87b7-4b98-a7d6-1c06c8366120",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\"IBM taught me tokenization\", \n",
    "         \"Special tokenizers are ready and they will blow your mind\", \n",
    "         \"just saying hi!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "186877cd-4a4d-48bc-88b4-dd84c4b8f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends <bos> at the beginning and <eos> at the end of the tokenized sentences \n",
    "# using a loop that iterates over th sentences in the input data\n",
    "tokenizer_en = get_tokenizer('spacy', \n",
    "                             language='en_core_web_sm'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97ce53cf-1d57-4ebe-89c5-1a7d732a72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15d0ad10-386e-497f-9319-b3b21e637fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>', 'IBM', 'taught', 'me', 'tokenization', '<eos>'],\n",
       " ['<bos>',\n",
       "  'Special',\n",
       "  'tokenizers',\n",
       "  'are',\n",
       "  'ready',\n",
       "  'and',\n",
       "  'they',\n",
       "  'will',\n",
       "  'blow',\n",
       "  'your',\n",
       "  'mind',\n",
       "  '<eos>'],\n",
       " ['<bos>', 'just', 'saying', 'hi', '!', '<eos>']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3248b-11d0-455a-b8ff-8722bd1e5abb",
   "metadata": {},
   "source": [
    "___\n",
    "### Special tokens in PyTorch: `<pad>`\n",
    "\n",
    "O exemplo de código mostra o uso do token <pad> para garantir que todas as frases tenham o mesmo comprimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1a9a5cb-1acb-4a8b-8ec0-3ab82d6db7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads the tokenized lines\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e265923-dc7e-4032-b624-c289d3635656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>',\n",
       "  'IBM',\n",
       "  'taught',\n",
       "  'me',\n",
       "  'tokenization',\n",
       "  '<eos>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " ['<bos>',\n",
       "  'Special',\n",
       "  'tokenizers',\n",
       "  'are',\n",
       "  'ready',\n",
       "  'and',\n",
       "  'they',\n",
       "  'will',\n",
       "  'blow',\n",
       "  'your',\n",
       "  'mind',\n",
       "  '<eos>'],\n",
       " ['<bos>',\n",
       "  'just',\n",
       "  'saying',\n",
       "  'hi',\n",
       "  '!',\n",
       "  '<eos>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056ea46-f111-42b9-8f4a-0285f993c2c1",
   "metadata": {},
   "source": [
    "___\n",
    "### Dataset class in PyTorch\n",
    "[site](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "A classe Dataset permite acessar e recuperar amostras individuais de um conjunto de dados.\n",
    "\r\n",
    "O exemplo de código mostra como você pode criar um conjunto de dados personalizado e acessar amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5dd03116-5232-4f94-a7ed-c64897e81349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4410612f-277d-444e-9e40-a9fd80cbb49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
       " \"Fae's a fickle friend, Harry.\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"If you want to know what a man's like, take a \\\n",
    "good look at how he treats his inferiors, not his equals.\",\n",
    "\"Fae's a fickle friend, Harry.\"]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b0ef1af-6d43-4245-9db7-ae621e8711c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    # Returns the data length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    # Returns one item on the index\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f6af796-aa34-44a9-b190-cd12d4cdc636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset object\n",
    "dataset = CustomDataset(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44b4e75f-ba4c-4fb5-a87d-20fdaee0e720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5400cd3-f858-4ee2-ad43-3fd6c00a9ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2034b-dba6-4a8a-9ec8-0e52239d8a9e",
   "metadata": {},
   "source": [
    "___\n",
    "### Dataloader class in PyTorch\n",
    "[site](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Uma classe DataLoader permite carregamento e iteração eficientes sobre conjuntos de dados para treinamento de modelos de aprendizado profundo.\n",
    "\n",
    "O exemplo de código mostra como você pode usar a classe DataLoader para gerar lotes de sentenças para processamento posterior, como treinamento de um modelo de rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b76efb43-5fa1-4c46-9e83-8240c0c31ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7501195a-9e37-4512-952a-63f3c548a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6392cfd7-f956-423a-b251-4639106bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b995ff2-776f-41a5-bef7-3b8d48701450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an iterator object\n",
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "356e1af2-2bc9-4a0c-a273-d9e8ba54f67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fae's a fickle friend, Harry.\",\n",
       " \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calls the next function to return new batches of samples\n",
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8c4bb5a-4790-416c-85e8-e1ac5789d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(sentences)\n",
    "\n",
    "# Specifies a batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Creates a data loader\n",
    "dataloader = DataLoader(custom_dataset,\n",
    "batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e0ff10a-4995-436c-b175-81857315f1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\", \"Fae's a fickle friend, Harry.\"]\n"
     ]
    }
   ],
   "source": [
    "# Prints the sentences in each batch\n",
    "for batch in dataloader:\n",
    " print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa98e0a-883d-44ba-998e-d17294fec4f8",
   "metadata": {},
   "source": [
    "___\n",
    "### Custom collate function in PyTorch\n",
    "\n",
    "A função de colagem personalizada é uma função definida pelo usuário que especifica como as amostras individuais são agrupadas em batches. Você pode usar a função de colagem para tarefas como tokenização, conversão de índices tokenizados e transformação do resultado em um tensor.\n",
    "\n",
    "O exemplo de código mostra como você pode usar uma função de colagem personalizada em um data loader.l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "976f0a2f-73a1-41f1-82c0-644b52b7326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bef3b0ca-d624-45e3-a4ca-cfafce94aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a custom collate function\n",
    "def collate_fn(batch):\n",
    "    tensor_batch = []\n",
    "    # Tokenizes each sample in the batch\n",
    "    for sample in batch:\n",
    "        tokens = tokenizer(sample)\n",
    "        \n",
    "    # Maps tokens to numbers using the vocab\n",
    "    tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "\n",
    "    # Pads the sequences within the batch to have equal lengths\n",
    "    padded_batch = pad_sequence(tensor_batch,batch_first=True)\n",
    "    \n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d0a658e2-fc98-41a9-b623-0c179f7a476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data loader using the collate function and the custom dataset\n",
    "dataloader = DataLoader(custom_dataset,\n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True, \n",
    "                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e5d8425-f5a3-4ddc-b2b0-6ecaf245dbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  0,  0,  0, 10,  0,  0]])\n",
      "shape of sample 1 \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    print(\"shape of sample\",len(batch), \"\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a9745d-6545-4713-8d1f-7671ecfd4d95",
   "metadata": {},
   "source": [
    "____\n",
    "Esse material tem como referência o curso [Generative AI and LLMs: Architecture and Data Preparation](https://www.coursera.org/learn/generative-ai-llm-architecture-data-preparation?specialization=generative-ai-engineering-with-llms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
